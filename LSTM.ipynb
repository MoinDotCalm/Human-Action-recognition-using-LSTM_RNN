{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from random import randint\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "DATASET_PATH = \"data/HAR_pose_activities/database/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "n_steps = 32 # 32 timesteps per series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the networks inputs\n",
    "\n",
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    X_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.split(',') for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    file.close()\n",
    "    blocks = int(len(X_) / n_steps)\n",
    "    \n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "\n",
    "    return X_ \n",
    "\n",
    "# Load the networks outputs\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # for 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train = load_X(X_train_path)\n",
    "X_test = load_X(X_test_path)\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(22625, 32, 36) (5751, 1) 251.01117 126.12204\n",
      "\n",
      "The dataset has not been preprocessed, is not normalised etc\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "\n",
    "n_hidden = 34 # Hidden layer num of features\n",
    "n_classes = 6 \n",
    "\n",
    "#updated for learning-rate decay\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = True\n",
    "learning_rate = 0.0020 \n",
    "init_learning_rate = 0.005\n",
    "decay_rate = 0.96 #the base of the exponential in the decay\n",
    "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "training_iters = training_data_count *300  # Loop 300 times on the dataset, ie 300 epochs\n",
    "batch_size = 256\n",
    "display_iter = batch_size*8  # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])   \n",
    "    # Rectifies Linear Unit activation function used\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n",
    "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
    "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    batch_labels = np.empty((batch_size,1)) \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        # index = random sample from _unsampled (indices)\n",
    "        index = random.choice(_unsampled)\n",
    "        batch_s[i] = _train[index] \n",
    "        batch_labels[i] = _labels[index]\n",
    "        _unsampled.remove(index)\n",
    "\n",
    "\n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # One hot encoding of the network outputs\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-a1d7a6b10f56>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-a1d7a6b10f56>:14: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-a1d7a6b10f56>:15: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\Users\\navni\\anaconda3\\envs\\tfcontrib\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x0000016EAFD507B8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EF641C208>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000016EFAE43940>>: AttributeError: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:From <ipython-input-6-750114f69579>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "if decaying_learning_rate:\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #256:  Learning rate = 0.005000:   Batch Loss = 3.783322, Accuracy = 0.11328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.3415613174438477, Accuracy = 0.21213701367378235\n",
      "Iter #2048:  Learning rate = 0.005000:   Batch Loss = 3.160298, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.157527208328247, Accuracy = 0.07946444302797318\n",
      "Iter #4096:  Learning rate = 0.005000:   Batch Loss = 3.041912, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.0210204124450684, Accuracy = 0.23665449023246765\n",
      "Iter #6144:  Learning rate = 0.005000:   Batch Loss = 3.041152, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.9795093536376953, Accuracy = 0.21144148707389832\n",
      "Iter #8192:  Learning rate = 0.005000:   Batch Loss = 2.950046, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.947268009185791, Accuracy = 0.2135280817747116\n",
      "Iter #10240:  Learning rate = 0.005000:   Batch Loss = 3.098590, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 3.0447468757629395, Accuracy = 0.13701964914798737\n",
      "Iter #12288:  Learning rate = 0.005000:   Batch Loss = 2.908806, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.8919734954833984, Accuracy = 0.22830812633037567\n",
      "Iter #14336:  Learning rate = 0.005000:   Batch Loss = 2.877589, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.836514711380005, Accuracy = 0.21144148707389832\n",
      "Iter #16384:  Learning rate = 0.005000:   Batch Loss = 2.791945, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.785747766494751, Accuracy = 0.22952529788017273\n",
      "Iter #18432:  Learning rate = 0.005000:   Batch Loss = 2.752855, Accuracy = 0.26171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.7562856674194336, Accuracy = 0.22830812633037567\n",
      "Iter #20480:  Learning rate = 0.005000:   Batch Loss = 2.765290, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.73037052154541, Accuracy = 0.22830812633037567\n",
      "Iter #22528:  Learning rate = 0.005000:   Batch Loss = 2.688262, Accuracy = 0.26171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.704455852508545, Accuracy = 0.22952529788017273\n",
      "Iter #24576:  Learning rate = 0.005000:   Batch Loss = 2.699846, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.6647109985351562, Accuracy = 0.22830812633037567\n",
      "Iter #26624:  Learning rate = 0.005000:   Batch Loss = 2.634977, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.628983497619629, Accuracy = 0.231264129281044\n",
      "Iter #28672:  Learning rate = 0.005000:   Batch Loss = 2.622290, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.6048007011413574, Accuracy = 0.22952529788017273\n",
      "Iter #30720:  Learning rate = 0.005000:   Batch Loss = 2.602449, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.5683560371398926, Accuracy = 0.23074248433113098\n",
      "Iter #32768:  Learning rate = 0.005000:   Batch Loss = 2.554852, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.5942392349243164, Accuracy = 0.1358024626970291\n",
      "Iter #34816:  Learning rate = 0.005000:   Batch Loss = 2.552435, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.5169858932495117, Accuracy = 0.22830812633037567\n",
      "Iter #36864:  Learning rate = 0.005000:   Batch Loss = 2.535980, Accuracy = 0.1796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.489529609680176, Accuracy = 0.21231089532375336\n",
      "Iter #38912:  Learning rate = 0.005000:   Batch Loss = 2.478836, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.4719595909118652, Accuracy = 0.23039472103118896\n",
      "Iter #40960:  Learning rate = 0.005000:   Batch Loss = 2.482290, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.4437475204467773, Accuracy = 0.22830812633037567\n",
      "Iter #43008:  Learning rate = 0.005000:   Batch Loss = 2.436764, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.430595636367798, Accuracy = 0.2109198421239853\n",
      "Iter #45056:  Learning rate = 0.005000:   Batch Loss = 2.427351, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.392148017883301, Accuracy = 0.22830812633037567\n",
      "Iter #47104:  Learning rate = 0.005000:   Batch Loss = 2.376502, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3786470890045166, Accuracy = 0.23039472103118896\n",
      "Iter #49152:  Learning rate = 0.005000:   Batch Loss = 2.402109, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3572373390197754, Accuracy = 0.23039472103118896\n",
      "Iter #51200:  Learning rate = 0.005000:   Batch Loss = 2.368399, Accuracy = 0.16796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3349647521972656, Accuracy = 0.21144148707389832\n",
      "Iter #53248:  Learning rate = 0.005000:   Batch Loss = 2.339039, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.319338321685791, Accuracy = 0.22830812633037567\n",
      "Iter #55296:  Learning rate = 0.005000:   Batch Loss = 2.298666, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.296502113342285, Accuracy = 0.22830812633037567\n",
      "Iter #57344:  Learning rate = 0.005000:   Batch Loss = 2.300131, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.285764694213867, Accuracy = 0.23039472103118896\n",
      "Iter #59392:  Learning rate = 0.005000:   Batch Loss = 2.269220, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.259517192840576, Accuracy = 0.21231089532375336\n",
      "Iter #61440:  Learning rate = 0.005000:   Batch Loss = 2.274254, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.241589069366455, Accuracy = 0.22830812633037567\n",
      "Iter #63488:  Learning rate = 0.005000:   Batch Loss = 2.205338, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.216615676879883, Accuracy = 0.22830812633037567\n",
      "Iter #65536:  Learning rate = 0.005000:   Batch Loss = 2.229576, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.2753548622131348, Accuracy = 0.21144148707389832\n",
      "Iter #67584:  Learning rate = 0.005000:   Batch Loss = 2.236820, Accuracy = 0.17578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.188971519470215, Accuracy = 0.23039472103118896\n",
      "Iter #69632:  Learning rate = 0.005000:   Batch Loss = 2.216192, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1830270290374756, Accuracy = 0.21231089532375336\n",
      "Iter #71680:  Learning rate = 0.005000:   Batch Loss = 2.133830, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1684086322784424, Accuracy = 0.22848200798034668\n",
      "Iter #73728:  Learning rate = 0.005000:   Batch Loss = 2.143126, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1446409225463867, Accuracy = 0.22830812633037567\n",
      "Iter #75776:  Learning rate = 0.005000:   Batch Loss = 2.151443, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1357131004333496, Accuracy = 0.23039472103118896\n",
      "Iter #77824:  Learning rate = 0.005000:   Batch Loss = 2.125713, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1250123977661133, Accuracy = 0.21231089532375336\n",
      "Iter #79872:  Learning rate = 0.005000:   Batch Loss = 2.130639, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.108023166656494, Accuracy = 0.23039472103118896\n",
      "Iter #81920:  Learning rate = 0.005000:   Batch Loss = 2.128398, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.09822416305542, Accuracy = 0.21231089532375336\n",
      "Iter #83968:  Learning rate = 0.005000:   Batch Loss = 2.084167, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0815999507904053, Accuracy = 0.21231089532375336\n",
      "Iter #86016:  Learning rate = 0.005000:   Batch Loss = 2.110133, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.071958065032959, Accuracy = 0.22830812633037567\n",
      "Iter #88064:  Learning rate = 0.005000:   Batch Loss = 2.057312, Accuracy = 0.26171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0704545974731445, Accuracy = 0.22830812633037567\n",
      "Iter #90112:  Learning rate = 0.005000:   Batch Loss = 2.070127, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0502166748046875, Accuracy = 0.22969917953014374\n",
      "Iter #92160:  Learning rate = 0.005000:   Batch Loss = 2.072658, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.038830518722534, Accuracy = 0.21144148707389832\n",
      "Iter #94208:  Learning rate = 0.005000:   Batch Loss = 2.040573, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.024251937866211, Accuracy = 0.22848200798034668\n",
      "Iter #96256:  Learning rate = 0.005000:   Batch Loss = 2.038539, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0192670822143555, Accuracy = 0.22830812633037567\n",
      "Iter #98304:  Learning rate = 0.005000:   Batch Loss = 2.018917, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0102736949920654, Accuracy = 0.22830812633037567\n",
      "Iter #100352:  Learning rate = 0.004800:   Batch Loss = 2.018914, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.003899335861206, Accuracy = 0.22952529788017273\n",
      "Iter #102400:  Learning rate = 0.004800:   Batch Loss = 2.016107, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9992573261260986, Accuracy = 0.22830812633037567\n",
      "Iter #104448:  Learning rate = 0.004800:   Batch Loss = 1.984448, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9817172288894653, Accuracy = 0.21144148707389832\n",
      "Iter #106496:  Learning rate = 0.004800:   Batch Loss = 1.986148, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9712154865264893, Accuracy = 0.22952529788017273\n",
      "Iter #108544:  Learning rate = 0.004800:   Batch Loss = 1.985911, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9640089273452759, Accuracy = 0.22830812633037567\n",
      "Iter #110592:  Learning rate = 0.004800:   Batch Loss = 1.983021, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9640445709228516, Accuracy = 0.21144148707389832\n",
      "Iter #112640:  Learning rate = 0.004800:   Batch Loss = 1.931389, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9658541679382324, Accuracy = 0.22952529788017273\n",
      "Iter #114688:  Learning rate = 0.004800:   Batch Loss = 1.947636, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9433656930923462, Accuracy = 0.22848200798034668\n",
      "Iter #116736:  Learning rate = 0.004800:   Batch Loss = 1.918603, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9409855604171753, Accuracy = 0.22830812633037567\n",
      "Iter #118784:  Learning rate = 0.004800:   Batch Loss = 1.905438, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9319721460342407, Accuracy = 0.22969917953014374\n",
      "Iter #120832:  Learning rate = 0.004800:   Batch Loss = 1.952595, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9246753454208374, Accuracy = 0.22952529788017273\n",
      "Iter #122880:  Learning rate = 0.004800:   Batch Loss = 1.938352, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.919948697090149, Accuracy = 0.22830812633037567\n",
      "Iter #124928:  Learning rate = 0.004800:   Batch Loss = 1.909973, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.913814663887024, Accuracy = 0.21144148707389832\n",
      "Iter #126976:  Learning rate = 0.004800:   Batch Loss = 1.909044, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9143798351287842, Accuracy = 0.21144148707389832\n",
      "Iter #129024:  Learning rate = 0.004800:   Batch Loss = 1.890860, Accuracy = 0.15234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.902843713760376, Accuracy = 0.22848200798034668\n",
      "Iter #131072:  Learning rate = 0.004800:   Batch Loss = 1.954869, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8977962732315063, Accuracy = 0.22830812633037567\n",
      "Iter #133120:  Learning rate = 0.004800:   Batch Loss = 1.879496, Accuracy = 0.296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.899807095527649, Accuracy = 0.22830812633037567\n",
      "Iter #135168:  Learning rate = 0.004800:   Batch Loss = 1.902274, Accuracy = 0.171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8899153470993042, Accuracy = 0.21144148707389832\n",
      "Iter #137216:  Learning rate = 0.004800:   Batch Loss = 1.891782, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8800578117370605, Accuracy = 0.22952529788017273\n",
      "Iter #139264:  Learning rate = 0.004800:   Batch Loss = 1.884105, Accuracy = 0.15625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8763072490692139, Accuracy = 0.22848200798034668\n",
      "Iter #141312:  Learning rate = 0.004800:   Batch Loss = 1.897456, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8759324550628662, Accuracy = 0.22830812633037567\n",
      "Iter #143360:  Learning rate = 0.004800:   Batch Loss = 1.892049, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8712375164031982, Accuracy = 0.22952529788017273\n",
      "Iter #145408:  Learning rate = 0.004800:   Batch Loss = 1.881877, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.87141752243042, Accuracy = 0.21144148707389832\n",
      "Iter #147456:  Learning rate = 0.004800:   Batch Loss = 1.904695, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8644033670425415, Accuracy = 0.22830812633037567\n",
      "Iter #149504:  Learning rate = 0.004800:   Batch Loss = 1.901310, Accuracy = 0.2578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8648940324783325, Accuracy = 0.22830812633037567\n",
      "Iter #151552:  Learning rate = 0.004800:   Batch Loss = 1.917012, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.852721929550171, Accuracy = 0.22848200798034668\n",
      "Iter #153600:  Learning rate = 0.004800:   Batch Loss = 1.844779, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8510336875915527, Accuracy = 0.22952529788017273\n",
      "Iter #155648:  Learning rate = 0.004800:   Batch Loss = 1.856210, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8469297885894775, Accuracy = 0.22830812633037567\n",
      "Iter #157696:  Learning rate = 0.004800:   Batch Loss = 1.858486, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8431452512741089, Accuracy = 0.2291775345802307\n",
      "Iter #159744:  Learning rate = 0.004800:   Batch Loss = 1.814826, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8421047925949097, Accuracy = 0.22952529788017273\n",
      "Iter #161792:  Learning rate = 0.004800:   Batch Loss = 1.842083, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.84040367603302, Accuracy = 0.22830812633037567\n",
      "Iter #163840:  Learning rate = 0.004800:   Batch Loss = 1.882037, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8306329250335693, Accuracy = 0.22848200798034668\n",
      "Iter #165888:  Learning rate = 0.004800:   Batch Loss = 1.844076, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8287317752838135, Accuracy = 0.22848200798034668\n",
      "Iter #167936:  Learning rate = 0.004800:   Batch Loss = 1.843695, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8274825811386108, Accuracy = 0.21144148707389832\n",
      "Iter #169984:  Learning rate = 0.004800:   Batch Loss = 1.839351, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8266682624816895, Accuracy = 0.22830812633037567\n",
      "Iter #172032:  Learning rate = 0.004800:   Batch Loss = 1.823564, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8252581357955933, Accuracy = 0.2290036529302597\n",
      "Iter #174080:  Learning rate = 0.004800:   Batch Loss = 1.806406, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8157974481582642, Accuracy = 0.23056860268115997\n",
      "Iter #176128:  Learning rate = 0.004800:   Batch Loss = 1.872781, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8140015602111816, Accuracy = 0.22848200798034668\n",
      "Iter #178176:  Learning rate = 0.004800:   Batch Loss = 1.828211, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8162790536880493, Accuracy = 0.22830812633037567\n",
      "Iter #180224:  Learning rate = 0.004800:   Batch Loss = 1.829331, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8100849390029907, Accuracy = 0.22830812633037567\n",
      "Iter #182272:  Learning rate = 0.004800:   Batch Loss = 1.841998, Accuracy = 0.1796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.808213710784912, Accuracy = 0.23039472103118896\n",
      "Iter #184320:  Learning rate = 0.004800:   Batch Loss = 1.830351, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.811131238937378, Accuracy = 0.22952529788017273\n",
      "Iter #186368:  Learning rate = 0.004800:   Batch Loss = 1.806219, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8146668672561646, Accuracy = 0.22830812633037567\n",
      "Iter #188416:  Learning rate = 0.004800:   Batch Loss = 1.809578, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.801147699356079, Accuracy = 0.22830812633037567\n",
      "Iter #190464:  Learning rate = 0.004800:   Batch Loss = 1.824277, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7987314462661743, Accuracy = 0.23056860268115997\n",
      "Iter #192512:  Learning rate = 0.004800:   Batch Loss = 1.814835, Accuracy = 0.265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8056362867355347, Accuracy = 0.22830812633037567\n",
      "Iter #194560:  Learning rate = 0.004800:   Batch Loss = 1.796614, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8014147281646729, Accuracy = 0.22830812633037567\n",
      "Iter #196608:  Learning rate = 0.004800:   Batch Loss = 1.820575, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7912675142288208, Accuracy = 0.22830812633037567\n",
      "Iter #198656:  Learning rate = 0.004800:   Batch Loss = 1.760865, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7902846336364746, Accuracy = 0.22830812633037567\n",
      "Iter #200704:  Learning rate = 0.004608:   Batch Loss = 1.812316, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7893626689910889, Accuracy = 0.22848200798034668\n",
      "Iter #202752:  Learning rate = 0.004608:   Batch Loss = 1.838960, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.790076494216919, Accuracy = 0.21144148707389832\n",
      "Iter #204800:  Learning rate = 0.004608:   Batch Loss = 1.797782, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.789841651916504, Accuracy = 0.22830812633037567\n",
      "Iter #206848:  Learning rate = 0.004608:   Batch Loss = 1.785666, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7922310829162598, Accuracy = 0.22848200798034668\n",
      "Iter #208896:  Learning rate = 0.004608:   Batch Loss = 1.822898, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7860620021820068, Accuracy = 0.22952529788017273\n",
      "Iter #210944:  Learning rate = 0.004608:   Batch Loss = 1.793349, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7833890914916992, Accuracy = 0.22952529788017273\n",
      "Iter #212992:  Learning rate = 0.004608:   Batch Loss = 1.819081, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7821248769760132, Accuracy = 0.22830812633037567\n",
      "Iter #215040:  Learning rate = 0.004608:   Batch Loss = 1.804906, Accuracy = 0.26953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7839025259017944, Accuracy = 0.22830812633037567\n",
      "Iter #217088:  Learning rate = 0.004608:   Batch Loss = 1.788639, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7825915813446045, Accuracy = 0.22952529788017273\n",
      "Iter #219136:  Learning rate = 0.004608:   Batch Loss = 1.809074, Accuracy = 0.17578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7816368341445923, Accuracy = 0.21144148707389832\n",
      "Iter #221184:  Learning rate = 0.004608:   Batch Loss = 1.803942, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.777544379234314, Accuracy = 0.22830812633037567\n",
      "Iter #223232:  Learning rate = 0.004608:   Batch Loss = 1.784322, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7730224132537842, Accuracy = 0.21144148707389832\n",
      "Iter #225280:  Learning rate = 0.004608:   Batch Loss = 1.813123, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.774312138557434, Accuracy = 0.21144148707389832\n",
      "Iter #227328:  Learning rate = 0.004608:   Batch Loss = 1.787946, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.783987045288086, Accuracy = 0.22830812633037567\n",
      "Iter #229376:  Learning rate = 0.004608:   Batch Loss = 1.770741, Accuracy = 0.2734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7774255275726318, Accuracy = 0.22830812633037567\n",
      "Iter #231424:  Learning rate = 0.004608:   Batch Loss = 1.807064, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7676637172698975, Accuracy = 0.22830812633037567\n",
      "Iter #233472:  Learning rate = 0.004608:   Batch Loss = 1.807026, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.769209861755371, Accuracy = 0.21144148707389832\n",
      "Iter #235520:  Learning rate = 0.004608:   Batch Loss = 1.789954, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.770715594291687, Accuracy = 0.22830812633037567\n",
      "Iter #237568:  Learning rate = 0.004608:   Batch Loss = 1.795208, Accuracy = 0.16796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.769376277923584, Accuracy = 0.22952529788017273\n",
      "Iter #239616:  Learning rate = 0.004608:   Batch Loss = 1.832429, Accuracy = 0.1640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7726411819458008, Accuracy = 0.21144148707389832\n",
      "Iter #241664:  Learning rate = 0.004608:   Batch Loss = 1.770206, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7666966915130615, Accuracy = 0.22830812633037567\n",
      "Iter #243712:  Learning rate = 0.004608:   Batch Loss = 1.779512, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.765523076057434, Accuracy = 0.22830812633037567\n",
      "Iter #245760:  Learning rate = 0.004608:   Batch Loss = 1.801321, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7648500204086304, Accuracy = 0.22952529788017273\n",
      "Iter #247808:  Learning rate = 0.004608:   Batch Loss = 1.790322, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7680915594100952, Accuracy = 0.22952529788017273\n",
      "Iter #249856:  Learning rate = 0.004608:   Batch Loss = 1.772742, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.763625979423523, Accuracy = 0.22830812633037567\n",
      "Iter #251904:  Learning rate = 0.004608:   Batch Loss = 1.767281, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7602671384811401, Accuracy = 0.22830812633037567\n",
      "Iter #253952:  Learning rate = 0.004608:   Batch Loss = 1.763936, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7607104778289795, Accuracy = 0.21144148707389832\n",
      "Iter #256000:  Learning rate = 0.004608:   Batch Loss = 1.793344, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.764324426651001, Accuracy = 0.22830812633037567\n",
      "Iter #258048:  Learning rate = 0.004608:   Batch Loss = 1.789009, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7581255435943604, Accuracy = 0.22830812633037567\n",
      "Iter #260096:  Learning rate = 0.004608:   Batch Loss = 1.758898, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7598148584365845, Accuracy = 0.22813424468040466\n",
      "Iter #262144:  Learning rate = 0.004608:   Batch Loss = 1.785566, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7566766738891602, Accuracy = 0.21144148707389832\n",
      "Iter #264192:  Learning rate = 0.004608:   Batch Loss = 1.787258, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7592076063156128, Accuracy = 0.22952529788017273\n",
      "Iter #266240:  Learning rate = 0.004608:   Batch Loss = 1.805669, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.764723300933838, Accuracy = 0.22830812633037567\n",
      "Iter #268288:  Learning rate = 0.004608:   Batch Loss = 1.761463, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7592084407806396, Accuracy = 0.22830812633037567\n",
      "Iter #270336:  Learning rate = 0.004608:   Batch Loss = 1.746022, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7597148418426514, Accuracy = 0.22952529788017273\n",
      "Iter #272384:  Learning rate = 0.004608:   Batch Loss = 1.805627, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7565879821777344, Accuracy = 0.22830812633037567\n",
      "Iter #274432:  Learning rate = 0.004608:   Batch Loss = 1.761600, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7544350624084473, Accuracy = 0.21144148707389832\n",
      "Iter #276480:  Learning rate = 0.004608:   Batch Loss = 1.777577, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7550804615020752, Accuracy = 0.22830812633037567\n",
      "Iter #278528:  Learning rate = 0.004608:   Batch Loss = 1.764942, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.754048228263855, Accuracy = 0.22830812633037567\n",
      "Iter #280576:  Learning rate = 0.004608:   Batch Loss = 1.773220, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7545419931411743, Accuracy = 0.22848200798034668\n",
      "Iter #282624:  Learning rate = 0.004608:   Batch Loss = 1.766139, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7510873079299927, Accuracy = 0.22952529788017273\n",
      "Iter #284672:  Learning rate = 0.004608:   Batch Loss = 1.763537, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.751997470855713, Accuracy = 0.22830812633037567\n",
      "Iter #286720:  Learning rate = 0.004608:   Batch Loss = 1.761742, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7532252073287964, Accuracy = 0.22830812633037567\n",
      "Iter #288768:  Learning rate = 0.004608:   Batch Loss = 1.761850, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7510346174240112, Accuracy = 0.22848200798034668\n",
      "Iter #290816:  Learning rate = 0.004608:   Batch Loss = 1.749546, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.752357006072998, Accuracy = 0.22830812633037567\n",
      "Iter #292864:  Learning rate = 0.004608:   Batch Loss = 1.759362, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.751569151878357, Accuracy = 0.22952529788017273\n",
      "Iter #294912:  Learning rate = 0.004608:   Batch Loss = 1.753534, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7522259950637817, Accuracy = 0.22952529788017273\n",
      "Iter #296960:  Learning rate = 0.004608:   Batch Loss = 1.764108, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7514976263046265, Accuracy = 0.22830812633037567\n",
      "Iter #299008:  Learning rate = 0.004608:   Batch Loss = 1.744498, Accuracy = 0.27734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.748878002166748, Accuracy = 0.22830812633037567\n",
      "Iter #301056:  Learning rate = 0.004424:   Batch Loss = 1.770119, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7477887868881226, Accuracy = 0.2290036529302597\n",
      "Iter #303104:  Learning rate = 0.004424:   Batch Loss = 1.737083, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.749039888381958, Accuracy = 0.22830812633037567\n",
      "Iter #305152:  Learning rate = 0.004424:   Batch Loss = 1.747521, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7490483522415161, Accuracy = 0.22848200798034668\n",
      "Iter #307200:  Learning rate = 0.004424:   Batch Loss = 1.775650, Accuracy = 0.15234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7439484596252441, Accuracy = 0.2672578692436218\n",
      "Iter #309248:  Learning rate = 0.004424:   Batch Loss = 1.749917, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7492671012878418, Accuracy = 0.19022779166698456\n",
      "Iter #311296:  Learning rate = 0.004424:   Batch Loss = 1.736626, Accuracy = 0.2578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7501462697982788, Accuracy = 0.22830812633037567\n",
      "Iter #313344:  Learning rate = 0.004424:   Batch Loss = 1.751387, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7489159107208252, Accuracy = 0.22830812633037567\n",
      "Iter #315392:  Learning rate = 0.004424:   Batch Loss = 1.776768, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7511883974075317, Accuracy = 0.19022779166698456\n",
      "Iter #317440:  Learning rate = 0.004424:   Batch Loss = 1.736835, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.750862717628479, Accuracy = 0.2952530086040497\n",
      "Iter #319488:  Learning rate = 0.004424:   Batch Loss = 1.773534, Accuracy = 0.1796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7472153902053833, Accuracy = 0.21144148707389832\n",
      "Iter #321536:  Learning rate = 0.004424:   Batch Loss = 1.763061, Accuracy = 0.16796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7466706037521362, Accuracy = 0.22952529788017273\n",
      "Iter #323584:  Learning rate = 0.004424:   Batch Loss = 1.761428, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7488586902618408, Accuracy = 0.22830812633037567\n",
      "Iter #325632:  Learning rate = 0.004424:   Batch Loss = 1.734963, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7444995641708374, Accuracy = 0.21144148707389832\n",
      "Iter #327680:  Learning rate = 0.004424:   Batch Loss = 1.746344, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7436615228652954, Accuracy = 0.21144148707389832\n",
      "Iter #329728:  Learning rate = 0.004424:   Batch Loss = 1.733602, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7470577955245972, Accuracy = 0.22830812633037567\n",
      "Iter #331776:  Learning rate = 0.004424:   Batch Loss = 1.799389, Accuracy = 0.171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7481104135513306, Accuracy = 0.22830812633037567\n",
      "Iter #333824:  Learning rate = 0.004424:   Batch Loss = 1.745110, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.746222972869873, Accuracy = 0.22830812633037567\n",
      "Iter #335872:  Learning rate = 0.004424:   Batch Loss = 1.746194, Accuracy = 0.1796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.743414282798767, Accuracy = 0.22952529788017273\n",
      "Iter #337920:  Learning rate = 0.004424:   Batch Loss = 1.737080, Accuracy = 0.26953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7431215047836304, Accuracy = 0.22830812633037567\n",
      "Iter #339968:  Learning rate = 0.004424:   Batch Loss = 1.745021, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7448774576187134, Accuracy = 0.22830812633037567\n",
      "Iter #342016:  Learning rate = 0.004424:   Batch Loss = 1.738597, Accuracy = 0.265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7455629110336304, Accuracy = 0.21144148707389832\n",
      "Iter #344064:  Learning rate = 0.004424:   Batch Loss = 1.738851, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7427988052368164, Accuracy = 0.21144148707389832\n",
      "Iter #346112:  Learning rate = 0.004424:   Batch Loss = 1.732228, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.743428349494934, Accuracy = 0.22830812633037567\n",
      "Iter #348160:  Learning rate = 0.004424:   Batch Loss = 1.740151, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.744311809539795, Accuracy = 0.22952529788017273\n",
      "Iter #350208:  Learning rate = 0.004424:   Batch Loss = 1.748587, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7453269958496094, Accuracy = 0.22830812633037567\n",
      "Iter #352256:  Learning rate = 0.004424:   Batch Loss = 1.743717, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7479948997497559, Accuracy = 0.21144148707389832\n",
      "Iter #354304:  Learning rate = 0.004424:   Batch Loss = 1.773902, Accuracy = 0.17578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7432284355163574, Accuracy = 0.21144148707389832\n",
      "Iter #356352:  Learning rate = 0.004424:   Batch Loss = 1.777717, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.743211269378662, Accuracy = 0.22830812633037567\n",
      "Iter #358400:  Learning rate = 0.004424:   Batch Loss = 1.752295, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7437947988510132, Accuracy = 0.22830812633037567\n",
      "Iter #360448:  Learning rate = 0.004424:   Batch Loss = 1.758560, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7418135404586792, Accuracy = 0.22830812633037567\n",
      "Iter #362496:  Learning rate = 0.004424:   Batch Loss = 1.760733, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7419641017913818, Accuracy = 0.22830812633037567\n",
      "Iter #364544:  Learning rate = 0.004424:   Batch Loss = 1.742606, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7398302555084229, Accuracy = 0.22952529788017273\n",
      "Iter #366592:  Learning rate = 0.004424:   Batch Loss = 1.738655, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7409167289733887, Accuracy = 0.22952529788017273\n",
      "Iter #368640:  Learning rate = 0.004424:   Batch Loss = 1.731417, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7417737245559692, Accuracy = 0.21144148707389832\n",
      "Iter #370688:  Learning rate = 0.004424:   Batch Loss = 1.735850, Accuracy = 0.2578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7419445514678955, Accuracy = 0.22830812633037567\n",
      "Iter #372736:  Learning rate = 0.004424:   Batch Loss = 1.792776, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7412742376327515, Accuracy = 0.22830812633037567\n",
      "Iter #374784:  Learning rate = 0.004424:   Batch Loss = 1.752674, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.739829182624817, Accuracy = 0.22952529788017273\n",
      "Iter #376832:  Learning rate = 0.004424:   Batch Loss = 1.740223, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7405664920806885, Accuracy = 0.22830812633037567\n",
      "Iter #378880:  Learning rate = 0.004424:   Batch Loss = 1.759557, Accuracy = 0.2578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.740092396736145, Accuracy = 0.22830812633037567\n",
      "Iter #380928:  Learning rate = 0.004424:   Batch Loss = 1.734534, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7401584386825562, Accuracy = 0.22830812633037567\n",
      "Iter #382976:  Learning rate = 0.004424:   Batch Loss = 1.767342, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7388625144958496, Accuracy = 0.22830812633037567\n",
      "Iter #385024:  Learning rate = 0.004424:   Batch Loss = 1.707500, Accuracy = 0.27734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7373257875442505, Accuracy = 0.22830812633037567\n",
      "Iter #387072:  Learning rate = 0.004424:   Batch Loss = 1.751843, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7382382154464722, Accuracy = 0.22952529788017273\n",
      "Iter #389120:  Learning rate = 0.004424:   Batch Loss = 1.734800, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7413095235824585, Accuracy = 0.22830812633037567\n",
      "Iter #391168:  Learning rate = 0.004424:   Batch Loss = 1.766272, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7401337623596191, Accuracy = 0.22830812633037567\n",
      "Iter #393216:  Learning rate = 0.004424:   Batch Loss = 1.772198, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.739642858505249, Accuracy = 0.22830812633037567\n",
      "Iter #395264:  Learning rate = 0.004424:   Batch Loss = 1.741755, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7404026985168457, Accuracy = 0.22830812633037567\n",
      "Iter #397312:  Learning rate = 0.004424:   Batch Loss = 1.756499, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7414003610610962, Accuracy = 0.22830812633037567\n",
      "Iter #399360:  Learning rate = 0.004424:   Batch Loss = 1.732504, Accuracy = 0.28515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.739660620689392, Accuracy = 0.22830812633037567\n",
      "Iter #401408:  Learning rate = 0.004247:   Batch Loss = 1.774601, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7363994121551514, Accuracy = 0.22830812633037567\n",
      "Iter #403456:  Learning rate = 0.004247:   Batch Loss = 1.784236, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7352982759475708, Accuracy = 0.22830812633037567\n",
      "Iter #405504:  Learning rate = 0.004247:   Batch Loss = 1.762218, Accuracy = 0.16015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7361704111099243, Accuracy = 0.22796034812927246\n",
      "Iter #407552:  Learning rate = 0.004247:   Batch Loss = 1.753185, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7380621433258057, Accuracy = 0.22830812633037567\n",
      "Iter #409600:  Learning rate = 0.004247:   Batch Loss = 1.741461, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.740793228149414, Accuracy = 0.22830812633037567\n",
      "Iter #411648:  Learning rate = 0.004247:   Batch Loss = 1.755728, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7400840520858765, Accuracy = 0.22830812633037567\n",
      "Iter #413696:  Learning rate = 0.004247:   Batch Loss = 1.753173, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.738748550415039, Accuracy = 0.22830812633037567\n",
      "Iter #415744:  Learning rate = 0.004247:   Batch Loss = 1.772599, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7370340824127197, Accuracy = 0.22830812633037567\n",
      "Iter #417792:  Learning rate = 0.004247:   Batch Loss = 1.734307, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7357077598571777, Accuracy = 0.22830812633037567\n",
      "Iter #419840:  Learning rate = 0.004247:   Batch Loss = 1.721592, Accuracy = 0.2734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7351934909820557, Accuracy = 0.22830812633037567\n",
      "Iter #421888:  Learning rate = 0.004247:   Batch Loss = 1.727417, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.735161542892456, Accuracy = 0.22830812633037567\n",
      "Iter #423936:  Learning rate = 0.004247:   Batch Loss = 1.748000, Accuracy = 0.16796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7363662719726562, Accuracy = 0.22830812633037567\n",
      "Iter #425984:  Learning rate = 0.004247:   Batch Loss = 1.732466, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7375985383987427, Accuracy = 0.22952529788017273\n",
      "Iter #428032:  Learning rate = 0.004247:   Batch Loss = 1.766487, Accuracy = 0.16796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7370625734329224, Accuracy = 0.22952529788017273\n",
      "Iter #430080:  Learning rate = 0.004247:   Batch Loss = 1.723816, Accuracy = 0.17578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7364102602005005, Accuracy = 0.23178577423095703\n",
      "Iter #432128:  Learning rate = 0.004247:   Batch Loss = 1.724667, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7356773614883423, Accuracy = 0.2112676054239273\n",
      "Iter #434176:  Learning rate = 0.004247:   Batch Loss = 1.762678, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7365269660949707, Accuracy = 0.22830812633037567\n",
      "Iter #436224:  Learning rate = 0.004247:   Batch Loss = 1.753108, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7360141277313232, Accuracy = 0.231264129281044\n",
      "Iter #438272:  Learning rate = 0.004247:   Batch Loss = 1.751107, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.735872507095337, Accuracy = 0.231090247631073\n",
      "Iter #440320:  Learning rate = 0.004247:   Batch Loss = 1.724965, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7345179319381714, Accuracy = 0.22830812633037567\n",
      "Iter #442368:  Learning rate = 0.004247:   Batch Loss = 1.742452, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7346082925796509, Accuracy = 0.22830812633037567\n",
      "Iter #444416:  Learning rate = 0.004247:   Batch Loss = 1.742887, Accuracy = 0.17578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7359126806259155, Accuracy = 0.22830812633037567\n",
      "Iter #446464:  Learning rate = 0.004247:   Batch Loss = 1.745362, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7379628419876099, Accuracy = 0.22830812633037567\n",
      "Iter #448512:  Learning rate = 0.004247:   Batch Loss = 1.776586, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737120270729065, Accuracy = 0.22830812633037567\n",
      "Iter #450560:  Learning rate = 0.004247:   Batch Loss = 1.750326, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.738676905632019, Accuracy = 0.22830812633037567\n",
      "Iter #452608:  Learning rate = 0.004247:   Batch Loss = 1.757528, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7386882305145264, Accuracy = 0.22952529788017273\n",
      "Iter #454656:  Learning rate = 0.004247:   Batch Loss = 1.743056, Accuracy = 0.17578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7353607416152954, Accuracy = 0.22952529788017273\n",
      "Iter #456704:  Learning rate = 0.004247:   Batch Loss = 1.701526, Accuracy = 0.26953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7335984706878662, Accuracy = 0.22830812633037567\n",
      "Iter #458752:  Learning rate = 0.004247:   Batch Loss = 1.759076, Accuracy = 0.27734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.736352801322937, Accuracy = 0.22830812633037567\n",
      "Iter #460800:  Learning rate = 0.004247:   Batch Loss = 1.727762, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737594723701477, Accuracy = 0.22830812633037567\n",
      "Iter #462848:  Learning rate = 0.004247:   Batch Loss = 1.755174, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737053394317627, Accuracy = 0.21144148707389832\n",
      "Iter #464896:  Learning rate = 0.004247:   Batch Loss = 1.789320, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7367993593215942, Accuracy = 0.22830812633037567\n",
      "Iter #466944:  Learning rate = 0.004247:   Batch Loss = 1.780021, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.739492416381836, Accuracy = 0.22830812633037567\n",
      "Iter #468992:  Learning rate = 0.004247:   Batch Loss = 1.759940, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737645149230957, Accuracy = 0.22830812633037567\n",
      "Iter #471040:  Learning rate = 0.004247:   Batch Loss = 1.766485, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7364224195480347, Accuracy = 0.22830812633037567\n",
      "Iter #473088:  Learning rate = 0.004247:   Batch Loss = 1.740427, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7364691495895386, Accuracy = 0.22830812633037567\n",
      "Iter #475136:  Learning rate = 0.004247:   Batch Loss = 1.748314, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7353911399841309, Accuracy = 0.22830812633037567\n",
      "Iter #477184:  Learning rate = 0.004247:   Batch Loss = 1.733947, Accuracy = 0.27734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7357290983200073, Accuracy = 0.22830812633037567\n",
      "Iter #479232:  Learning rate = 0.004247:   Batch Loss = 1.750649, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.736026644706726, Accuracy = 0.22830812633037567\n",
      "Iter #481280:  Learning rate = 0.004247:   Batch Loss = 1.745659, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.735790491104126, Accuracy = 0.22830812633037567\n",
      "Iter #483328:  Learning rate = 0.004247:   Batch Loss = 1.734538, Accuracy = 0.26171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7370655536651611, Accuracy = 0.22830812633037567\n",
      "Iter #485376:  Learning rate = 0.004247:   Batch Loss = 1.770901, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7374396324157715, Accuracy = 0.22830812633037567\n",
      "Iter #487424:  Learning rate = 0.004247:   Batch Loss = 1.758085, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7363574504852295, Accuracy = 0.22830812633037567\n",
      "Iter #489472:  Learning rate = 0.004247:   Batch Loss = 1.739432, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7355984449386597, Accuracy = 0.22830812633037567\n",
      "Iter #491520:  Learning rate = 0.004247:   Batch Loss = 1.741093, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7353729009628296, Accuracy = 0.22830812633037567\n",
      "Iter #493568:  Learning rate = 0.004247:   Batch Loss = 1.705019, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7347310781478882, Accuracy = 0.22830812633037567\n",
      "Iter #495616:  Learning rate = 0.004247:   Batch Loss = 1.762027, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.734533667564392, Accuracy = 0.22830812633037567\n",
      "Iter #497664:  Learning rate = 0.004247:   Batch Loss = 1.744040, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.734615683555603, Accuracy = 0.22969917953014374\n",
      "Iter #499712:  Learning rate = 0.004247:   Batch Loss = 1.741272, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7339009046554565, Accuracy = 0.22830812633037567\n",
      "Iter #501760:  Learning rate = 0.004077:   Batch Loss = 1.707698, Accuracy = 0.26953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7344138622283936, Accuracy = 0.22830812633037567\n",
      "Iter #503808:  Learning rate = 0.004077:   Batch Loss = 1.735698, Accuracy = 0.2578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.734961748123169, Accuracy = 0.22830812633037567\n",
      "Iter #505856:  Learning rate = 0.004077:   Batch Loss = 1.736370, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7358448505401611, Accuracy = 0.21144148707389832\n",
      "Iter #507904:  Learning rate = 0.004077:   Batch Loss = 1.762549, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7352566719055176, Accuracy = 0.22952529788017273\n",
      "Iter #509952:  Learning rate = 0.004077:   Batch Loss = 1.746936, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7350207567214966, Accuracy = 0.22952529788017273\n",
      "Iter #512000:  Learning rate = 0.004077:   Batch Loss = 1.741769, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.735779881477356, Accuracy = 0.21144148707389832\n",
      "Iter #514048:  Learning rate = 0.004077:   Batch Loss = 1.752175, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7358498573303223, Accuracy = 0.22830812633037567\n",
      "Iter #516096:  Learning rate = 0.004077:   Batch Loss = 1.718559, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7363286018371582, Accuracy = 0.22830812633037567\n",
      "Iter #518144:  Learning rate = 0.004077:   Batch Loss = 1.754797, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7349185943603516, Accuracy = 0.22830812633037567\n",
      "Iter #520192:  Learning rate = 0.004077:   Batch Loss = 1.778891, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7351511716842651, Accuracy = 0.22830812633037567\n",
      "Iter #522240:  Learning rate = 0.004077:   Batch Loss = 1.748578, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7368226051330566, Accuracy = 0.22952529788017273\n",
      "Iter #524288:  Learning rate = 0.004077:   Batch Loss = 1.766136, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7359001636505127, Accuracy = 0.22830812633037567\n",
      "Iter #526336:  Learning rate = 0.004077:   Batch Loss = 1.765335, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7338054180145264, Accuracy = 0.22830812633037567\n",
      "Iter #528384:  Learning rate = 0.004077:   Batch Loss = 1.770219, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.732926845550537, Accuracy = 0.22830812633037567\n",
      "Iter #530432:  Learning rate = 0.004077:   Batch Loss = 1.750169, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737817645072937, Accuracy = 0.22830812633037567\n",
      "Iter #532480:  Learning rate = 0.004077:   Batch Loss = 1.732017, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737088680267334, Accuracy = 0.21144148707389832\n",
      "Iter #534528:  Learning rate = 0.004077:   Batch Loss = 1.745244, Accuracy = 0.28125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7336938381195068, Accuracy = 0.22830812633037567\n",
      "Iter #536576:  Learning rate = 0.004077:   Batch Loss = 1.714149, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7339335680007935, Accuracy = 0.22830812633037567\n",
      "Iter #538624:  Learning rate = 0.004077:   Batch Loss = 1.739549, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7340058088302612, Accuracy = 0.22830812633037567\n",
      "Iter #540672:  Learning rate = 0.004077:   Batch Loss = 1.720983, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7344058752059937, Accuracy = 0.21144148707389832\n",
      "Iter #542720:  Learning rate = 0.004077:   Batch Loss = 1.734645, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7349836826324463, Accuracy = 0.22830812633037567\n",
      "Iter #544768:  Learning rate = 0.004077:   Batch Loss = 1.765250, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7354596853256226, Accuracy = 0.22830812633037567\n",
      "Iter #546816:  Learning rate = 0.004077:   Batch Loss = 1.731088, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7339274883270264, Accuracy = 0.21144148707389832\n",
      "Iter #548864:  Learning rate = 0.004077:   Batch Loss = 1.734778, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7340574264526367, Accuracy = 0.22952529788017273\n",
      "Iter #550912:  Learning rate = 0.004077:   Batch Loss = 1.755184, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.733225703239441, Accuracy = 0.22830812633037567\n",
      "Iter #552960:  Learning rate = 0.004077:   Batch Loss = 1.739924, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7342628240585327, Accuracy = 0.21144148707389832\n",
      "Iter #555008:  Learning rate = 0.004077:   Batch Loss = 1.758093, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7348599433898926, Accuracy = 0.21144148707389832\n",
      "Iter #557056:  Learning rate = 0.004077:   Batch Loss = 1.756064, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7331165075302124, Accuracy = 0.22830812633037567\n",
      "Iter #559104:  Learning rate = 0.004077:   Batch Loss = 1.785202, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7342098951339722, Accuracy = 0.22830812633037567\n",
      "Iter #561152:  Learning rate = 0.004077:   Batch Loss = 1.763544, Accuracy = 0.26953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7349865436553955, Accuracy = 0.22830812633037567\n",
      "Iter #563200:  Learning rate = 0.004077:   Batch Loss = 1.736877, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.735373616218567, Accuracy = 0.22830812633037567\n",
      "Iter #565248:  Learning rate = 0.004077:   Batch Loss = 1.721719, Accuracy = 0.26171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7332149744033813, Accuracy = 0.22830812633037567\n",
      "Iter #567296:  Learning rate = 0.004077:   Batch Loss = 1.714599, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.731890320777893, Accuracy = 0.22830812633037567\n",
      "Iter #569344:  Learning rate = 0.004077:   Batch Loss = 1.748480, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7330132722854614, Accuracy = 0.21144148707389832\n",
      "Iter #571392:  Learning rate = 0.004077:   Batch Loss = 1.750254, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.732706069946289, Accuracy = 0.22830812633037567\n",
      "Iter #573440:  Learning rate = 0.004077:   Batch Loss = 1.772475, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7366610765457153, Accuracy = 0.22830812633037567\n",
      "Iter #575488:  Learning rate = 0.004077:   Batch Loss = 1.736316, Accuracy = 0.234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7355225086212158, Accuracy = 0.22830812633037567\n",
      "Iter #577536:  Learning rate = 0.004077:   Batch Loss = 1.750495, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7346800565719604, Accuracy = 0.22830812633037567\n",
      "Iter #579584:  Learning rate = 0.004077:   Batch Loss = 1.747167, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7339122295379639, Accuracy = 0.22830812633037567\n",
      "Iter #581632:  Learning rate = 0.004077:   Batch Loss = 1.759751, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.734298825263977, Accuracy = 0.22830812633037567\n",
      "Iter #583680:  Learning rate = 0.004077:   Batch Loss = 1.734090, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7327922582626343, Accuracy = 0.22830812633037567\n",
      "Iter #585728:  Learning rate = 0.004077:   Batch Loss = 1.751555, Accuracy = 0.18359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7350600957870483, Accuracy = 0.21144148707389832\n",
      "Iter #587776:  Learning rate = 0.004077:   Batch Loss = 1.734669, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7331104278564453, Accuracy = 0.22952529788017273\n",
      "Iter #589824:  Learning rate = 0.004077:   Batch Loss = 1.728055, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7337088584899902, Accuracy = 0.22952529788017273\n",
      "Iter #591872:  Learning rate = 0.004077:   Batch Loss = 1.770799, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7338340282440186, Accuracy = 0.22830812633037567\n",
      "Iter #593920:  Learning rate = 0.004077:   Batch Loss = 1.732157, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7342381477355957, Accuracy = 0.22830812633037567\n",
      "Iter #595968:  Learning rate = 0.004077:   Batch Loss = 1.737295, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7328640222549438, Accuracy = 0.22987306118011475\n",
      "Iter #598016:  Learning rate = 0.004077:   Batch Loss = 1.740435, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7313945293426514, Accuracy = 0.22830812633037567\n",
      "Iter #600064:  Learning rate = 0.003914:   Batch Loss = 1.781443, Accuracy = 0.1640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7328715324401855, Accuracy = 0.22830812633037567\n",
      "Iter #602112:  Learning rate = 0.003914:   Batch Loss = 1.751267, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7344210147857666, Accuracy = 0.22830812633037567\n",
      "Iter #604160:  Learning rate = 0.003914:   Batch Loss = 1.772266, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.735925316810608, Accuracy = 0.22830812633037567\n",
      "Iter #606208:  Learning rate = 0.003914:   Batch Loss = 1.749225, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.73472261428833, Accuracy = 0.22830812633037567\n",
      "Iter #608256:  Learning rate = 0.003914:   Batch Loss = 1.745560, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7323781251907349, Accuracy = 0.22830812633037567\n",
      "Iter #610304:  Learning rate = 0.003914:   Batch Loss = 1.716628, Accuracy = 0.3046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7321271896362305, Accuracy = 0.22830812633037567\n",
      "Iter #612352:  Learning rate = 0.003914:   Batch Loss = 1.763119, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7312923669815063, Accuracy = 0.22830812633037567\n",
      "Iter #614400:  Learning rate = 0.003914:   Batch Loss = 1.719249, Accuracy = 0.24609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7327107191085815, Accuracy = 0.22830812633037567\n",
      "Iter #616448:  Learning rate = 0.003914:   Batch Loss = 1.742431, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7348474264144897, Accuracy = 0.21144148707389832\n",
      "Iter #618496:  Learning rate = 0.003914:   Batch Loss = 1.727013, Accuracy = 0.2890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7313183546066284, Accuracy = 0.22830812633037567\n",
      "Iter #620544:  Learning rate = 0.003914:   Batch Loss = 1.753093, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7329412698745728, Accuracy = 0.22830812633037567\n",
      "Iter #622592:  Learning rate = 0.003914:   Batch Loss = 1.750055, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7354799509048462, Accuracy = 0.22830812633037567\n",
      "Iter #624640:  Learning rate = 0.003914:   Batch Loss = 1.779323, Accuracy = 0.19921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.734856128692627, Accuracy = 0.22830812633037567\n",
      "Iter #626688:  Learning rate = 0.003914:   Batch Loss = 1.744418, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7338486909866333, Accuracy = 0.22830812633037567\n",
      "Iter #628736:  Learning rate = 0.003914:   Batch Loss = 1.772981, Accuracy = 0.16015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7333451509475708, Accuracy = 0.22830812633037567\n",
      "Iter #630784:  Learning rate = 0.003914:   Batch Loss = 1.753641, Accuracy = 0.1875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7328108549118042, Accuracy = 0.21144148707389832\n",
      "Iter #632832:  Learning rate = 0.003914:   Batch Loss = 1.721314, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7333904504776, Accuracy = 0.21144148707389832\n",
      "Iter #634880:  Learning rate = 0.003914:   Batch Loss = 1.721189, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7343966960906982, Accuracy = 0.21144148707389832\n",
      "Iter #636928:  Learning rate = 0.003914:   Batch Loss = 1.717358, Accuracy = 0.21875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.733656883239746, Accuracy = 0.22830812633037567\n",
      "Iter #638976:  Learning rate = 0.003914:   Batch Loss = 1.765206, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7318257093429565, Accuracy = 0.22830812633037567\n",
      "Iter #641024:  Learning rate = 0.003914:   Batch Loss = 1.747886, Accuracy = 0.171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7317367792129517, Accuracy = 0.22952529788017273\n",
      "Iter #643072:  Learning rate = 0.003914:   Batch Loss = 1.761207, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.732153058052063, Accuracy = 0.22830812633037567\n",
      "Iter #645120:  Learning rate = 0.003914:   Batch Loss = 1.743893, Accuracy = 0.25\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7323719263076782, Accuracy = 0.22830812633037567\n",
      "Iter #647168:  Learning rate = 0.003914:   Batch Loss = 1.744850, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7331420183181763, Accuracy = 0.22830812633037567\n",
      "Iter #649216:  Learning rate = 0.003914:   Batch Loss = 1.716275, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7341889142990112, Accuracy = 0.22830812633037567\n",
      "Iter #651264:  Learning rate = 0.003914:   Batch Loss = 1.772685, Accuracy = 0.20703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7347148656845093, Accuracy = 0.22830812633037567\n",
      "Iter #653312:  Learning rate = 0.003914:   Batch Loss = 1.747225, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7345337867736816, Accuracy = 0.22830812633037567\n",
      "Iter #655360:  Learning rate = 0.003914:   Batch Loss = 1.750982, Accuracy = 0.2265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7340545654296875, Accuracy = 0.22830812633037567\n",
      "Iter #657408:  Learning rate = 0.003914:   Batch Loss = 1.747653, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7331002950668335, Accuracy = 0.22830812633037567\n",
      "Iter #659456:  Learning rate = 0.003914:   Batch Loss = 1.769964, Accuracy = 0.23828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7337654829025269, Accuracy = 0.22830812633037567\n",
      "Iter #661504:  Learning rate = 0.003914:   Batch Loss = 1.754446, Accuracy = 0.2109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7342149019241333, Accuracy = 0.22830812633037567\n",
      "Iter #663552:  Learning rate = 0.003914:   Batch Loss = 1.735786, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7322332859039307, Accuracy = 0.22830812633037567\n",
      "Iter #665600:  Learning rate = 0.003914:   Batch Loss = 1.753894, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7316625118255615, Accuracy = 0.22830812633037567\n",
      "Iter #667648:  Learning rate = 0.003914:   Batch Loss = 1.761144, Accuracy = 0.1953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7312567234039307, Accuracy = 0.22830812633037567\n",
      "Iter #669696:  Learning rate = 0.003914:   Batch Loss = 1.722942, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7316322326660156, Accuracy = 0.22830812633037567\n",
      "Iter #671744:  Learning rate = 0.003914:   Batch Loss = 1.709527, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7323031425476074, Accuracy = 0.22830812633037567\n",
      "Iter #673792:  Learning rate = 0.003914:   Batch Loss = 1.763626, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7331151962280273, Accuracy = 0.22830812633037567\n",
      "Iter #675840:  Learning rate = 0.003914:   Batch Loss = 1.723592, Accuracy = 0.265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7331446409225464, Accuracy = 0.22830812633037567\n",
      "Iter #677888:  Learning rate = 0.003914:   Batch Loss = 1.746882, Accuracy = 0.19140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7328299283981323, Accuracy = 0.22830812633037567\n",
      "Iter #679936:  Learning rate = 0.003914:   Batch Loss = 1.732131, Accuracy = 0.25390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7321163415908813, Accuracy = 0.22830812633037567\n",
      "Iter #681984:  Learning rate = 0.003914:   Batch Loss = 1.716867, Accuracy = 0.21484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7320057153701782, Accuracy = 0.22848200798034668\n",
      "Iter #684032:  Learning rate = 0.003914:   Batch Loss = 1.722872, Accuracy = 0.2421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7321751117706299, Accuracy = 0.22830812633037567\n",
      "Iter #686080:  Learning rate = 0.003914:   Batch Loss = 1.745777, Accuracy = 0.22265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7328606843948364, Accuracy = 0.22848200798034668\n",
      "Iter #688128:  Learning rate = 0.003914:   Batch Loss = 1.740269, Accuracy = 0.23046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.732195496559143, Accuracy = 0.22830812633037567\n",
      "Iter #690176:  Learning rate = 0.003914:   Batch Loss = 1.747725, Accuracy = 0.203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7307379245758057, Accuracy = 0.23022082448005676\n",
      "Iter #692224:  Learning rate = 0.003914:   Batch Loss = 1.725105, Accuracy = 0.30078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7263613939285278, Accuracy = 0.26308467984199524\n",
      "Iter #694272:  Learning rate = 0.003914:   Batch Loss = 1.678802, Accuracy = 0.31640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6919214725494385, Accuracy = 0.3022083044052124\n",
      "Iter #696320:  Learning rate = 0.003914:   Batch Loss = 1.718702, Accuracy = 0.296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7153867483139038, Accuracy = 0.3114241063594818\n",
      "Iter #698368:  Learning rate = 0.003914:   Batch Loss = 1.705439, Accuracy = 0.26171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6997822523117065, Accuracy = 0.3128151595592499\n",
      "Iter #700416:  Learning rate = 0.003757:   Batch Loss = 1.661444, Accuracy = 0.30859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6848500967025757, Accuracy = 0.35680750012397766\n",
      "Iter #702464:  Learning rate = 0.003757:   Batch Loss = 1.670239, Accuracy = 0.2890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.783146858215332, Accuracy = 0.208833247423172\n",
      "Iter #704512:  Learning rate = 0.003757:   Batch Loss = 1.675487, Accuracy = 0.31640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7276328802108765, Accuracy = 0.2907320559024811\n",
      "Iter #706560:  Learning rate = 0.003757:   Batch Loss = 1.617258, Accuracy = 0.37109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6600234508514404, Accuracy = 0.36584940552711487\n",
      "Iter #708608:  Learning rate = 0.003757:   Batch Loss = 1.765343, Accuracy = 0.29296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7690260410308838, Accuracy = 0.27490872144699097\n",
      "Iter #710656:  Learning rate = 0.003757:   Batch Loss = 1.559679, Accuracy = 0.42578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6024812459945679, Accuracy = 0.4115805923938751\n",
      "Iter #712704:  Learning rate = 0.003757:   Batch Loss = 1.565597, Accuracy = 0.33984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5415782928466797, Accuracy = 0.3894974887371063\n",
      "Iter #714752:  Learning rate = 0.003757:   Batch Loss = 1.480051, Accuracy = 0.44921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4774692058563232, Accuracy = 0.42323073744773865\n",
      "Iter #716800:  Learning rate = 0.003757:   Batch Loss = 1.473922, Accuracy = 0.484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4257864952087402, Accuracy = 0.4661797881126404\n",
      "Iter #718848:  Learning rate = 0.003757:   Batch Loss = 1.429807, Accuracy = 0.4296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3307652473449707, Accuracy = 0.5106937885284424\n",
      "Iter #720896:  Learning rate = 0.003757:   Batch Loss = 1.543375, Accuracy = 0.40234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4394906759262085, Accuracy = 0.4383585453033447\n",
      "Iter #722944:  Learning rate = 0.003757:   Batch Loss = 2.167925, Accuracy = 0.30078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.310726523399353, Accuracy = 0.4776560664176941\n",
      "Iter #724992:  Learning rate = 0.003757:   Batch Loss = 1.321021, Accuracy = 0.46484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3248097896575928, Accuracy = 0.4886106848716736\n",
      "Iter #727040:  Learning rate = 0.003757:   Batch Loss = 1.259470, Accuracy = 0.52734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2059086561203003, Accuracy = 0.5202573537826538\n",
      "Iter #729088:  Learning rate = 0.003757:   Batch Loss = 1.398615, Accuracy = 0.515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7602858543395996, Accuracy = 0.3352460563182831\n",
      "Iter #731136:  Learning rate = 0.003757:   Batch Loss = 1.453010, Accuracy = 0.47265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3085362911224365, Accuracy = 0.49469658732414246\n",
      "Iter #733184:  Learning rate = 0.003757:   Batch Loss = 1.346645, Accuracy = 0.48828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2244566679000854, Accuracy = 0.5482524633407593\n",
      "Iter #735232:  Learning rate = 0.003757:   Batch Loss = 1.254806, Accuracy = 0.51171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2406443357467651, Accuracy = 0.5529473423957825\n",
      "Iter #737280:  Learning rate = 0.003757:   Batch Loss = 1.261037, Accuracy = 0.5390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.155094027519226, Accuracy = 0.5659885406494141\n",
      "Iter #739328:  Learning rate = 0.003757:   Batch Loss = 1.127499, Accuracy = 0.59765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.084235429763794, Accuracy = 0.6063293218612671\n",
      "Iter #741376:  Learning rate = 0.003757:   Batch Loss = 1.085049, Accuracy = 0.58203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0733368396759033, Accuracy = 0.5701617002487183\n",
      "Iter #743424:  Learning rate = 0.003757:   Batch Loss = 1.019994, Accuracy = 0.625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9799668788909912, Accuracy = 0.6313684582710266\n",
      "Iter #745472:  Learning rate = 0.003757:   Batch Loss = 0.944673, Accuracy = 0.62890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0536699295043945, Accuracy = 0.6056337952613831\n",
      "Iter #747520:  Learning rate = 0.003757:   Batch Loss = 0.962521, Accuracy = 0.67578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9245238900184631, Accuracy = 0.6362372040748596\n",
      "Iter #749568:  Learning rate = 0.003757:   Batch Loss = 1.081402, Accuracy = 0.609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9935861825942993, Accuracy = 0.612762987613678\n",
      "Iter #751616:  Learning rate = 0.003757:   Batch Loss = 0.946906, Accuracy = 0.66015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.122454285621643, Accuracy = 0.5955486297607422\n",
      "Iter #753664:  Learning rate = 0.003757:   Batch Loss = 0.868284, Accuracy = 0.69140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0071508884429932, Accuracy = 0.6052860617637634\n",
      "Iter #755712:  Learning rate = 0.003757:   Batch Loss = 0.888923, Accuracy = 0.6875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8662363290786743, Accuracy = 0.6821422576904297\n",
      "Iter #757760:  Learning rate = 0.003757:   Batch Loss = 0.925343, Accuracy = 0.67578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8620177507400513, Accuracy = 0.6864892840385437\n",
      "Iter #759808:  Learning rate = 0.003757:   Batch Loss = 0.748337, Accuracy = 0.73046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8188610672950745, Accuracy = 0.6965745091438293\n",
      "Iter #761856:  Learning rate = 0.003757:   Batch Loss = 0.813477, Accuracy = 0.70703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8781330585479736, Accuracy = 0.6616240739822388\n",
      "Iter #763904:  Learning rate = 0.003757:   Batch Loss = 0.831702, Accuracy = 0.6953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7635606527328491, Accuracy = 0.7261345982551575\n",
      "Iter #765952:  Learning rate = 0.003757:   Batch Loss = 0.745469, Accuracy = 0.734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7962243556976318, Accuracy = 0.7179620862007141\n",
      "Iter #768000:  Learning rate = 0.003757:   Batch Loss = 0.767655, Accuracy = 0.72265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7509626150131226, Accuracy = 0.7436967492103577\n",
      "Iter #770048:  Learning rate = 0.003757:   Batch Loss = 0.666924, Accuracy = 0.7890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8136453628540039, Accuracy = 0.7096157073974609\n",
      "Iter #772096:  Learning rate = 0.003757:   Batch Loss = 0.800140, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8582611083984375, Accuracy = 0.6852720975875854\n",
      "Iter #774144:  Learning rate = 0.003757:   Batch Loss = 0.777921, Accuracy = 0.72265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8626400828361511, Accuracy = 0.6941401362419128\n",
      "Iter #776192:  Learning rate = 0.003757:   Batch Loss = 0.707801, Accuracy = 0.7578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7640126943588257, Accuracy = 0.7247435450553894\n",
      "Iter #778240:  Learning rate = 0.003757:   Batch Loss = 0.786023, Accuracy = 0.71484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7231199145317078, Accuracy = 0.7537819743156433\n",
      "Iter #780288:  Learning rate = 0.003757:   Batch Loss = 0.634289, Accuracy = 0.78125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7208147048950195, Accuracy = 0.7452616691589355\n",
      "Iter #782336:  Learning rate = 0.003757:   Batch Loss = 0.680546, Accuracy = 0.76171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7016149759292603, Accuracy = 0.7419579029083252\n",
      "Iter #784384:  Learning rate = 0.003757:   Batch Loss = 0.655454, Accuracy = 0.7890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6930867433547974, Accuracy = 0.7503042817115784\n",
      "Iter #786432:  Learning rate = 0.003757:   Batch Loss = 0.557966, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7938597202301025, Accuracy = 0.716571033000946\n",
      "Iter #788480:  Learning rate = 0.003757:   Batch Loss = 0.764768, Accuracy = 0.72265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7480619549751282, Accuracy = 0.715875506401062\n",
      "Iter #790528:  Learning rate = 0.003757:   Batch Loss = 0.645909, Accuracy = 0.765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7161954641342163, Accuracy = 0.7583029270172119\n",
      "Iter #792576:  Learning rate = 0.003757:   Batch Loss = 0.640144, Accuracy = 0.80078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7696154713630676, Accuracy = 0.7224830389022827\n",
      "Iter #794624:  Learning rate = 0.003757:   Batch Loss = 0.699194, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.670627236366272, Accuracy = 0.7748217582702637\n",
      "Iter #796672:  Learning rate = 0.003757:   Batch Loss = 0.670430, Accuracy = 0.7421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6852302551269531, Accuracy = 0.7456094622612\n",
      "Iter #798720:  Learning rate = 0.003757:   Batch Loss = 0.604110, Accuracy = 0.80078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6528435945510864, Accuracy = 0.77377849817276\n",
      "Iter #800768:  Learning rate = 0.003607:   Batch Loss = 0.520307, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6676577925682068, Accuracy = 0.77377849817276\n",
      "Iter #802816:  Learning rate = 0.003607:   Batch Loss = 0.724422, Accuracy = 0.7578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7317153215408325, Accuracy = 0.7353503704071045\n",
      "Iter #804864:  Learning rate = 0.003607:   Batch Loss = 0.636928, Accuracy = 0.7890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6494032144546509, Accuracy = 0.778125524520874\n",
      "Iter #806912:  Learning rate = 0.003607:   Batch Loss = 0.582433, Accuracy = 0.8046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6751121878623962, Accuracy = 0.7553468942642212\n",
      "Iter #808960:  Learning rate = 0.003607:   Batch Loss = 0.622924, Accuracy = 0.79296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6192346215248108, Accuracy = 0.7904711961746216\n",
      "Iter #811008:  Learning rate = 0.003607:   Batch Loss = 0.598256, Accuracy = 0.81640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6703278422355652, Accuracy = 0.7591723203659058\n",
      "Iter #813056:  Learning rate = 0.003607:   Batch Loss = 0.632877, Accuracy = 0.80078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7491680979728699, Accuracy = 0.7238740921020508\n",
      "Iter #815104:  Learning rate = 0.003607:   Batch Loss = 0.578446, Accuracy = 0.796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8343431353569031, Accuracy = 0.692575216293335\n",
      "Iter #817152:  Learning rate = 0.003607:   Batch Loss = 0.791914, Accuracy = 0.734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.692897617816925, Accuracy = 0.7496087551116943\n",
      "Iter #819200:  Learning rate = 0.003607:   Batch Loss = 0.577135, Accuracy = 0.81640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6272791028022766, Accuracy = 0.7875152230262756\n",
      "Iter #821248:  Learning rate = 0.003607:   Batch Loss = 0.537562, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6674889922142029, Accuracy = 0.7647365927696228\n",
      "Iter #823296:  Learning rate = 0.003607:   Batch Loss = 0.559661, Accuracy = 0.83984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6336403489112854, Accuracy = 0.7751695513725281\n",
      "Iter #825344:  Learning rate = 0.003607:   Batch Loss = 0.774936, Accuracy = 0.71484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.62675541639328, Accuracy = 0.7854286432266235\n",
      "Iter #827392:  Learning rate = 0.003607:   Batch Loss = 0.542901, Accuracy = 0.8515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6231694221496582, Accuracy = 0.7929055690765381\n",
      "Iter #829440:  Learning rate = 0.003607:   Batch Loss = 0.671247, Accuracy = 0.77734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6451835632324219, Accuracy = 0.7800382375717163\n",
      "Iter #831488:  Learning rate = 0.003607:   Batch Loss = 0.549826, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5757230520248413, Accuracy = 0.8155103325843811\n",
      "Iter #833536:  Learning rate = 0.003607:   Batch Loss = 0.658612, Accuracy = 0.765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6141098141670227, Accuracy = 0.7929055690765381\n",
      "Iter #835584:  Learning rate = 0.003607:   Batch Loss = 0.596016, Accuracy = 0.77734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6136618852615356, Accuracy = 0.7920361757278442\n",
      "Iter #837632:  Learning rate = 0.003607:   Batch Loss = 0.555907, Accuracy = 0.828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6085610389709473, Accuracy = 0.7847331166267395\n",
      "Iter #839680:  Learning rate = 0.003607:   Batch Loss = 0.578451, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6222679018974304, Accuracy = 0.790818989276886\n",
      "Iter #841728:  Learning rate = 0.003607:   Batch Loss = 0.539815, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5916578769683838, Accuracy = 0.7986437082290649\n",
      "Iter #843776:  Learning rate = 0.003607:   Batch Loss = 0.598445, Accuracy = 0.81640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5797094702720642, Accuracy = 0.8101199865341187\n",
      "Iter #845824:  Learning rate = 0.003607:   Batch Loss = 0.570503, Accuracy = 0.796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7102147936820984, Accuracy = 0.7659537196159363\n",
      "Iter #847872:  Learning rate = 0.003607:   Batch Loss = 0.544018, Accuracy = 0.8359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5856803059577942, Accuracy = 0.8036863207817078\n",
      "Iter #849920:  Learning rate = 0.003607:   Batch Loss = 0.498020, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6184918284416199, Accuracy = 0.7897756695747375\n",
      "Iter #851968:  Learning rate = 0.003607:   Batch Loss = 0.562580, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6334434747695923, Accuracy = 0.7824726104736328\n",
      "Iter #854016:  Learning rate = 0.003607:   Batch Loss = 0.437690, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6154506802558899, Accuracy = 0.7998608946800232\n",
      "Iter #856064:  Learning rate = 0.003607:   Batch Loss = 0.672741, Accuracy = 0.7578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5610382556915283, Accuracy = 0.8095983266830444\n",
      "Iter #858112:  Learning rate = 0.003607:   Batch Loss = 0.518723, Accuracy = 0.83984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6086539030075073, Accuracy = 0.7906451225280762\n",
      "Iter #860160:  Learning rate = 0.003607:   Batch Loss = 0.658450, Accuracy = 0.7890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.61878502368927, Accuracy = 0.7889062762260437\n",
      "Iter #862208:  Learning rate = 0.003607:   Batch Loss = 0.643654, Accuracy = 0.76171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6803198456764221, Accuracy = 0.7609111666679382\n",
      "Iter #864256:  Learning rate = 0.003607:   Batch Loss = 0.657532, Accuracy = 0.765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5947147607803345, Accuracy = 0.8061206936836243\n",
      "Iter #866304:  Learning rate = 0.003607:   Batch Loss = 1.838802, Accuracy = 0.59765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1557397842407227, Accuracy = 0.6464962363243103\n",
      "Iter #868352:  Learning rate = 0.003607:   Batch Loss = 0.968009, Accuracy = 0.64453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8243102431297302, Accuracy = 0.7353503704071045\n",
      "Iter #870400:  Learning rate = 0.003607:   Batch Loss = 0.826995, Accuracy = 0.6953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.839715301990509, Accuracy = 0.6941401362419128\n",
      "Iter #872448:  Learning rate = 0.003607:   Batch Loss = 0.720958, Accuracy = 0.765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7760897278785706, Accuracy = 0.7217875123023987\n",
      "Iter #874496:  Learning rate = 0.003607:   Batch Loss = 0.640913, Accuracy = 0.80078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.749126136302948, Accuracy = 0.7447400689125061\n",
      "Iter #876544:  Learning rate = 0.003607:   Batch Loss = 0.654133, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6540242433547974, Accuracy = 0.7904711961746216\n",
      "Iter #878592:  Learning rate = 0.003607:   Batch Loss = 0.628802, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6627181768417358, Accuracy = 0.7856025099754333\n",
      "Iter #880640:  Learning rate = 0.003607:   Batch Loss = 0.558368, Accuracy = 0.8359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7160881757736206, Accuracy = 0.751173734664917\n",
      "Iter #882688:  Learning rate = 0.003607:   Batch Loss = 0.529384, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6371216177940369, Accuracy = 0.7944705486297607\n",
      "Iter #884736:  Learning rate = 0.003607:   Batch Loss = 0.543256, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5957493782043457, Accuracy = 0.8142931461334229\n",
      "Iter #886784:  Learning rate = 0.003607:   Batch Loss = 0.565214, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6219292879104614, Accuracy = 0.8045557141304016\n",
      "Iter #888832:  Learning rate = 0.003607:   Batch Loss = 0.538859, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6134771108627319, Accuracy = 0.7979481816291809\n",
      "Iter #890880:  Learning rate = 0.003607:   Batch Loss = 0.506299, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6063846349716187, Accuracy = 0.8122065663337708\n",
      "Iter #892928:  Learning rate = 0.003607:   Batch Loss = 0.550403, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6549011468887329, Accuracy = 0.7878629565238953\n",
      "Iter #894976:  Learning rate = 0.003607:   Batch Loss = 1.956348, Accuracy = 0.37109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8208750486373901, Accuracy = 0.3350721597671509\n",
      "Iter #897024:  Learning rate = 0.003607:   Batch Loss = 1.589976, Accuracy = 0.41015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7467138767242432, Accuracy = 0.3248130679130554\n",
      "Iter #899072:  Learning rate = 0.003607:   Batch Loss = 1.343501, Accuracy = 0.46875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.357519507408142, Accuracy = 0.46357154846191406\n",
      "Iter #901120:  Learning rate = 0.003463:   Batch Loss = 1.383078, Accuracy = 0.42578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.310139775276184, Accuracy = 0.4616588354110718\n",
      "Iter #903168:  Learning rate = 0.003463:   Batch Loss = 1.175558, Accuracy = 0.5703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3218270540237427, Accuracy = 0.4757433533668518\n",
      "Iter #905216:  Learning rate = 0.003463:   Batch Loss = 1.340732, Accuracy = 0.4765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1853840351104736, Accuracy = 0.5341679453849792\n",
      "Iter #907264:  Learning rate = 0.003463:   Batch Loss = 1.217002, Accuracy = 0.5390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1257970333099365, Accuracy = 0.5953747034072876\n",
      "Iter #909312:  Learning rate = 0.003463:   Batch Loss = 0.964591, Accuracy = 0.6484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.110504150390625, Accuracy = 0.5889410376548767\n",
      "Iter #911360:  Learning rate = 0.003463:   Batch Loss = 1.196796, Accuracy = 0.51171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0977638959884644, Accuracy = 0.574508786201477\n",
      "Iter #913408:  Learning rate = 0.003463:   Batch Loss = 1.040950, Accuracy = 0.59765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0237013101577759, Accuracy = 0.6073726415634155\n",
      "Iter #915456:  Learning rate = 0.003463:   Batch Loss = 0.989348, Accuracy = 0.6796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9758340716362, Accuracy = 0.6416275501251221\n",
      "Iter #917504:  Learning rate = 0.003463:   Batch Loss = 0.900062, Accuracy = 0.703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.916218101978302, Accuracy = 0.6624934673309326\n",
      "Iter #919552:  Learning rate = 0.003463:   Batch Loss = 0.909080, Accuracy = 0.6640625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8869892358779907, Accuracy = 0.693270742893219\n",
      "Iter #921600:  Learning rate = 0.003463:   Batch Loss = 1.055931, Accuracy = 0.6484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8495859503746033, Accuracy = 0.6786645650863647\n",
      "Iter #923648:  Learning rate = 0.003463:   Batch Loss = 0.844906, Accuracy = 0.69921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8566095232963562, Accuracy = 0.6936184763908386\n",
      "Iter #925696:  Learning rate = 0.003463:   Batch Loss = 1.283740, Accuracy = 0.5078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8288956880569458, Accuracy = 0.7049208879470825\n",
      "Iter #927744:  Learning rate = 0.003463:   Batch Loss = 0.859045, Accuracy = 0.73046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1228060722351074, Accuracy = 0.6031994223594666\n",
      "Iter #929792:  Learning rate = 0.003463:   Batch Loss = 0.887178, Accuracy = 0.7109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8806276917457581, Accuracy = 0.6678838729858398\n",
      "Iter #931840:  Learning rate = 0.003463:   Batch Loss = 0.762601, Accuracy = 0.78515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7744207978248596, Accuracy = 0.7310032844543457\n",
      "Iter #933888:  Learning rate = 0.003463:   Batch Loss = 0.750106, Accuracy = 0.7265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7489780187606812, Accuracy = 0.7490870952606201\n",
      "Iter #935936:  Learning rate = 0.003463:   Batch Loss = 0.733378, Accuracy = 0.765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.690706193447113, Accuracy = 0.7784733176231384\n",
      "Iter #937984:  Learning rate = 0.003463:   Batch Loss = 0.674631, Accuracy = 0.79296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7824416160583496, Accuracy = 0.7247435450553894\n",
      "Iter #940032:  Learning rate = 0.003463:   Batch Loss = 0.754083, Accuracy = 0.76171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7060348987579346, Accuracy = 0.7595200538635254\n",
      "Iter #942080:  Learning rate = 0.003463:   Batch Loss = 0.634471, Accuracy = 0.8203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.714552640914917, Accuracy = 0.7579551339149475\n",
      "Iter #944128:  Learning rate = 0.003463:   Batch Loss = 0.698915, Accuracy = 0.78125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7289036512374878, Accuracy = 0.747000515460968\n",
      "Iter #946176:  Learning rate = 0.003463:   Batch Loss = 0.785637, Accuracy = 0.75390625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7394899129867554, Accuracy = 0.7591723203659058\n",
      "Iter #948224:  Learning rate = 0.003463:   Batch Loss = 0.772365, Accuracy = 0.7109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7453592419624329, Accuracy = 0.7348287105560303\n",
      "Iter #950272:  Learning rate = 0.003463:   Batch Loss = 0.681680, Accuracy = 0.7734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8602668046951294, Accuracy = 0.6819683313369751\n",
      "Iter #952320:  Learning rate = 0.003463:   Batch Loss = 0.774868, Accuracy = 0.73828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6533476114273071, Accuracy = 0.782994270324707\n",
      "Iter #954368:  Learning rate = 0.003463:   Batch Loss = 0.667220, Accuracy = 0.77734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7032382488250732, Accuracy = 0.7636932730674744\n",
      "Iter #956416:  Learning rate = 0.003463:   Batch Loss = 0.733826, Accuracy = 0.76171875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6916263103485107, Accuracy = 0.7685620188713074\n",
      "Iter #958464:  Learning rate = 0.003463:   Batch Loss = 0.649439, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6695619821548462, Accuracy = 0.7770822644233704\n",
      "Iter #960512:  Learning rate = 0.003463:   Batch Loss = 0.679995, Accuracy = 0.78125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6617022156715393, Accuracy = 0.7776039242744446\n",
      "Iter #962560:  Learning rate = 0.003463:   Batch Loss = 0.587025, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6247119307518005, Accuracy = 0.8002086877822876\n",
      "Iter #964608:  Learning rate = 0.003463:   Batch Loss = 0.606621, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6302345395088196, Accuracy = 0.7939488887786865\n",
      "Iter #966656:  Learning rate = 0.003463:   Batch Loss = 0.543704, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6671534180641174, Accuracy = 0.7849069833755493\n",
      "Iter #968704:  Learning rate = 0.003463:   Batch Loss = 0.514210, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6292452812194824, Accuracy = 0.7953399419784546\n",
      "Iter #970752:  Learning rate = 0.003463:   Batch Loss = 0.662994, Accuracy = 0.796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7307985424995422, Accuracy = 0.758998453617096\n",
      "Iter #972800:  Learning rate = 0.003463:   Batch Loss = 0.609518, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6365876793861389, Accuracy = 0.796383261680603\n",
      "Iter #974848:  Learning rate = 0.003463:   Batch Loss = 0.855450, Accuracy = 0.7109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6982526779174805, Accuracy = 0.7643887996673584\n",
      "Iter #976896:  Learning rate = 0.003463:   Batch Loss = 0.577281, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7193673849105835, Accuracy = 0.7609111666679382\n",
      "Iter #978944:  Learning rate = 0.003463:   Batch Loss = 0.635142, Accuracy = 0.78125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.635615885257721, Accuracy = 0.7996870279312134\n",
      "Iter #980992:  Learning rate = 0.003463:   Batch Loss = 0.579428, Accuracy = 0.8515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.7086825370788574, Accuracy = 0.7798643708229065\n",
      "Iter #983040:  Learning rate = 0.003463:   Batch Loss = 0.590319, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5974482297897339, Accuracy = 0.8120326995849609\n",
      "Iter #985088:  Learning rate = 0.003463:   Batch Loss = 0.548963, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5905251502990723, Accuracy = 0.8123804330825806\n",
      "Iter #987136:  Learning rate = 0.003463:   Batch Loss = 0.582711, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6643127799034119, Accuracy = 0.7770822644233704\n",
      "Iter #989184:  Learning rate = 0.003463:   Batch Loss = 0.504868, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6089185476303101, Accuracy = 0.8047296404838562\n",
      "Iter #991232:  Learning rate = 0.003463:   Batch Loss = 0.549230, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5779596567153931, Accuracy = 0.8155103325843811\n",
      "Iter #993280:  Learning rate = 0.003463:   Batch Loss = 0.571176, Accuracy = 0.8359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5733951330184937, Accuracy = 0.8163797855377197\n",
      "Iter #995328:  Learning rate = 0.003463:   Batch Loss = 0.565790, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6009128093719482, Accuracy = 0.8116849064826965\n",
      "Iter #997376:  Learning rate = 0.003463:   Batch Loss = 0.572674, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5925623178482056, Accuracy = 0.8061206936836243\n",
      "Iter #999424:  Learning rate = 0.003463:   Batch Loss = 0.594260, Accuracy = 0.83984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.676282525062561, Accuracy = 0.7769083380699158\n",
      "Iter #1001472:  Learning rate = 0.003324:   Batch Loss = 0.535173, Accuracy = 0.828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5979078412055969, Accuracy = 0.8092505931854248\n",
      "Iter #1003520:  Learning rate = 0.003324:   Batch Loss = 0.565246, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6015781760215759, Accuracy = 0.8155103325843811\n",
      "Iter #1005568:  Learning rate = 0.003324:   Batch Loss = 0.620018, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5904970169067383, Accuracy = 0.8095983266830444\n",
      "Iter #1007616:  Learning rate = 0.003324:   Batch Loss = 0.616832, Accuracy = 0.80078125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5974814295768738, Accuracy = 0.8174230456352234\n",
      "Iter #1009664:  Learning rate = 0.003324:   Batch Loss = 0.554969, Accuracy = 0.83984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5682459473609924, Accuracy = 0.8184663653373718\n",
      "Iter #1011712:  Learning rate = 0.003324:   Batch Loss = 0.534258, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5692058801651001, Accuracy = 0.8221178650856018\n",
      "Iter #1013760:  Learning rate = 0.003324:   Batch Loss = 0.526921, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5768171548843384, Accuracy = 0.8162058591842651\n",
      "Iter #1015808:  Learning rate = 0.003324:   Batch Loss = 0.544942, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.582278311252594, Accuracy = 0.8116849064826965\n",
      "Iter #1017856:  Learning rate = 0.003324:   Batch Loss = 0.482495, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5713552236557007, Accuracy = 0.8155103325843811\n",
      "Iter #1019904:  Learning rate = 0.003324:   Batch Loss = 0.452078, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5850393772125244, Accuracy = 0.813423752784729\n",
      "Iter #1021952:  Learning rate = 0.003324:   Batch Loss = 0.556395, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5807337164878845, Accuracy = 0.8076856136322021\n",
      "Iter #1024000:  Learning rate = 0.003324:   Batch Loss = 0.485271, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5595032572746277, Accuracy = 0.8186402320861816\n",
      "Iter #1026048:  Learning rate = 0.003324:   Batch Loss = 0.512057, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.588034987449646, Accuracy = 0.8118588328361511\n",
      "Iter #1028096:  Learning rate = 0.003324:   Batch Loss = 0.493302, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.558289647102356, Accuracy = 0.8233350515365601\n",
      "Iter #1030144:  Learning rate = 0.003324:   Batch Loss = 0.568673, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5631552934646606, Accuracy = 0.8158581256866455\n",
      "Iter #1032192:  Learning rate = 0.003324:   Batch Loss = 0.517853, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5266084671020508, Accuracy = 0.834637463092804\n",
      "Iter #1034240:  Learning rate = 0.003324:   Batch Loss = 0.511481, Accuracy = 0.828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.538394033908844, Accuracy = 0.8325508832931519\n",
      "Iter #1036288:  Learning rate = 0.003324:   Batch Loss = 0.445484, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5188245177268982, Accuracy = 0.8440271019935608\n",
      "Iter #1038336:  Learning rate = 0.003324:   Batch Loss = 0.447155, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.538501501083374, Accuracy = 0.8381150960922241\n",
      "Iter #1040384:  Learning rate = 0.003324:   Batch Loss = 0.442783, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5198909044265747, Accuracy = 0.8455920815467834\n",
      "Iter #1042432:  Learning rate = 0.003324:   Batch Loss = 0.425311, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5251726508140564, Accuracy = 0.840897262096405\n",
      "Iter #1044480:  Learning rate = 0.003324:   Batch Loss = 0.535681, Accuracy = 0.8203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5268888473510742, Accuracy = 0.8370718359947205\n",
      "Iter #1046528:  Learning rate = 0.003324:   Batch Loss = 0.421346, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5468196868896484, Accuracy = 0.8229873180389404\n",
      "Iter #1048576:  Learning rate = 0.003324:   Batch Loss = 0.607701, Accuracy = 0.8203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5626521110534668, Accuracy = 0.821248471736908\n",
      "Iter #1050624:  Learning rate = 0.003324:   Batch Loss = 0.502777, Accuracy = 0.8515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5201018452644348, Accuracy = 0.8407233357429504\n",
      "Iter #1052672:  Learning rate = 0.003324:   Batch Loss = 0.535824, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6073306798934937, Accuracy = 0.8095983266830444\n",
      "Iter #1054720:  Learning rate = 0.003324:   Batch Loss = 0.425649, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5149924159049988, Accuracy = 0.8448965549468994\n",
      "Iter #1056768:  Learning rate = 0.003324:   Batch Loss = 0.523523, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6064915657043457, Accuracy = 0.8026430010795593\n",
      "Iter #1058816:  Learning rate = 0.003324:   Batch Loss = 0.673866, Accuracy = 0.7734375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6577345728874207, Accuracy = 0.7664753794670105\n",
      "Iter #1060864:  Learning rate = 0.003324:   Batch Loss = 0.579979, Accuracy = 0.80859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5517002940177917, Accuracy = 0.8283776640892029\n",
      "Iter #1062912:  Learning rate = 0.003324:   Batch Loss = 0.561305, Accuracy = 0.7890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5675336122512817, Accuracy = 0.8125543594360352\n",
      "Iter #1064960:  Learning rate = 0.003324:   Batch Loss = 0.482479, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5113322138786316, Accuracy = 0.8426360487937927\n",
      "Iter #1067008:  Learning rate = 0.003324:   Batch Loss = 0.502772, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5510580539703369, Accuracy = 0.8243783712387085\n",
      "Iter #1069056:  Learning rate = 0.003324:   Batch Loss = 0.488927, Accuracy = 0.828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.540549099445343, Accuracy = 0.826117217540741\n",
      "Iter #1071104:  Learning rate = 0.003324:   Batch Loss = 0.449832, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5063962936401367, Accuracy = 0.8414188623428345\n",
      "Iter #1073152:  Learning rate = 0.003324:   Batch Loss = 0.475919, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5398988127708435, Accuracy = 0.8249000310897827\n",
      "Iter #1075200:  Learning rate = 0.003324:   Batch Loss = 0.508204, Accuracy = 0.8359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5233513116836548, Accuracy = 0.8382890224456787\n",
      "Iter #1077248:  Learning rate = 0.003324:   Batch Loss = 0.530320, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5199807286262512, Accuracy = 0.8398539423942566\n",
      "Iter #1079296:  Learning rate = 0.003324:   Batch Loss = 0.594978, Accuracy = 0.7890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6246584057807922, Accuracy = 0.8033385276794434\n",
      "Iter #1081344:  Learning rate = 0.003324:   Batch Loss = 0.492628, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5427855849266052, Accuracy = 0.8290731906890869\n",
      "Iter #1083392:  Learning rate = 0.003324:   Batch Loss = 0.561479, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.534705638885498, Accuracy = 0.8323769569396973\n",
      "Iter #1085440:  Learning rate = 0.003324:   Batch Loss = 0.492074, Accuracy = 0.828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5194138288497925, Accuracy = 0.8266388177871704\n",
      "Iter #1087488:  Learning rate = 0.003324:   Batch Loss = 0.424910, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5368556380271912, Accuracy = 0.8327247500419617\n",
      "Iter #1089536:  Learning rate = 0.003324:   Batch Loss = 0.469104, Accuracy = 0.8515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4916001558303833, Accuracy = 0.844548761844635\n",
      "Iter #1091584:  Learning rate = 0.003324:   Batch Loss = 0.511216, Accuracy = 0.83984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4845258295536041, Accuracy = 0.8450704216957092\n",
      "Iter #1093632:  Learning rate = 0.003324:   Batch Loss = 0.421519, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.524812638759613, Accuracy = 0.8322030901908875\n",
      "Iter #1095680:  Learning rate = 0.003324:   Batch Loss = 0.456614, Accuracy = 0.8515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5244588255882263, Accuracy = 0.8330724835395813\n",
      "Iter #1097728:  Learning rate = 0.003324:   Batch Loss = 0.393024, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5019251108169556, Accuracy = 0.843853235244751\n",
      "Iter #1099776:  Learning rate = 0.003324:   Batch Loss = 0.490278, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5260109901428223, Accuracy = 0.8325508832931519\n",
      "Iter #1101824:  Learning rate = 0.003191:   Batch Loss = 0.472683, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5690573453903198, Accuracy = 0.821248471736908\n",
      "Iter #1103872:  Learning rate = 0.003191:   Batch Loss = 0.512270, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5271151661872864, Accuracy = 0.8313336968421936\n",
      "Iter #1105920:  Learning rate = 0.003191:   Batch Loss = 0.546621, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6112624406814575, Accuracy = 0.8017736077308655\n",
      "Iter #1107968:  Learning rate = 0.003191:   Batch Loss = 0.457930, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4923989176750183, Accuracy = 0.8476786613464355\n",
      "Iter #1110016:  Learning rate = 0.003191:   Batch Loss = 0.395813, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5361027121543884, Accuracy = 0.8330724835395813\n",
      "Iter #1112064:  Learning rate = 0.003191:   Batch Loss = 0.417428, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48397451639175415, Accuracy = 0.843853235244751\n",
      "Iter #1114112:  Learning rate = 0.003191:   Batch Loss = 0.372745, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4943774342536926, Accuracy = 0.8473309278488159\n",
      "Iter #1116160:  Learning rate = 0.003191:   Batch Loss = 0.407312, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5401315093040466, Accuracy = 0.8315075635910034\n",
      "Iter #1118208:  Learning rate = 0.003191:   Batch Loss = 0.438103, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5019652247428894, Accuracy = 0.8454182147979736\n",
      "Iter #1120256:  Learning rate = 0.003191:   Batch Loss = 0.434246, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5119461417198181, Accuracy = 0.8356807231903076\n",
      "Iter #1122304:  Learning rate = 0.003191:   Batch Loss = 0.462181, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5125865936279297, Accuracy = 0.8327247500419617\n",
      "Iter #1124352:  Learning rate = 0.003191:   Batch Loss = 0.412175, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4820961356163025, Accuracy = 0.848721981048584\n",
      "Iter #1126400:  Learning rate = 0.003191:   Batch Loss = 0.366477, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4982301890850067, Accuracy = 0.8480264544487\n",
      "Iter #1128448:  Learning rate = 0.003191:   Batch Loss = 0.416173, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5341099500656128, Accuracy = 0.8295948505401611\n",
      "Iter #1130496:  Learning rate = 0.003191:   Batch Loss = 0.531574, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.50606369972229, Accuracy = 0.8348113298416138\n",
      "Iter #1132544:  Learning rate = 0.003191:   Batch Loss = 0.518388, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.47734299302101135, Accuracy = 0.8534168004989624\n",
      "Iter #1134592:  Learning rate = 0.003191:   Batch Loss = 0.390882, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4878171384334564, Accuracy = 0.8473309278488159\n",
      "Iter #1136640:  Learning rate = 0.003191:   Batch Loss = 0.408447, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48150962591171265, Accuracy = 0.853764533996582\n",
      "Iter #1138688:  Learning rate = 0.003191:   Batch Loss = 0.484098, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4959336519241333, Accuracy = 0.843853235244751\n",
      "Iter #1140736:  Learning rate = 0.003191:   Batch Loss = 0.506478, Accuracy = 0.83984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4657788574695587, Accuracy = 0.8549817204475403\n",
      "Iter #1142784:  Learning rate = 0.003191:   Batch Loss = 0.445399, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5131748914718628, Accuracy = 0.8426360487937927\n",
      "Iter #1144832:  Learning rate = 0.003191:   Batch Loss = 0.444522, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5769707560539246, Accuracy = 0.8076856136322021\n",
      "Iter #1146880:  Learning rate = 0.003191:   Batch Loss = 0.672468, Accuracy = 0.8046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8455392718315125, Accuracy = 0.7374369502067566\n",
      "Iter #1148928:  Learning rate = 0.003191:   Batch Loss = 0.583167, Accuracy = 0.8359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5523465871810913, Accuracy = 0.8109893798828125\n",
      "Iter #1150976:  Learning rate = 0.003191:   Batch Loss = 0.513339, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.567028284072876, Accuracy = 0.8076856136322021\n",
      "Iter #1153024:  Learning rate = 0.003191:   Batch Loss = 0.441367, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5528978109359741, Accuracy = 0.8196835517883301\n",
      "Iter #1155072:  Learning rate = 0.003191:   Batch Loss = 0.446816, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5405901670455933, Accuracy = 0.831159770488739\n",
      "Iter #1157120:  Learning rate = 0.003191:   Batch Loss = 0.375415, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4971187710762024, Accuracy = 0.8468092679977417\n",
      "Iter #1159168:  Learning rate = 0.003191:   Batch Loss = 0.408808, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.47801995277404785, Accuracy = 0.858633279800415\n",
      "Iter #1161216:  Learning rate = 0.003191:   Batch Loss = 0.462821, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5181669592857361, Accuracy = 0.8424621820449829\n",
      "Iter #1163264:  Learning rate = 0.003191:   Batch Loss = 0.333201, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4757686257362366, Accuracy = 0.8497652411460876\n",
      "Iter #1165312:  Learning rate = 0.003191:   Batch Loss = 0.473420, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.49222636222839355, Accuracy = 0.8455920815467834\n",
      "Iter #1167360:  Learning rate = 0.003191:   Batch Loss = 0.434305, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5397498607635498, Accuracy = 0.8290731906890869\n",
      "Iter #1169408:  Learning rate = 0.003191:   Batch Loss = 0.557295, Accuracy = 0.82421875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5541645884513855, Accuracy = 0.8295948505401611\n",
      "Iter #1171456:  Learning rate = 0.003191:   Batch Loss = 0.359586, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.462676078081131, Accuracy = 0.8598504662513733\n",
      "Iter #1173504:  Learning rate = 0.003191:   Batch Loss = 0.376209, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5815582275390625, Accuracy = 0.8172491788864136\n",
      "Iter #1175552:  Learning rate = 0.003191:   Batch Loss = 0.510797, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5055032968521118, Accuracy = 0.8395061492919922\n",
      "Iter #1177600:  Learning rate = 0.003191:   Batch Loss = 0.443188, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4819568693637848, Accuracy = 0.8483741879463196\n",
      "Iter #1179648:  Learning rate = 0.003191:   Batch Loss = 0.421370, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4845290780067444, Accuracy = 0.8534168004989624\n",
      "Iter #1181696:  Learning rate = 0.003191:   Batch Loss = 0.393547, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.47505491971969604, Accuracy = 0.8504607677459717\n",
      "Iter #1183744:  Learning rate = 0.003191:   Batch Loss = 0.538454, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4799409806728363, Accuracy = 0.8435055017471313\n",
      "Iter #1185792:  Learning rate = 0.003191:   Batch Loss = 0.416306, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.451143354177475, Accuracy = 0.8588071465492249\n",
      "Iter #1187840:  Learning rate = 0.003191:   Batch Loss = 0.424031, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4701065719127655, Accuracy = 0.8513302206993103\n",
      "Iter #1189888:  Learning rate = 0.003191:   Batch Loss = 0.434231, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4776858687400818, Accuracy = 0.8480264544487\n",
      "Iter #1191936:  Learning rate = 0.003191:   Batch Loss = 0.467570, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48061317205429077, Accuracy = 0.8433315753936768\n",
      "Iter #1193984:  Learning rate = 0.003191:   Batch Loss = 0.394520, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5359683036804199, Accuracy = 0.826117217540741\n",
      "Iter #1196032:  Learning rate = 0.003191:   Batch Loss = 0.519152, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5076478123664856, Accuracy = 0.8374195694923401\n",
      "Iter #1198080:  Learning rate = 0.003191:   Batch Loss = 0.446700, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4718261957168579, Accuracy = 0.8483741879463196\n",
      "Iter #1200128:  Learning rate = 0.003064:   Batch Loss = 0.442789, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4932437837123871, Accuracy = 0.8381150960922241\n",
      "Iter #1202176:  Learning rate = 0.003064:   Batch Loss = 0.424638, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4671373963356018, Accuracy = 0.8574160933494568\n",
      "Iter #1204224:  Learning rate = 0.003064:   Batch Loss = 0.404630, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4805901050567627, Accuracy = 0.8488958477973938\n",
      "Iter #1206272:  Learning rate = 0.003064:   Batch Loss = 0.421255, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4496515095233917, Accuracy = 0.8607198596000671\n",
      "Iter #1208320:  Learning rate = 0.003064:   Batch Loss = 0.454412, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4615488648414612, Accuracy = 0.853069007396698\n",
      "Iter #1210368:  Learning rate = 0.003064:   Batch Loss = 0.451038, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5024836659431458, Accuracy = 0.843853235244751\n",
      "Iter #1212416:  Learning rate = 0.003064:   Batch Loss = 0.454683, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4546472430229187, Accuracy = 0.8582854866981506\n",
      "Iter #1214464:  Learning rate = 0.003064:   Batch Loss = 0.371723, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46703121066093445, Accuracy = 0.8570683598518372\n",
      "Iter #1216512:  Learning rate = 0.003064:   Batch Loss = 0.351686, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.47672238945961, Accuracy = 0.8565467000007629\n",
      "Iter #1218560:  Learning rate = 0.003064:   Batch Loss = 0.428586, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4739285409450531, Accuracy = 0.8551556468009949\n",
      "Iter #1220608:  Learning rate = 0.003064:   Batch Loss = 0.368869, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4700578451156616, Accuracy = 0.8521996140480042\n",
      "Iter #1222656:  Learning rate = 0.003064:   Batch Loss = 0.405998, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48140785098075867, Accuracy = 0.8426360487937927\n",
      "Iter #1224704:  Learning rate = 0.003064:   Batch Loss = 0.365025, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45140013098716736, Accuracy = 0.8608937859535217\n",
      "Iter #1226752:  Learning rate = 0.003064:   Batch Loss = 0.405014, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4713187515735626, Accuracy = 0.8521996140480042\n",
      "Iter #1228800:  Learning rate = 0.003064:   Batch Loss = 0.442741, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44454652070999146, Accuracy = 0.8687185049057007\n",
      "Iter #1230848:  Learning rate = 0.003064:   Batch Loss = 0.353574, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46503961086273193, Accuracy = 0.857937753200531\n",
      "Iter #1232896:  Learning rate = 0.003064:   Batch Loss = 0.451513, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4904455542564392, Accuracy = 0.8499391674995422\n",
      "Iter #1234944:  Learning rate = 0.003064:   Batch Loss = 0.451745, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4513912796974182, Accuracy = 0.8612415194511414\n",
      "Iter #1236992:  Learning rate = 0.003064:   Batch Loss = 0.450050, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45485350489616394, Accuracy = 0.8591549396514893\n",
      "Iter #1239040:  Learning rate = 0.003064:   Batch Loss = 0.376760, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46381235122680664, Accuracy = 0.8567205667495728\n",
      "Iter #1241088:  Learning rate = 0.003064:   Batch Loss = 0.387198, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4433313310146332, Accuracy = 0.8643714189529419\n",
      "Iter #1243136:  Learning rate = 0.003064:   Batch Loss = 0.371626, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4715714454650879, Accuracy = 0.8549817204475403\n",
      "Iter #1245184:  Learning rate = 0.003064:   Batch Loss = 0.441044, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4565749764442444, Accuracy = 0.8614153861999512\n",
      "Iter #1247232:  Learning rate = 0.003064:   Batch Loss = 0.446214, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4837348163127899, Accuracy = 0.8499391674995422\n",
      "Iter #1249280:  Learning rate = 0.003064:   Batch Loss = 0.449054, Accuracy = 0.8515625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4651855230331421, Accuracy = 0.8541123270988464\n",
      "Iter #1251328:  Learning rate = 0.003064:   Batch Loss = 0.375697, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44687366485595703, Accuracy = 0.8589810729026794\n",
      "Iter #1253376:  Learning rate = 0.003064:   Batch Loss = 0.418311, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.49781954288482666, Accuracy = 0.8396800756454468\n",
      "Iter #1255424:  Learning rate = 0.003064:   Batch Loss = 0.408031, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.49019649624824524, Accuracy = 0.8433315753936768\n",
      "Iter #1257472:  Learning rate = 0.003064:   Batch Loss = 0.425208, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4549978971481323, Accuracy = 0.8603721261024475\n",
      "Iter #1259520:  Learning rate = 0.003064:   Batch Loss = 0.365815, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4478406012058258, Accuracy = 0.8589810729026794\n",
      "Iter #1261568:  Learning rate = 0.003064:   Batch Loss = 0.496070, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4676870107650757, Accuracy = 0.8551556468009949\n",
      "Iter #1263616:  Learning rate = 0.003064:   Batch Loss = 0.378166, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43155279755592346, Accuracy = 0.8709789514541626\n",
      "Iter #1265664:  Learning rate = 0.003064:   Batch Loss = 0.406843, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4483431577682495, Accuracy = 0.8659363389015198\n",
      "Iter #1267712:  Learning rate = 0.003064:   Batch Loss = 0.338359, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4424828588962555, Accuracy = 0.8628064393997192\n",
      "Iter #1269760:  Learning rate = 0.003064:   Batch Loss = 0.371061, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4462720453739166, Accuracy = 0.8626325726509094\n",
      "Iter #1271808:  Learning rate = 0.003064:   Batch Loss = 0.350309, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.47510960698127747, Accuracy = 0.8546339869499207\n",
      "Iter #1273856:  Learning rate = 0.003064:   Batch Loss = 0.420401, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4842461347579956, Accuracy = 0.8553295135498047\n",
      "Iter #1275904:  Learning rate = 0.003064:   Batch Loss = 0.440569, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4872305393218994, Accuracy = 0.8442010283470154\n",
      "Iter #1277952:  Learning rate = 0.003064:   Batch Loss = 0.415486, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4565806984901428, Accuracy = 0.8551556468009949\n",
      "Iter #1280000:  Learning rate = 0.003064:   Batch Loss = 0.351184, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44527915120124817, Accuracy = 0.8607198596000671\n",
      "Iter #1282048:  Learning rate = 0.003064:   Batch Loss = 0.347832, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4866645932197571, Accuracy = 0.8497652411460876\n",
      "Iter #1284096:  Learning rate = 0.003064:   Batch Loss = 0.433390, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4765045642852783, Accuracy = 0.8448965549468994\n",
      "Iter #1286144:  Learning rate = 0.003064:   Batch Loss = 0.449148, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44764280319213867, Accuracy = 0.8612415194511414\n",
      "Iter #1288192:  Learning rate = 0.003064:   Batch Loss = 0.357284, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44117218255996704, Accuracy = 0.8626325726509094\n",
      "Iter #1290240:  Learning rate = 0.003064:   Batch Loss = 0.355564, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48758313059806824, Accuracy = 0.8391584157943726\n",
      "Iter #1292288:  Learning rate = 0.003064:   Batch Loss = 0.428126, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4438576102256775, Accuracy = 0.8633280992507935\n",
      "Iter #1294336:  Learning rate = 0.003064:   Batch Loss = 0.324731, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4462219178676605, Accuracy = 0.8654147386550903\n",
      "Iter #1296384:  Learning rate = 0.003064:   Batch Loss = 0.372599, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4447013735771179, Accuracy = 0.8612415194511414\n",
      "Iter #1298432:  Learning rate = 0.003064:   Batch Loss = 0.346770, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4389292001724243, Accuracy = 0.86576247215271\n",
      "Iter #1300480:  Learning rate = 0.002941:   Batch Loss = 0.348865, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.490706205368042, Accuracy = 0.8506346940994263\n",
      "Iter #1302528:  Learning rate = 0.002941:   Batch Loss = 0.380076, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46000558137893677, Accuracy = 0.8591549396514893\n",
      "Iter #1304576:  Learning rate = 0.002941:   Batch Loss = 0.431945, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43681013584136963, Accuracy = 0.866457998752594\n",
      "Iter #1306624:  Learning rate = 0.002941:   Batch Loss = 0.385658, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5110880136489868, Accuracy = 0.8412449955940247\n",
      "Iter #1308672:  Learning rate = 0.002941:   Batch Loss = 0.391348, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4428388476371765, Accuracy = 0.8629803657531738\n",
      "Iter #1310720:  Learning rate = 0.002941:   Batch Loss = 0.347328, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45622938871383667, Accuracy = 0.8654147386550903\n",
      "Iter #1312768:  Learning rate = 0.002941:   Batch Loss = 0.372344, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44816187024116516, Accuracy = 0.8661102652549744\n",
      "Iter #1314816:  Learning rate = 0.002941:   Batch Loss = 0.434763, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45765992999076843, Accuracy = 0.8582854866981506\n",
      "Iter #1316864:  Learning rate = 0.002941:   Batch Loss = 0.445566, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44584372639656067, Accuracy = 0.8629803657531738\n",
      "Iter #1318912:  Learning rate = 0.002941:   Batch Loss = 0.350167, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44971126317977905, Accuracy = 0.8617631793022156\n",
      "Iter #1320960:  Learning rate = 0.002941:   Batch Loss = 0.422959, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45142531394958496, Accuracy = 0.8555033802986145\n",
      "Iter #1323008:  Learning rate = 0.002941:   Batch Loss = 0.427240, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4566341042518616, Accuracy = 0.8570683598518372\n",
      "Iter #1325056:  Learning rate = 0.002941:   Batch Loss = 0.438846, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4609913229942322, Accuracy = 0.857937753200531\n",
      "Iter #1327104:  Learning rate = 0.002941:   Batch Loss = 0.432502, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4744769036769867, Accuracy = 0.8508085608482361\n",
      "Iter #1329152:  Learning rate = 0.002941:   Batch Loss = 0.387033, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43247154355049133, Accuracy = 0.8681968450546265\n",
      "Iter #1331200:  Learning rate = 0.002941:   Batch Loss = 0.507722, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4720916450023651, Accuracy = 0.8565467000007629\n",
      "Iter #1333248:  Learning rate = 0.002941:   Batch Loss = 0.398863, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4277796745300293, Accuracy = 0.8695878982543945\n",
      "Iter #1335296:  Learning rate = 0.002941:   Batch Loss = 0.300915, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4519357979297638, Accuracy = 0.8683707118034363\n",
      "Iter #1337344:  Learning rate = 0.002941:   Batch Loss = 0.377104, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4924355745315552, Accuracy = 0.8513302206993103\n",
      "Iter #1339392:  Learning rate = 0.002941:   Batch Loss = 0.436926, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43447378277778625, Accuracy = 0.8641975522041321\n",
      "Iter #1341440:  Learning rate = 0.002941:   Batch Loss = 0.365493, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4288982152938843, Accuracy = 0.8666318655014038\n",
      "Iter #1343488:  Learning rate = 0.002941:   Batch Loss = 0.386577, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48160025477409363, Accuracy = 0.8546339869499207\n",
      "Iter #1345536:  Learning rate = 0.002941:   Batch Loss = 0.403034, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4405810534954071, Accuracy = 0.8619370460510254\n",
      "Iter #1347584:  Learning rate = 0.002941:   Batch Loss = 0.393095, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45527780055999756, Accuracy = 0.8607198596000671\n",
      "Iter #1349632:  Learning rate = 0.002941:   Batch Loss = 0.389886, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4190083146095276, Accuracy = 0.8709789514541626\n",
      "Iter #1351680:  Learning rate = 0.002941:   Batch Loss = 0.378463, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45968368649482727, Accuracy = 0.8563728332519531\n",
      "Iter #1353728:  Learning rate = 0.002941:   Batch Loss = 0.385661, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4269716739654541, Accuracy = 0.8702834248542786\n",
      "Iter #1355776:  Learning rate = 0.002941:   Batch Loss = 0.429008, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44109803438186646, Accuracy = 0.8588071465492249\n",
      "Iter #1357824:  Learning rate = 0.002941:   Batch Loss = 0.402317, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44254758954048157, Accuracy = 0.8661102652549744\n",
      "Iter #1359872:  Learning rate = 0.002941:   Batch Loss = 0.402841, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.436635285615921, Accuracy = 0.8640236258506775\n",
      "Iter #1361920:  Learning rate = 0.002941:   Batch Loss = 0.377710, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.446668803691864, Accuracy = 0.8568944334983826\n",
      "Iter #1363968:  Learning rate = 0.002941:   Batch Loss = 0.388903, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4845508337020874, Accuracy = 0.8475047945976257\n",
      "Iter #1366016:  Learning rate = 0.002941:   Batch Loss = 0.444283, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46791329979896545, Accuracy = 0.8551556468009949\n",
      "Iter #1368064:  Learning rate = 0.002941:   Batch Loss = 0.403679, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5221395492553711, Accuracy = 0.8405494689941406\n",
      "Iter #1370112:  Learning rate = 0.002941:   Batch Loss = 0.363295, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4458004832267761, Accuracy = 0.8577638864517212\n",
      "Iter #1372160:  Learning rate = 0.002941:   Batch Loss = 0.404136, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44624581933021545, Accuracy = 0.8607198596000671\n",
      "Iter #1374208:  Learning rate = 0.002941:   Batch Loss = 0.420178, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48666876554489136, Accuracy = 0.8469831347465515\n",
      "Iter #1376256:  Learning rate = 0.002941:   Batch Loss = 0.400314, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4419715106487274, Accuracy = 0.8619370460510254\n",
      "Iter #1378304:  Learning rate = 0.002941:   Batch Loss = 0.356965, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4248703122138977, Accuracy = 0.8744566440582275\n",
      "Iter #1380352:  Learning rate = 0.002941:   Batch Loss = 0.417034, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.420154869556427, Accuracy = 0.8786298036575317\n",
      "Iter #1382400:  Learning rate = 0.002941:   Batch Loss = 0.423965, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42660006880760193, Accuracy = 0.8718483448028564\n",
      "Iter #1384448:  Learning rate = 0.002941:   Batch Loss = 0.332137, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4546428918838501, Accuracy = 0.8574160933494568\n",
      "Iter #1386496:  Learning rate = 0.002941:   Batch Loss = 0.387339, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44359731674194336, Accuracy = 0.8638497591018677\n",
      "Iter #1388544:  Learning rate = 0.002941:   Batch Loss = 0.370938, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4313163459300995, Accuracy = 0.8666318655014038\n",
      "Iter #1390592:  Learning rate = 0.002941:   Batch Loss = 0.437122, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44442132115364075, Accuracy = 0.8648930788040161\n",
      "Iter #1392640:  Learning rate = 0.002941:   Batch Loss = 0.300115, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43621954321861267, Accuracy = 0.8617631793022156\n",
      "Iter #1394688:  Learning rate = 0.002941:   Batch Loss = 0.375803, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4411843419075012, Accuracy = 0.8624587059020996\n",
      "Iter #1396736:  Learning rate = 0.002941:   Batch Loss = 0.367967, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4414815306663513, Accuracy = 0.8701095581054688\n",
      "Iter #1398784:  Learning rate = 0.002941:   Batch Loss = 0.390130, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.478515088558197, Accuracy = 0.8499391674995422\n",
      "Iter #1400832:  Learning rate = 0.002823:   Batch Loss = 0.351087, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44836869835853577, Accuracy = 0.8588071465492249\n",
      "Iter #1402880:  Learning rate = 0.002823:   Batch Loss = 0.457724, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4882308840751648, Accuracy = 0.8508085608482361\n",
      "Iter #1404928:  Learning rate = 0.002823:   Batch Loss = 0.361876, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4248642027378082, Accuracy = 0.866457998752594\n",
      "Iter #1406976:  Learning rate = 0.002823:   Batch Loss = 0.324073, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.47293055057525635, Accuracy = 0.8532429337501526\n",
      "Iter #1409024:  Learning rate = 0.002823:   Batch Loss = 0.496742, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4208431839942932, Accuracy = 0.8697617650032043\n",
      "Iter #1411072:  Learning rate = 0.002823:   Batch Loss = 0.357545, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44589024782180786, Accuracy = 0.8650669455528259\n",
      "Iter #1413120:  Learning rate = 0.002823:   Batch Loss = 0.313696, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4352734088897705, Accuracy = 0.8701095581054688\n",
      "Iter #1415168:  Learning rate = 0.002823:   Batch Loss = 0.355273, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4508737325668335, Accuracy = 0.8622848391532898\n",
      "Iter #1417216:  Learning rate = 0.002823:   Batch Loss = 0.417311, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43428945541381836, Accuracy = 0.8699356913566589\n",
      "Iter #1419264:  Learning rate = 0.002823:   Batch Loss = 0.440478, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46573129296302795, Accuracy = 0.8553295135498047\n",
      "Iter #1421312:  Learning rate = 0.002823:   Batch Loss = 0.351985, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4380858838558197, Accuracy = 0.8685445785522461\n",
      "Iter #1423360:  Learning rate = 0.002823:   Batch Loss = 0.474155, Accuracy = 0.8125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46920353174209595, Accuracy = 0.8560250401496887\n",
      "Iter #1425408:  Learning rate = 0.002823:   Batch Loss = 0.322468, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4200136363506317, Accuracy = 0.871326744556427\n",
      "Iter #1427456:  Learning rate = 0.002823:   Batch Loss = 0.425432, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4468725323677063, Accuracy = 0.8631542325019836\n",
      "Iter #1429504:  Learning rate = 0.002823:   Batch Loss = 0.375924, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42278391122817993, Accuracy = 0.8695878982543945\n",
      "Iter #1431552:  Learning rate = 0.002823:   Batch Loss = 0.377741, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4114491939544678, Accuracy = 0.8704572916030884\n",
      "Iter #1433600:  Learning rate = 0.002823:   Batch Loss = 0.384942, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4173879623413086, Accuracy = 0.876369297504425\n",
      "Iter #1435648:  Learning rate = 0.002823:   Batch Loss = 0.432154, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43018004298210144, Accuracy = 0.8690662384033203\n",
      "Iter #1437696:  Learning rate = 0.002823:   Batch Loss = 0.342100, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42795252799987793, Accuracy = 0.8748043775558472\n",
      "Iter #1439744:  Learning rate = 0.002823:   Batch Loss = 0.409219, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4405909478664398, Accuracy = 0.8633280992507935\n",
      "Iter #1441792:  Learning rate = 0.002823:   Batch Loss = 0.438908, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45500776171684265, Accuracy = 0.8614153861999512\n",
      "Iter #1443840:  Learning rate = 0.002823:   Batch Loss = 0.417200, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4464549720287323, Accuracy = 0.8521996140480042\n",
      "Iter #1445888:  Learning rate = 0.002823:   Batch Loss = 0.401597, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43583837151527405, Accuracy = 0.8542861938476562\n",
      "Iter #1447936:  Learning rate = 0.002823:   Batch Loss = 0.437546, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4519417881965637, Accuracy = 0.8582854866981506\n",
      "Iter #1449984:  Learning rate = 0.002823:   Batch Loss = 0.312126, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42097049951553345, Accuracy = 0.8744566440582275\n",
      "Iter #1452032:  Learning rate = 0.002823:   Batch Loss = 0.371131, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4078397750854492, Accuracy = 0.8735871911048889\n",
      "Iter #1454080:  Learning rate = 0.002823:   Batch Loss = 0.343176, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4202879071235657, Accuracy = 0.8734133243560791\n",
      "Iter #1456128:  Learning rate = 0.002823:   Batch Loss = 0.380028, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40263456106185913, Accuracy = 0.8735871911048889\n",
      "Iter #1458176:  Learning rate = 0.002823:   Batch Loss = 0.349851, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4638904333114624, Accuracy = 0.8595026731491089\n",
      "Iter #1460224:  Learning rate = 0.002823:   Batch Loss = 0.346087, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4508649408817291, Accuracy = 0.8535906672477722\n",
      "Iter #1462272:  Learning rate = 0.002823:   Batch Loss = 0.485354, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5228717923164368, Accuracy = 0.8368979096412659\n",
      "Iter #1464320:  Learning rate = 0.002823:   Batch Loss = 0.390583, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5634896755218506, Accuracy = 0.8092505931854248\n",
      "Iter #1466368:  Learning rate = 0.002823:   Batch Loss = 0.410760, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4938323497772217, Accuracy = 0.8419405221939087\n",
      "Iter #1468416:  Learning rate = 0.002823:   Batch Loss = 0.395513, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44190046191215515, Accuracy = 0.8610676527023315\n",
      "Iter #1470464:  Learning rate = 0.002823:   Batch Loss = 0.372637, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43767407536506653, Accuracy = 0.8617631793022156\n",
      "Iter #1472512:  Learning rate = 0.002823:   Batch Loss = 0.374873, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.461830198764801, Accuracy = 0.8612415194511414\n",
      "Iter #1474560:  Learning rate = 0.002823:   Batch Loss = 0.348200, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44986164569854736, Accuracy = 0.8676751852035522\n",
      "Iter #1476608:  Learning rate = 0.002823:   Batch Loss = 0.351378, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4198385775089264, Accuracy = 0.8746305108070374\n",
      "Iter #1478656:  Learning rate = 0.002823:   Batch Loss = 0.335180, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.392324835062027, Accuracy = 0.87984699010849\n",
      "Iter #1480704:  Learning rate = 0.002823:   Batch Loss = 0.405601, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4068661034107208, Accuracy = 0.8794991970062256\n",
      "Iter #1482752:  Learning rate = 0.002823:   Batch Loss = 0.418730, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3875337541103363, Accuracy = 0.8834985494613647\n",
      "Iter #1484800:  Learning rate = 0.002823:   Batch Loss = 0.352567, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41228923201560974, Accuracy = 0.874282717704773\n",
      "Iter #1486848:  Learning rate = 0.002823:   Batch Loss = 0.415083, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4294038414955139, Accuracy = 0.8680229783058167\n",
      "Iter #1488896:  Learning rate = 0.002823:   Batch Loss = 0.494323, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4382556974887848, Accuracy = 0.8640236258506775\n",
      "Iter #1490944:  Learning rate = 0.002823:   Batch Loss = 0.424560, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44434523582458496, Accuracy = 0.8567205667495728\n",
      "Iter #1492992:  Learning rate = 0.002823:   Batch Loss = 0.388125, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4176980257034302, Accuracy = 0.8699356913566589\n",
      "Iter #1495040:  Learning rate = 0.002823:   Batch Loss = 0.333223, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41933536529541016, Accuracy = 0.8767170906066895\n",
      "Iter #1497088:  Learning rate = 0.002823:   Batch Loss = 0.357538, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41537103056907654, Accuracy = 0.8767170906066895\n",
      "Iter #1499136:  Learning rate = 0.002823:   Batch Loss = 0.317496, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41290023922920227, Accuracy = 0.8748043775558472\n",
      "Iter #1501184:  Learning rate = 0.002710:   Batch Loss = 0.343773, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39511239528656006, Accuracy = 0.8808902502059937\n",
      "Iter #1503232:  Learning rate = 0.002710:   Batch Loss = 0.371927, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41099002957344055, Accuracy = 0.8765432238578796\n",
      "Iter #1505280:  Learning rate = 0.002710:   Batch Loss = 0.359339, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42816975712776184, Accuracy = 0.8680229783058167\n",
      "Iter #1507328:  Learning rate = 0.002710:   Batch Loss = 0.373974, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4467299282550812, Accuracy = 0.8621109127998352\n",
      "Iter #1509376:  Learning rate = 0.002710:   Batch Loss = 0.397180, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5793235898017883, Accuracy = 0.821943998336792\n",
      "Iter #1511424:  Learning rate = 0.002710:   Batch Loss = 0.511737, Accuracy = 0.84375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.49576178193092346, Accuracy = 0.8407233357429504\n",
      "Iter #1513472:  Learning rate = 0.002710:   Batch Loss = 0.396260, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4612748622894287, Accuracy = 0.8528951406478882\n",
      "Iter #1515520:  Learning rate = 0.002710:   Batch Loss = 0.406178, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45141029357910156, Accuracy = 0.8542861938476562\n",
      "Iter #1517568:  Learning rate = 0.002710:   Batch Loss = 0.335736, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4584486186504364, Accuracy = 0.8527212738990784\n",
      "Iter #1519616:  Learning rate = 0.002710:   Batch Loss = 0.482481, Accuracy = 0.83203125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4612242877483368, Accuracy = 0.8595026731491089\n",
      "Iter #1521664:  Learning rate = 0.002710:   Batch Loss = 0.337308, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4302070736885071, Accuracy = 0.8685445785522461\n",
      "Iter #1523712:  Learning rate = 0.002710:   Batch Loss = 0.341610, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43134427070617676, Accuracy = 0.8711528182029724\n",
      "Iter #1525760:  Learning rate = 0.002710:   Batch Loss = 0.364575, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42036575078964233, Accuracy = 0.8708050847053528\n",
      "Iter #1527808:  Learning rate = 0.002710:   Batch Loss = 0.313308, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41565167903900146, Accuracy = 0.8709789514541626\n",
      "Iter #1529856:  Learning rate = 0.002710:   Batch Loss = 0.432152, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42522257566452026, Accuracy = 0.8716744780540466\n",
      "Iter #1531904:  Learning rate = 0.002710:   Batch Loss = 0.276917, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.400581955909729, Accuracy = 0.8824552297592163\n",
      "Iter #1533952:  Learning rate = 0.002710:   Batch Loss = 0.329312, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40562301874160767, Accuracy = 0.8735871911048889\n",
      "Iter #1536000:  Learning rate = 0.002710:   Batch Loss = 0.334705, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44804126024246216, Accuracy = 0.8591549396514893\n",
      "Iter #1538048:  Learning rate = 0.002710:   Batch Loss = 0.370768, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41412481665611267, Accuracy = 0.8761954307556152\n",
      "Iter #1540096:  Learning rate = 0.002710:   Batch Loss = 0.344933, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44156384468078613, Accuracy = 0.8648930788040161\n",
      "Iter #1542144:  Learning rate = 0.002710:   Batch Loss = 0.372311, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40329962968826294, Accuracy = 0.8808902502059937\n",
      "Iter #1544192:  Learning rate = 0.002710:   Batch Loss = 0.377811, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4138041138648987, Accuracy = 0.8732394576072693\n",
      "Iter #1546240:  Learning rate = 0.002710:   Batch Loss = 0.358305, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4296894669532776, Accuracy = 0.8690662384033203\n",
      "Iter #1548288:  Learning rate = 0.002710:   Batch Loss = 0.345920, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4048253297805786, Accuracy = 0.8788036704063416\n",
      "Iter #1550336:  Learning rate = 0.002710:   Batch Loss = 0.336353, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3827590346336365, Accuracy = 0.8850634694099426\n",
      "Iter #1552384:  Learning rate = 0.002710:   Batch Loss = 0.380598, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40440937876701355, Accuracy = 0.8767170906066895\n",
      "Iter #1554432:  Learning rate = 0.002710:   Batch Loss = 0.365218, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41651037335395813, Accuracy = 0.870631217956543\n",
      "Iter #1556480:  Learning rate = 0.002710:   Batch Loss = 0.312183, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42638060450553894, Accuracy = 0.8732394576072693\n",
      "Iter #1558528:  Learning rate = 0.002710:   Batch Loss = 0.353110, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40951740741729736, Accuracy = 0.8739349842071533\n",
      "Iter #1560576:  Learning rate = 0.002710:   Batch Loss = 0.334574, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4216584861278534, Accuracy = 0.8668057918548584\n",
      "Iter #1562624:  Learning rate = 0.002710:   Batch Loss = 0.278720, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43735307455062866, Accuracy = 0.863502025604248\n",
      "Iter #1564672:  Learning rate = 0.002710:   Batch Loss = 0.343034, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4632323384284973, Accuracy = 0.8621109127998352\n",
      "Iter #1566720:  Learning rate = 0.002710:   Batch Loss = 0.338625, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3972906768321991, Accuracy = 0.8812380433082581\n",
      "Iter #1568768:  Learning rate = 0.002710:   Batch Loss = 0.356236, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43477314710617065, Accuracy = 0.8694140315055847\n",
      "Iter #1570816:  Learning rate = 0.002710:   Batch Loss = 0.358664, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43399763107299805, Accuracy = 0.8640236258506775\n",
      "Iter #1572864:  Learning rate = 0.002710:   Batch Loss = 0.418025, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39848706126213074, Accuracy = 0.8789775967597961\n",
      "Iter #1574912:  Learning rate = 0.002710:   Batch Loss = 0.428093, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4410885274410248, Accuracy = 0.8610676527023315\n",
      "Iter #1576960:  Learning rate = 0.002710:   Batch Loss = 0.348864, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40424656867980957, Accuracy = 0.874978244304657\n",
      "Iter #1579008:  Learning rate = 0.002710:   Batch Loss = 0.395842, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45954543352127075, Accuracy = 0.8652408123016357\n",
      "Iter #1581056:  Learning rate = 0.002710:   Batch Loss = 0.349162, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45345503091812134, Accuracy = 0.8591549396514893\n",
      "Iter #1583104:  Learning rate = 0.002710:   Batch Loss = 0.267864, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4348140060901642, Accuracy = 0.8669796586036682\n",
      "Iter #1585152:  Learning rate = 0.002710:   Batch Loss = 0.312967, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39990097284317017, Accuracy = 0.8789775967597961\n",
      "Iter #1587200:  Learning rate = 0.002710:   Batch Loss = 0.342578, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42686596512794495, Accuracy = 0.8701095581054688\n",
      "Iter #1589248:  Learning rate = 0.002710:   Batch Loss = 0.357300, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41503357887268066, Accuracy = 0.874282717704773\n",
      "Iter #1591296:  Learning rate = 0.002710:   Batch Loss = 0.467599, Accuracy = 0.86328125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.6763332486152649, Accuracy = 0.796383261680603\n",
      "Iter #1593344:  Learning rate = 0.002710:   Batch Loss = 0.363101, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4312942326068878, Accuracy = 0.8687185049057007\n",
      "Iter #1595392:  Learning rate = 0.002710:   Batch Loss = 0.347478, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4034572243690491, Accuracy = 0.8737610578536987\n",
      "Iter #1597440:  Learning rate = 0.002710:   Batch Loss = 0.342529, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4891652762889862, Accuracy = 0.8551556468009949\n",
      "Iter #1599488:  Learning rate = 0.002710:   Batch Loss = 0.368538, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4118753969669342, Accuracy = 0.8727177977561951\n",
      "Iter #1601536:  Learning rate = 0.002602:   Batch Loss = 0.337887, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.395659476518631, Accuracy = 0.879151463508606\n",
      "Iter #1603584:  Learning rate = 0.002602:   Batch Loss = 0.281781, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.402737021446228, Accuracy = 0.8775864839553833\n",
      "Iter #1605632:  Learning rate = 0.002602:   Batch Loss = 0.358256, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39226600527763367, Accuracy = 0.8789775967597961\n",
      "Iter #1607680:  Learning rate = 0.002602:   Batch Loss = 0.326259, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4232402443885803, Accuracy = 0.8697617650032043\n",
      "Iter #1609728:  Learning rate = 0.002602:   Batch Loss = 0.352604, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3839629292488098, Accuracy = 0.8838462829589844\n",
      "Iter #1611776:  Learning rate = 0.002602:   Batch Loss = 0.352510, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3913896679878235, Accuracy = 0.880542516708374\n",
      "Iter #1613824:  Learning rate = 0.002602:   Batch Loss = 0.344737, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.54314124584198, Accuracy = 0.8292471170425415\n",
      "Iter #1615872:  Learning rate = 0.002602:   Batch Loss = 0.381334, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4859810173511505, Accuracy = 0.8506346940994263\n",
      "Iter #1617920:  Learning rate = 0.002602:   Batch Loss = 0.386593, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40906184911727905, Accuracy = 0.8770648837089539\n",
      "Iter #1619968:  Learning rate = 0.002602:   Batch Loss = 0.332448, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38771820068359375, Accuracy = 0.879151463508606\n",
      "Iter #1622016:  Learning rate = 0.002602:   Batch Loss = 0.318765, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39855414628982544, Accuracy = 0.8803686499595642\n",
      "Iter #1624064:  Learning rate = 0.002602:   Batch Loss = 0.389200, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40611153841018677, Accuracy = 0.8789775967597961\n",
      "Iter #1626112:  Learning rate = 0.002602:   Batch Loss = 0.340010, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4136495888233185, Accuracy = 0.8754999041557312\n",
      "Iter #1628160:  Learning rate = 0.002602:   Batch Loss = 0.277412, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3870934247970581, Accuracy = 0.8817597031593323\n",
      "Iter #1630208:  Learning rate = 0.002602:   Batch Loss = 0.320698, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41733938455581665, Accuracy = 0.8734133243560791\n",
      "Iter #1632256:  Learning rate = 0.002602:   Batch Loss = 0.414962, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40684062242507935, Accuracy = 0.8774126172065735\n",
      "Iter #1634304:  Learning rate = 0.002602:   Batch Loss = 0.385746, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3919556140899658, Accuracy = 0.8829768896102905\n",
      "Iter #1636352:  Learning rate = 0.002602:   Batch Loss = 0.338221, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39109817147254944, Accuracy = 0.8819335699081421\n",
      "Iter #1638400:  Learning rate = 0.002602:   Batch Loss = 0.389380, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.419315904378891, Accuracy = 0.8711528182029724\n",
      "Iter #1640448:  Learning rate = 0.002602:   Batch Loss = 0.375823, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3991689383983612, Accuracy = 0.8807163834571838\n",
      "Iter #1642496:  Learning rate = 0.002602:   Batch Loss = 0.397297, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5113773345947266, Accuracy = 0.8400278091430664\n",
      "Iter #1644544:  Learning rate = 0.002602:   Batch Loss = 0.301262, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3960043489933014, Accuracy = 0.8789775967597961\n",
      "Iter #1646592:  Learning rate = 0.002602:   Batch Loss = 0.373080, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38096049427986145, Accuracy = 0.8838462829589844\n",
      "Iter #1648640:  Learning rate = 0.002602:   Batch Loss = 0.334076, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40843501687049866, Accuracy = 0.8748043775558472\n",
      "Iter #1650688:  Learning rate = 0.002602:   Batch Loss = 0.331836, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4101678133010864, Accuracy = 0.8768909573554993\n",
      "Iter #1652736:  Learning rate = 0.002602:   Batch Loss = 0.324087, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4132346510887146, Accuracy = 0.8695878982543945\n",
      "Iter #1654784:  Learning rate = 0.002602:   Batch Loss = 0.398447, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42601191997528076, Accuracy = 0.8716744780540466\n",
      "Iter #1656832:  Learning rate = 0.002602:   Batch Loss = 0.327388, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40328288078308105, Accuracy = 0.8669796586036682\n",
      "Iter #1658880:  Learning rate = 0.002602:   Batch Loss = 0.355255, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3920748233795166, Accuracy = 0.8732394576072693\n",
      "Iter #1660928:  Learning rate = 0.002602:   Batch Loss = 0.347991, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3946354389190674, Accuracy = 0.8822813630104065\n",
      "Iter #1662976:  Learning rate = 0.002602:   Batch Loss = 0.289608, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3896927237510681, Accuracy = 0.8862806558609009\n",
      "Iter #1665024:  Learning rate = 0.002602:   Batch Loss = 0.379017, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40179184079170227, Accuracy = 0.8800208568572998\n",
      "Iter #1667072:  Learning rate = 0.002602:   Batch Loss = 0.336892, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3948242664337158, Accuracy = 0.8817597031593323\n",
      "Iter #1669120:  Learning rate = 0.002602:   Batch Loss = 0.328313, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4003787040710449, Accuracy = 0.8768909573554993\n",
      "Iter #1671168:  Learning rate = 0.002602:   Batch Loss = 0.320505, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3915179669857025, Accuracy = 0.8800208568572998\n",
      "Iter #1673216:  Learning rate = 0.002602:   Batch Loss = 0.352723, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38545435667037964, Accuracy = 0.8888888955116272\n",
      "Iter #1675264:  Learning rate = 0.002602:   Batch Loss = 0.324835, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3964093029499054, Accuracy = 0.8751521706581116\n",
      "Iter #1677312:  Learning rate = 0.002602:   Batch Loss = 0.351425, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37951013445854187, Accuracy = 0.887671709060669\n",
      "Iter #1679360:  Learning rate = 0.002602:   Batch Loss = 0.373248, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38262873888015747, Accuracy = 0.8779342770576477\n",
      "Iter #1681408:  Learning rate = 0.002602:   Batch Loss = 0.341534, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38736140727996826, Accuracy = 0.8815858364105225\n",
      "Iter #1683456:  Learning rate = 0.002602:   Batch Loss = 0.427255, Accuracy = 0.859375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43327492475509644, Accuracy = 0.8708050847053528\n",
      "Iter #1685504:  Learning rate = 0.002602:   Batch Loss = 0.365531, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3963463306427002, Accuracy = 0.8786298036575317\n",
      "Iter #1687552:  Learning rate = 0.002602:   Batch Loss = 0.369478, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5012588500976562, Accuracy = 0.8490697145462036\n",
      "Iter #1689600:  Learning rate = 0.002602:   Batch Loss = 0.331300, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4382895827293396, Accuracy = 0.863502025604248\n",
      "Iter #1691648:  Learning rate = 0.002602:   Batch Loss = 0.364568, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41792938113212585, Accuracy = 0.8737610578536987\n",
      "Iter #1693696:  Learning rate = 0.002602:   Batch Loss = 0.304600, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4511758089065552, Accuracy = 0.8607198596000671\n",
      "Iter #1695744:  Learning rate = 0.002602:   Batch Loss = 0.326436, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40522098541259766, Accuracy = 0.8801947236061096\n",
      "Iter #1697792:  Learning rate = 0.002602:   Batch Loss = 0.363697, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40594327449798584, Accuracy = 0.8748043775558472\n",
      "Iter #1699840:  Learning rate = 0.002602:   Batch Loss = 0.330174, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41065436601638794, Accuracy = 0.8786298036575317\n",
      "Iter #1701888:  Learning rate = 0.002498:   Batch Loss = 0.326980, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3848969042301178, Accuracy = 0.8873239159584045\n",
      "Iter #1703936:  Learning rate = 0.002498:   Batch Loss = 0.370865, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41621896624565125, Accuracy = 0.8697617650032043\n",
      "Iter #1705984:  Learning rate = 0.002498:   Batch Loss = 0.324229, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41013962030410767, Accuracy = 0.8770648837089539\n",
      "Iter #1708032:  Learning rate = 0.002498:   Batch Loss = 0.378941, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4023975133895874, Accuracy = 0.8758476972579956\n",
      "Iter #1710080:  Learning rate = 0.002498:   Batch Loss = 0.366937, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4248921871185303, Accuracy = 0.8702834248542786\n",
      "Iter #1712128:  Learning rate = 0.002498:   Batch Loss = 0.356740, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4201432168483734, Accuracy = 0.8725439310073853\n",
      "Iter #1714176:  Learning rate = 0.002498:   Batch Loss = 0.352119, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3898744285106659, Accuracy = 0.8822813630104065\n",
      "Iter #1716224:  Learning rate = 0.002498:   Batch Loss = 0.365476, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5290101170539856, Accuracy = 0.8372457027435303\n",
      "Iter #1718272:  Learning rate = 0.002498:   Batch Loss = 0.399064, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5048778653144836, Accuracy = 0.8442010283470154\n",
      "Iter #1720320:  Learning rate = 0.002498:   Batch Loss = 0.386649, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4690762460231781, Accuracy = 0.8497652411460876\n",
      "Iter #1722368:  Learning rate = 0.002498:   Batch Loss = 0.369088, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.48446333408355713, Accuracy = 0.8521996140480042\n",
      "Iter #1724416:  Learning rate = 0.002498:   Batch Loss = 0.359863, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5246918201446533, Accuracy = 0.8377673625946045\n",
      "Iter #1726464:  Learning rate = 0.002498:   Batch Loss = 0.333899, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4281533360481262, Accuracy = 0.8697617650032043\n",
      "Iter #1728512:  Learning rate = 0.002498:   Batch Loss = 0.395422, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39858701825141907, Accuracy = 0.8819335699081421\n",
      "Iter #1730560:  Learning rate = 0.002498:   Batch Loss = 0.403802, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39915502071380615, Accuracy = 0.8768909573554993\n",
      "Iter #1732608:  Learning rate = 0.002498:   Batch Loss = 0.289367, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4068823754787445, Accuracy = 0.8814119100570679\n",
      "Iter #1734656:  Learning rate = 0.002498:   Batch Loss = 0.406383, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38761481642723083, Accuracy = 0.8838462829589844\n",
      "Iter #1736704:  Learning rate = 0.002498:   Batch Loss = 0.359413, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3789635896682739, Accuracy = 0.8831507563591003\n",
      "Iter #1738752:  Learning rate = 0.002498:   Batch Loss = 0.354760, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39730456471443176, Accuracy = 0.8808902502059937\n",
      "Iter #1740800:  Learning rate = 0.002498:   Batch Loss = 0.360145, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4359450936317444, Accuracy = 0.8629803657531738\n",
      "Iter #1742848:  Learning rate = 0.002498:   Batch Loss = 0.379930, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3973952829837799, Accuracy = 0.8814119100570679\n",
      "Iter #1744896:  Learning rate = 0.002498:   Batch Loss = 0.303294, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38852599263191223, Accuracy = 0.8796731233596802\n",
      "Iter #1746944:  Learning rate = 0.002498:   Batch Loss = 0.352189, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3888522982597351, Accuracy = 0.8852373361587524\n",
      "Iter #1748992:  Learning rate = 0.002498:   Batch Loss = 0.298138, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4136081039905548, Accuracy = 0.8695878982543945\n",
      "Iter #1751040:  Learning rate = 0.002498:   Batch Loss = 0.345232, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38514697551727295, Accuracy = 0.8815858364105225\n",
      "Iter #1753088:  Learning rate = 0.002498:   Batch Loss = 0.352710, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38007763028144836, Accuracy = 0.888367235660553\n",
      "Iter #1755136:  Learning rate = 0.002498:   Batch Loss = 0.266367, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3799062669277191, Accuracy = 0.8909754753112793\n",
      "Iter #1757184:  Learning rate = 0.002498:   Batch Loss = 0.344337, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4296548366546631, Accuracy = 0.8765432238578796\n",
      "Iter #1759232:  Learning rate = 0.002498:   Batch Loss = 0.272348, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4063451886177063, Accuracy = 0.8761954307556152\n",
      "Iter #1761280:  Learning rate = 0.002498:   Batch Loss = 0.334446, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38926559686660767, Accuracy = 0.8789775967597961\n",
      "Iter #1763328:  Learning rate = 0.002498:   Batch Loss = 0.312651, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4368076026439667, Accuracy = 0.8647191524505615\n",
      "Iter #1765376:  Learning rate = 0.002498:   Batch Loss = 0.325627, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3922728896141052, Accuracy = 0.8751521706581116\n",
      "Iter #1767424:  Learning rate = 0.002498:   Batch Loss = 0.334789, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4205261468887329, Accuracy = 0.8701095581054688\n",
      "Iter #1769472:  Learning rate = 0.002498:   Batch Loss = 0.273906, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3871101438999176, Accuracy = 0.8807163834571838\n",
      "Iter #1771520:  Learning rate = 0.002498:   Batch Loss = 0.292378, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3793187141418457, Accuracy = 0.8834985494613647\n",
      "Iter #1773568:  Learning rate = 0.002498:   Batch Loss = 0.358757, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3802695572376251, Accuracy = 0.8852373361587524\n",
      "Iter #1775616:  Learning rate = 0.002498:   Batch Loss = 0.423127, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3878123462200165, Accuracy = 0.8850634694099426\n",
      "Iter #1777664:  Learning rate = 0.002498:   Batch Loss = 0.364280, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4169893264770508, Accuracy = 0.875673770904541\n",
      "Iter #1779712:  Learning rate = 0.002498:   Batch Loss = 0.316269, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4224642217159271, Accuracy = 0.8673273921012878\n",
      "Iter #1781760:  Learning rate = 0.002498:   Batch Loss = 0.331486, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3803359866142273, Accuracy = 0.8841940760612488\n",
      "Iter #1783808:  Learning rate = 0.002498:   Batch Loss = 0.376679, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37841224670410156, Accuracy = 0.8948009014129639\n",
      "Iter #1785856:  Learning rate = 0.002498:   Batch Loss = 0.303024, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3726491928100586, Accuracy = 0.8880195021629333\n",
      "Iter #1787904:  Learning rate = 0.002498:   Batch Loss = 0.333712, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3790219724178314, Accuracy = 0.8829768896102905\n",
      "Iter #1789952:  Learning rate = 0.002498:   Batch Loss = 0.341176, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4072783887386322, Accuracy = 0.8808902502059937\n",
      "Iter #1792000:  Learning rate = 0.002498:   Batch Loss = 0.368336, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4277838468551636, Accuracy = 0.8645452857017517\n",
      "Iter #1794048:  Learning rate = 0.002498:   Batch Loss = 0.315059, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43581104278564453, Accuracy = 0.8690662384033203\n",
      "Iter #1796096:  Learning rate = 0.002498:   Batch Loss = 0.319484, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.44156545400619507, Accuracy = 0.8669796586036682\n",
      "Iter #1798144:  Learning rate = 0.002498:   Batch Loss = 0.412188, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4125690460205078, Accuracy = 0.874978244304657\n",
      "Iter #1800192:  Learning rate = 0.002398:   Batch Loss = 0.340171, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39009833335876465, Accuracy = 0.8826290965080261\n",
      "Iter #1802240:  Learning rate = 0.002398:   Batch Loss = 0.306316, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36319440603256226, Accuracy = 0.89393150806427\n",
      "Iter #1804288:  Learning rate = 0.002398:   Batch Loss = 0.274810, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38320067524909973, Accuracy = 0.8810641765594482\n",
      "Iter #1806336:  Learning rate = 0.002398:   Batch Loss = 0.231441, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3605351448059082, Accuracy = 0.8937575817108154\n",
      "Iter #1808384:  Learning rate = 0.002398:   Batch Loss = 0.322208, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.403478741645813, Accuracy = 0.8826290965080261\n",
      "Iter #1810432:  Learning rate = 0.002398:   Batch Loss = 0.367708, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3920062184333801, Accuracy = 0.889062762260437\n",
      "Iter #1812480:  Learning rate = 0.002398:   Batch Loss = 0.343966, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39125460386276245, Accuracy = 0.879151463508606\n",
      "Iter #1814528:  Learning rate = 0.002398:   Batch Loss = 0.291313, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3667960464954376, Accuracy = 0.8913232684135437\n",
      "Iter #1816576:  Learning rate = 0.002398:   Batch Loss = 0.314680, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3585923910140991, Accuracy = 0.8974091410636902\n",
      "Iter #1818624:  Learning rate = 0.002398:   Batch Loss = 0.332376, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3551546335220337, Accuracy = 0.8937575817108154\n",
      "Iter #1820672:  Learning rate = 0.002398:   Batch Loss = 0.382895, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3586183786392212, Accuracy = 0.8941053748130798\n",
      "Iter #1822720:  Learning rate = 0.002398:   Batch Loss = 0.312439, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.380248099565506, Accuracy = 0.8838462829589844\n",
      "Iter #1824768:  Learning rate = 0.002398:   Batch Loss = 0.303850, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3794151842594147, Accuracy = 0.8833246231079102\n",
      "Iter #1826816:  Learning rate = 0.002398:   Batch Loss = 0.276789, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35859179496765137, Accuracy = 0.8904538154602051\n",
      "Iter #1828864:  Learning rate = 0.002398:   Batch Loss = 0.267497, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3781866431236267, Accuracy = 0.8864545226097107\n",
      "Iter #1830912:  Learning rate = 0.002398:   Batch Loss = 0.340136, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3590685725212097, Accuracy = 0.8956702947616577\n",
      "Iter #1832960:  Learning rate = 0.002398:   Batch Loss = 0.305088, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.381195068359375, Accuracy = 0.875673770904541\n",
      "Iter #1835008:  Learning rate = 0.002398:   Batch Loss = 0.271989, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35961151123046875, Accuracy = 0.8902799487113953\n",
      "Iter #1837056:  Learning rate = 0.002398:   Batch Loss = 0.308599, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3625156879425049, Accuracy = 0.8857589960098267\n",
      "Iter #1839104:  Learning rate = 0.002398:   Batch Loss = 0.277623, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3692348897457123, Accuracy = 0.8892366290092468\n",
      "Iter #1841152:  Learning rate = 0.002398:   Batch Loss = 0.333260, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36740365624427795, Accuracy = 0.8913232684135437\n",
      "Iter #1843200:  Learning rate = 0.002398:   Batch Loss = 0.314171, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36056843400001526, Accuracy = 0.8916710019111633\n",
      "Iter #1845248:  Learning rate = 0.002398:   Batch Loss = 0.298036, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40439847111701965, Accuracy = 0.8777604103088379\n",
      "Iter #1847296:  Learning rate = 0.002398:   Batch Loss = 0.366340, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3764209449291229, Accuracy = 0.8840201497077942\n",
      "Iter #1849344:  Learning rate = 0.002398:   Batch Loss = 0.334589, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.49255016446113586, Accuracy = 0.8422883152961731\n",
      "Iter #1851392:  Learning rate = 0.002398:   Batch Loss = 0.402801, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.46797502040863037, Accuracy = 0.8457659482955933\n",
      "Iter #1853440:  Learning rate = 0.002398:   Batch Loss = 0.300097, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.42194950580596924, Accuracy = 0.8732394576072693\n",
      "Iter #1855488:  Learning rate = 0.002398:   Batch Loss = 0.373832, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4041329026222229, Accuracy = 0.8761954307556152\n",
      "Iter #1857536:  Learning rate = 0.002398:   Batch Loss = 0.343858, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3939136266708374, Accuracy = 0.8829768896102905\n",
      "Iter #1859584:  Learning rate = 0.002398:   Batch Loss = 0.380053, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38561156392097473, Accuracy = 0.8831507563591003\n",
      "Iter #1861632:  Learning rate = 0.002398:   Batch Loss = 0.287645, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3956163227558136, Accuracy = 0.8808902502059937\n",
      "Iter #1863680:  Learning rate = 0.002398:   Batch Loss = 0.269590, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3566248416900635, Accuracy = 0.8951486945152283\n",
      "Iter #1865728:  Learning rate = 0.002398:   Batch Loss = 0.316391, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34497836232185364, Accuracy = 0.8984524607658386\n",
      "Iter #1867776:  Learning rate = 0.002398:   Batch Loss = 0.284371, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34919431805610657, Accuracy = 0.9010607004165649\n",
      "Iter #1869824:  Learning rate = 0.002398:   Batch Loss = 0.358179, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37621867656707764, Accuracy = 0.8911493420600891\n",
      "Iter #1871872:  Learning rate = 0.002398:   Batch Loss = 0.291417, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38174501061439514, Accuracy = 0.8841940760612488\n",
      "Iter #1873920:  Learning rate = 0.002398:   Batch Loss = 0.584046, Accuracy = 0.828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.395075261592865, Accuracy = 0.8814119100570679\n",
      "Iter #1875968:  Learning rate = 0.002398:   Batch Loss = 0.413853, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4414026737213135, Accuracy = 0.8662841320037842\n",
      "Iter #1878016:  Learning rate = 0.002398:   Batch Loss = 0.295730, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3654935657978058, Accuracy = 0.888367235660553\n",
      "Iter #1880064:  Learning rate = 0.002398:   Batch Loss = 0.326480, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3564693331718445, Accuracy = 0.8972352743148804\n",
      "Iter #1882112:  Learning rate = 0.002398:   Batch Loss = 0.341036, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3611885905265808, Accuracy = 0.8934098482131958\n",
      "Iter #1884160:  Learning rate = 0.002398:   Batch Loss = 0.335503, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35751399397850037, Accuracy = 0.8961919546127319\n",
      "Iter #1886208:  Learning rate = 0.002398:   Batch Loss = 0.264305, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34756502509117126, Accuracy = 0.8974091410636902\n",
      "Iter #1888256:  Learning rate = 0.002398:   Batch Loss = 0.363695, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38621294498443604, Accuracy = 0.8899322152137756\n",
      "Iter #1890304:  Learning rate = 0.002398:   Batch Loss = 0.302376, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3398950397968292, Accuracy = 0.9010607004165649\n",
      "Iter #1892352:  Learning rate = 0.002398:   Batch Loss = 0.322568, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35647493600845337, Accuracy = 0.8914971351623535\n",
      "Iter #1894400:  Learning rate = 0.002398:   Batch Loss = 0.351755, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3517701327800751, Accuracy = 0.8937575817108154\n",
      "Iter #1896448:  Learning rate = 0.002398:   Batch Loss = 0.360036, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40526777505874634, Accuracy = 0.8716744780540466\n",
      "Iter #1898496:  Learning rate = 0.002398:   Batch Loss = 0.313267, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36315613985061646, Accuracy = 0.8848896026611328\n",
      "Iter #1900544:  Learning rate = 0.002302:   Batch Loss = 0.296126, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39020752906799316, Accuracy = 0.8803686499595642\n",
      "Iter #1902592:  Learning rate = 0.002302:   Batch Loss = 0.310573, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34108513593673706, Accuracy = 0.8988001942634583\n",
      "Iter #1904640:  Learning rate = 0.002302:   Batch Loss = 0.328284, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3519834280014038, Accuracy = 0.8951486945152283\n",
      "Iter #1906688:  Learning rate = 0.002302:   Batch Loss = 0.299638, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3498348295688629, Accuracy = 0.8921926617622375\n",
      "Iter #1908736:  Learning rate = 0.002302:   Batch Loss = 0.319711, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38628748059272766, Accuracy = 0.8840201497077942\n",
      "Iter #1910784:  Learning rate = 0.002302:   Batch Loss = 0.345367, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3588131070137024, Accuracy = 0.8941053748130798\n",
      "Iter #1912832:  Learning rate = 0.002302:   Batch Loss = 0.296776, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34605872631073, Accuracy = 0.8965397477149963\n",
      "Iter #1914880:  Learning rate = 0.002302:   Batch Loss = 0.297215, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3420370817184448, Accuracy = 0.8949747681617737\n",
      "Iter #1916928:  Learning rate = 0.002302:   Batch Loss = 0.297426, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3392248749732971, Accuracy = 0.9000173807144165\n",
      "Iter #1918976:  Learning rate = 0.002302:   Batch Loss = 0.234263, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34003937244415283, Accuracy = 0.8996696472167969\n",
      "Iter #1921024:  Learning rate = 0.002302:   Batch Loss = 0.386054, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3339185118675232, Accuracy = 0.9034950733184814\n",
      "Iter #1923072:  Learning rate = 0.002302:   Batch Loss = 0.271336, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34187883138656616, Accuracy = 0.898278534412384\n",
      "Iter #1925120:  Learning rate = 0.002302:   Batch Loss = 0.282098, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3481534719467163, Accuracy = 0.9000173807144165\n",
      "Iter #1927168:  Learning rate = 0.002302:   Batch Loss = 0.355335, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3813355565071106, Accuracy = 0.888367235660553\n",
      "Iter #1929216:  Learning rate = 0.002302:   Batch Loss = 0.344728, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40588799118995667, Accuracy = 0.8732394576072693\n",
      "Iter #1931264:  Learning rate = 0.002302:   Batch Loss = 0.287618, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34905722737312317, Accuracy = 0.8894105553627014\n",
      "Iter #1933312:  Learning rate = 0.002302:   Batch Loss = 0.295969, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35459664463996887, Accuracy = 0.8944531679153442\n",
      "Iter #1935360:  Learning rate = 0.002302:   Batch Loss = 0.288794, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3687727153301239, Accuracy = 0.8850634694099426\n",
      "Iter #1937408:  Learning rate = 0.002302:   Batch Loss = 0.302109, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34056007862091064, Accuracy = 0.8986263275146484\n",
      "Iter #1939456:  Learning rate = 0.002302:   Batch Loss = 0.331989, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.5307395458221436, Accuracy = 0.8374195694923401\n",
      "Iter #1941504:  Learning rate = 0.002302:   Batch Loss = 0.520604, Accuracy = 0.84765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45511317253112793, Accuracy = 0.8544601202011108\n",
      "Iter #1943552:  Learning rate = 0.002302:   Batch Loss = 0.385683, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45427775382995605, Accuracy = 0.852373480796814\n",
      "Iter #1945600:  Learning rate = 0.002302:   Batch Loss = 0.376697, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3707053065299988, Accuracy = 0.8909754753112793\n",
      "Iter #1947648:  Learning rate = 0.002302:   Batch Loss = 0.320676, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.353246808052063, Accuracy = 0.8948009014129639\n",
      "Iter #1949696:  Learning rate = 0.002302:   Batch Loss = 0.350833, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37454065680503845, Accuracy = 0.8887150287628174\n",
      "Iter #1951744:  Learning rate = 0.002302:   Batch Loss = 0.479382, Accuracy = 0.8359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37872692942619324, Accuracy = 0.8845418095588684\n",
      "Iter #1953792:  Learning rate = 0.002302:   Batch Loss = 0.337931, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.382318913936615, Accuracy = 0.8796731233596802\n",
      "Iter #1955840:  Learning rate = 0.002302:   Batch Loss = 0.490494, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3686903715133667, Accuracy = 0.8895844221115112\n",
      "Iter #1957888:  Learning rate = 0.002302:   Batch Loss = 0.416070, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38538652658462524, Accuracy = 0.8833246231079102\n",
      "Iter #1959936:  Learning rate = 0.002302:   Batch Loss = 0.311903, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38528865575790405, Accuracy = 0.8840201497077942\n",
      "Iter #1961984:  Learning rate = 0.002302:   Batch Loss = 0.293896, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34993189573287964, Accuracy = 0.8956702947616577\n",
      "Iter #1964032:  Learning rate = 0.002302:   Batch Loss = 0.291203, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3702782690525055, Accuracy = 0.8855851292610168\n",
      "Iter #1966080:  Learning rate = 0.002302:   Batch Loss = 0.321798, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3545447289943695, Accuracy = 0.8902799487113953\n",
      "Iter #1968128:  Learning rate = 0.002302:   Batch Loss = 0.336368, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33686819672584534, Accuracy = 0.8991479873657227\n",
      "Iter #1970176:  Learning rate = 0.002302:   Batch Loss = 0.294526, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34136030077934265, Accuracy = 0.8981046676635742\n",
      "Iter #1972224:  Learning rate = 0.002302:   Batch Loss = 0.332404, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35551247000694275, Accuracy = 0.8937575817108154\n",
      "Iter #1974272:  Learning rate = 0.002302:   Batch Loss = 0.328452, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36834490299224854, Accuracy = 0.8782820105552673\n",
      "Iter #1976320:  Learning rate = 0.002302:   Batch Loss = 0.322231, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4499664902687073, Accuracy = 0.8626325726509094\n",
      "Iter #1978368:  Learning rate = 0.002302:   Batch Loss = 0.369747, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34400880336761475, Accuracy = 0.8954964280128479\n",
      "Iter #1980416:  Learning rate = 0.002302:   Batch Loss = 0.356057, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37581002712249756, Accuracy = 0.8789775967597961\n",
      "Iter #1982464:  Learning rate = 0.002302:   Batch Loss = 0.344882, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36474865674972534, Accuracy = 0.8868023157119751\n",
      "Iter #1984512:  Learning rate = 0.002302:   Batch Loss = 0.285456, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3629571497440338, Accuracy = 0.8942792415618896\n",
      "Iter #1986560:  Learning rate = 0.002302:   Batch Loss = 0.328184, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33387672901153564, Accuracy = 0.9012345671653748\n",
      "Iter #1988608:  Learning rate = 0.002302:   Batch Loss = 0.267254, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3424224853515625, Accuracy = 0.8984524607658386\n",
      "Iter #1990656:  Learning rate = 0.002302:   Batch Loss = 0.378028, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34857505559921265, Accuracy = 0.8991479873657227\n",
      "Iter #1992704:  Learning rate = 0.002302:   Batch Loss = 0.283226, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33624470233917236, Accuracy = 0.9007129073143005\n",
      "Iter #1994752:  Learning rate = 0.002302:   Batch Loss = 0.332016, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33969631791114807, Accuracy = 0.9033211469650269\n",
      "Iter #1996800:  Learning rate = 0.002302:   Batch Loss = 0.295467, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3375701308250427, Accuracy = 0.9015823602676392\n",
      "Iter #1998848:  Learning rate = 0.002302:   Batch Loss = 0.251829, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3523755669593811, Accuracy = 0.8958442211151123\n",
      "Iter #2000896:  Learning rate = 0.002210:   Batch Loss = 0.348097, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3419780135154724, Accuracy = 0.8956702947616577\n",
      "Iter #2002944:  Learning rate = 0.002210:   Batch Loss = 0.244536, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3290005028247833, Accuracy = 0.8996696472167969\n",
      "Iter #2004992:  Learning rate = 0.002210:   Batch Loss = 0.325327, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3215731978416443, Accuracy = 0.9000173807144165\n",
      "Iter #2007040:  Learning rate = 0.002210:   Batch Loss = 0.267284, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3416445255279541, Accuracy = 0.8934098482131958\n",
      "Iter #2009088:  Learning rate = 0.002210:   Batch Loss = 0.309038, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32025882601737976, Accuracy = 0.9067988395690918\n",
      "Iter #2011136:  Learning rate = 0.002210:   Batch Loss = 0.335843, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34646281599998474, Accuracy = 0.8894105553627014\n",
      "Iter #2013184:  Learning rate = 0.002210:   Batch Loss = 0.285408, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3527500331401825, Accuracy = 0.8908016085624695\n",
      "Iter #2015232:  Learning rate = 0.002210:   Batch Loss = 0.304289, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3274823725223541, Accuracy = 0.9048861265182495\n",
      "Iter #2017280:  Learning rate = 0.002210:   Batch Loss = 0.272037, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3536936342716217, Accuracy = 0.8951486945152283\n",
      "Iter #2019328:  Learning rate = 0.002210:   Batch Loss = 0.278661, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3642606735229492, Accuracy = 0.8986263275146484\n",
      "Iter #2021376:  Learning rate = 0.002210:   Batch Loss = 0.278048, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32202139496803284, Accuracy = 0.9066249132156372\n",
      "Iter #2023424:  Learning rate = 0.002210:   Batch Loss = 0.272068, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33736780285835266, Accuracy = 0.9010607004165649\n",
      "Iter #2025472:  Learning rate = 0.002210:   Batch Loss = 0.303935, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3359638750553131, Accuracy = 0.9066249132156372\n",
      "Iter #2027520:  Learning rate = 0.002210:   Batch Loss = 0.314046, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35261520743370056, Accuracy = 0.8921926617622375\n",
      "Iter #2029568:  Learning rate = 0.002210:   Batch Loss = 0.305154, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33438000082969666, Accuracy = 0.894627034664154\n",
      "Iter #2031616:  Learning rate = 0.002210:   Batch Loss = 0.224990, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3509546220302582, Accuracy = 0.892540454864502\n",
      "Iter #2033664:  Learning rate = 0.002210:   Batch Loss = 0.274285, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3312686085700989, Accuracy = 0.9021039605140686\n",
      "Iter #2035712:  Learning rate = 0.002210:   Batch Loss = 0.323951, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3413570821285248, Accuracy = 0.8951486945152283\n",
      "Iter #2037760:  Learning rate = 0.002210:   Batch Loss = 0.274936, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3478444814682007, Accuracy = 0.8949747681617737\n",
      "Iter #2039808:  Learning rate = 0.002210:   Batch Loss = 0.238620, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36583155393600464, Accuracy = 0.8871500492095947\n",
      "Iter #2041856:  Learning rate = 0.002210:   Batch Loss = 0.292372, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36602041125297546, Accuracy = 0.8868023157119751\n",
      "Iter #2043904:  Learning rate = 0.002210:   Batch Loss = 0.273473, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3515792191028595, Accuracy = 0.8918448686599731\n",
      "Iter #2045952:  Learning rate = 0.002210:   Batch Loss = 0.279486, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3382154107093811, Accuracy = 0.8972352743148804\n",
      "Iter #2048000:  Learning rate = 0.002210:   Batch Loss = 0.260043, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3538372218608856, Accuracy = 0.8948009014129639\n",
      "Iter #2050048:  Learning rate = 0.002210:   Batch Loss = 0.272457, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33777910470962524, Accuracy = 0.8977569341659546\n",
      "Iter #2052096:  Learning rate = 0.002210:   Batch Loss = 0.278745, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3375428020954132, Accuracy = 0.9008867740631104\n",
      "Iter #2054144:  Learning rate = 0.002210:   Batch Loss = 0.303338, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3390255570411682, Accuracy = 0.901756227016449\n",
      "Iter #2056192:  Learning rate = 0.002210:   Batch Loss = 0.336515, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3991347551345825, Accuracy = 0.8767170906066895\n",
      "Iter #2058240:  Learning rate = 0.002210:   Batch Loss = 0.383597, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34100088477134705, Accuracy = 0.9019300937652588\n",
      "Iter #2060288:  Learning rate = 0.002210:   Batch Loss = 0.243657, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40200477838516235, Accuracy = 0.8833246231079102\n",
      "Iter #2062336:  Learning rate = 0.002210:   Batch Loss = 0.285850, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38705533742904663, Accuracy = 0.8852373361587524\n",
      "Iter #2064384:  Learning rate = 0.002210:   Batch Loss = 0.384243, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.41091644763946533, Accuracy = 0.875673770904541\n",
      "Iter #2066432:  Learning rate = 0.002210:   Batch Loss = 0.342275, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3612588942050934, Accuracy = 0.893235981464386\n",
      "Iter #2068480:  Learning rate = 0.002210:   Batch Loss = 0.309571, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32816994190216064, Accuracy = 0.9041905999183655\n",
      "Iter #2070528:  Learning rate = 0.002210:   Batch Loss = 0.204751, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3280458450317383, Accuracy = 0.9073204398155212\n",
      "Iter #2072576:  Learning rate = 0.002210:   Batch Loss = 0.252645, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.318999320268631, Accuracy = 0.9090592861175537\n",
      "Iter #2074624:  Learning rate = 0.002210:   Batch Loss = 0.291164, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34261322021484375, Accuracy = 0.8994957208633423\n",
      "Iter #2076672:  Learning rate = 0.002210:   Batch Loss = 0.293024, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36030271649360657, Accuracy = 0.8906277418136597\n",
      "Iter #2078720:  Learning rate = 0.002210:   Batch Loss = 0.326268, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.443400502204895, Accuracy = 0.8666318655014038\n",
      "Iter #2080768:  Learning rate = 0.002210:   Batch Loss = 0.286898, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3559756278991699, Accuracy = 0.8930620551109314\n",
      "Iter #2082816:  Learning rate = 0.002210:   Batch Loss = 0.280088, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37960976362228394, Accuracy = 0.893235981464386\n",
      "Iter #2084864:  Learning rate = 0.002210:   Batch Loss = 0.275569, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34773945808410645, Accuracy = 0.8942792415618896\n",
      "Iter #2086912:  Learning rate = 0.002210:   Batch Loss = 0.298350, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3512888252735138, Accuracy = 0.8930620551109314\n",
      "Iter #2088960:  Learning rate = 0.002210:   Batch Loss = 0.317529, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40858858823776245, Accuracy = 0.8772387504577637\n",
      "Iter #2091008:  Learning rate = 0.002210:   Batch Loss = 0.341117, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.39927494525909424, Accuracy = 0.8801947236061096\n",
      "Iter #2093056:  Learning rate = 0.002210:   Batch Loss = 0.331767, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35035964846611023, Accuracy = 0.8904538154602051\n",
      "Iter #2095104:  Learning rate = 0.002210:   Batch Loss = 0.369525, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3407917618751526, Accuracy = 0.9000173807144165\n",
      "Iter #2097152:  Learning rate = 0.002210:   Batch Loss = 0.257607, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3472795784473419, Accuracy = 0.896887481212616\n",
      "Iter #2099200:  Learning rate = 0.002210:   Batch Loss = 0.332609, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3628007769584656, Accuracy = 0.885411262512207\n",
      "Iter #2101248:  Learning rate = 0.002122:   Batch Loss = 0.274855, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33575084805488586, Accuracy = 0.902451753616333\n",
      "Iter #2103296:  Learning rate = 0.002122:   Batch Loss = 0.321915, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33837151527404785, Accuracy = 0.9029734134674072\n",
      "Iter #2105344:  Learning rate = 0.002122:   Batch Loss = 0.339315, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3522874116897583, Accuracy = 0.8988001942634583\n",
      "Iter #2107392:  Learning rate = 0.002122:   Batch Loss = 0.294244, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3533696234226227, Accuracy = 0.8937575817108154\n",
      "Iter #2109440:  Learning rate = 0.002122:   Batch Loss = 0.413045, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43923047184944153, Accuracy = 0.8704572916030884\n",
      "Iter #2111488:  Learning rate = 0.002122:   Batch Loss = 0.317147, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3295620381832123, Accuracy = 0.8998435139656067\n",
      "Iter #2113536:  Learning rate = 0.002122:   Batch Loss = 0.334812, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3308464288711548, Accuracy = 0.8998435139656067\n",
      "Iter #2115584:  Learning rate = 0.002122:   Batch Loss = 0.258052, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33490225672721863, Accuracy = 0.9015823602676392\n",
      "Iter #2117632:  Learning rate = 0.002122:   Batch Loss = 0.221970, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32088083028793335, Accuracy = 0.9052338600158691\n",
      "Iter #2119680:  Learning rate = 0.002122:   Batch Loss = 0.363381, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32592591643333435, Accuracy = 0.9052338600158691\n",
      "Iter #2121728:  Learning rate = 0.002122:   Batch Loss = 0.217040, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3208785951137543, Accuracy = 0.9052338600158691\n",
      "Iter #2123776:  Learning rate = 0.002122:   Batch Loss = 0.225792, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4082580804824829, Accuracy = 0.8775864839553833\n",
      "Iter #2125824:  Learning rate = 0.002122:   Batch Loss = 0.308324, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3678337037563324, Accuracy = 0.8868023157119751\n",
      "Iter #2127872:  Learning rate = 0.002122:   Batch Loss = 0.303900, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3300483524799347, Accuracy = 0.9045383334159851\n",
      "Iter #2129920:  Learning rate = 0.002122:   Batch Loss = 0.315975, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3333827257156372, Accuracy = 0.8974091410636902\n",
      "Iter #2131968:  Learning rate = 0.002122:   Batch Loss = 0.284310, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32222670316696167, Accuracy = 0.9033211469650269\n",
      "Iter #2134016:  Learning rate = 0.002122:   Batch Loss = 0.243696, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3219483196735382, Accuracy = 0.9010607004165649\n",
      "Iter #2136064:  Learning rate = 0.002122:   Batch Loss = 0.255170, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3621777296066284, Accuracy = 0.8881933689117432\n",
      "Iter #2138112:  Learning rate = 0.002122:   Batch Loss = 0.333553, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3752315938472748, Accuracy = 0.8864545226097107\n",
      "Iter #2140160:  Learning rate = 0.002122:   Batch Loss = 0.349832, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3101065456867218, Accuracy = 0.9106242656707764\n",
      "Iter #2142208:  Learning rate = 0.002122:   Batch Loss = 0.295985, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3209962248802185, Accuracy = 0.9069727063179016\n",
      "Iter #2144256:  Learning rate = 0.002122:   Batch Loss = 0.267837, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3301258087158203, Accuracy = 0.9014084339141846\n",
      "Iter #2146304:  Learning rate = 0.002122:   Batch Loss = 0.251068, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3327205777168274, Accuracy = 0.8988001942634583\n",
      "Iter #2148352:  Learning rate = 0.002122:   Batch Loss = 0.228590, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37539878487586975, Accuracy = 0.8873239159584045\n",
      "Iter #2150400:  Learning rate = 0.002122:   Batch Loss = 0.223170, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.40425968170166016, Accuracy = 0.876369297504425\n",
      "Iter #2152448:  Learning rate = 0.002122:   Batch Loss = 0.381528, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.318435937166214, Accuracy = 0.9085376262664795\n",
      "Iter #2154496:  Learning rate = 0.002122:   Batch Loss = 0.305600, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3165612518787384, Accuracy = 0.9069727063179016\n",
      "Iter #2156544:  Learning rate = 0.002122:   Batch Loss = 0.262978, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3177318274974823, Accuracy = 0.9078420996665955\n",
      "Iter #2158592:  Learning rate = 0.002122:   Batch Loss = 0.398373, Accuracy = 0.85546875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37510257959365845, Accuracy = 0.8866283893585205\n",
      "Iter #2160640:  Learning rate = 0.002122:   Batch Loss = 0.339114, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34454888105392456, Accuracy = 0.8996696472167969\n",
      "Iter #2162688:  Learning rate = 0.002122:   Batch Loss = 0.358281, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3239874839782715, Accuracy = 0.9045383334159851\n",
      "Iter #2164736:  Learning rate = 0.002122:   Batch Loss = 0.316739, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32949331402778625, Accuracy = 0.9010607004165649\n",
      "Iter #2166784:  Learning rate = 0.002122:   Batch Loss = 0.257962, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35542476177215576, Accuracy = 0.889758288860321\n",
      "Iter #2168832:  Learning rate = 0.002122:   Batch Loss = 0.278162, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32259225845336914, Accuracy = 0.9022778868675232\n",
      "Iter #2170880:  Learning rate = 0.002122:   Batch Loss = 0.309122, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32959577441215515, Accuracy = 0.9019300937652588\n",
      "Iter #2172928:  Learning rate = 0.002122:   Batch Loss = 0.308093, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3599134683609009, Accuracy = 0.8937575817108154\n",
      "Iter #2174976:  Learning rate = 0.002122:   Batch Loss = 0.308844, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3654147684574127, Accuracy = 0.888367235660553\n",
      "Iter #2177024:  Learning rate = 0.002122:   Batch Loss = 0.268686, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3599681258201599, Accuracy = 0.8906277418136597\n",
      "Iter #2179072:  Learning rate = 0.002122:   Batch Loss = 0.258021, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3487282395362854, Accuracy = 0.9001912474632263\n",
      "Iter #2181120:  Learning rate = 0.002122:   Batch Loss = 0.336398, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3388436436653137, Accuracy = 0.9008867740631104\n",
      "Iter #2183168:  Learning rate = 0.002122:   Batch Loss = 0.278019, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35674530267715454, Accuracy = 0.8913232684135437\n",
      "Iter #2185216:  Learning rate = 0.002122:   Batch Loss = 0.346502, Accuracy = 0.87109375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3906293511390686, Accuracy = 0.8803686499595642\n",
      "Iter #2187264:  Learning rate = 0.002122:   Batch Loss = 0.327832, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33849918842315674, Accuracy = 0.901756227016449\n",
      "Iter #2189312:  Learning rate = 0.002122:   Batch Loss = 0.282915, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33466261625289917, Accuracy = 0.9043644666671753\n",
      "Iter #2191360:  Learning rate = 0.002122:   Batch Loss = 0.218573, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32219576835632324, Accuracy = 0.9019300937652588\n",
      "Iter #2193408:  Learning rate = 0.002122:   Batch Loss = 0.369245, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3827529847621918, Accuracy = 0.8868023157119751\n",
      "Iter #2195456:  Learning rate = 0.002122:   Batch Loss = 0.321388, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3571620583534241, Accuracy = 0.8956702947616577\n",
      "Iter #2197504:  Learning rate = 0.002122:   Batch Loss = 0.309865, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34463921189308167, Accuracy = 0.8977569341659546\n",
      "Iter #2199552:  Learning rate = 0.002122:   Batch Loss = 0.323077, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33763065934181213, Accuracy = 0.9010607004165649\n",
      "Iter #2201600:  Learning rate = 0.002037:   Batch Loss = 0.308447, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33715611696243286, Accuracy = 0.901756227016449\n",
      "Iter #2203648:  Learning rate = 0.002037:   Batch Loss = 0.346790, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3812040090560913, Accuracy = 0.8786298036575317\n",
      "Iter #2205696:  Learning rate = 0.002037:   Batch Loss = 0.290320, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.338114470243454, Accuracy = 0.8958442211151123\n",
      "Iter #2207744:  Learning rate = 0.002037:   Batch Loss = 0.322602, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3360495865345001, Accuracy = 0.8994957208633423\n",
      "Iter #2209792:  Learning rate = 0.002037:   Batch Loss = 0.289942, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3259320855140686, Accuracy = 0.9050599932670593\n",
      "Iter #2211840:  Learning rate = 0.002037:   Batch Loss = 0.234212, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.320004940032959, Accuracy = 0.9057555198669434\n",
      "Iter #2213888:  Learning rate = 0.002037:   Batch Loss = 0.315608, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3255890905857086, Accuracy = 0.901756227016449\n",
      "Iter #2215936:  Learning rate = 0.002037:   Batch Loss = 0.303541, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3497653603553772, Accuracy = 0.8960180878639221\n",
      "Iter #2217984:  Learning rate = 0.002037:   Batch Loss = 0.289479, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3696511387825012, Accuracy = 0.888367235660553\n",
      "Iter #2220032:  Learning rate = 0.002037:   Batch Loss = 0.319702, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33464112877845764, Accuracy = 0.8991479873657227\n",
      "Iter #2222080:  Learning rate = 0.002037:   Batch Loss = 0.239042, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.347373902797699, Accuracy = 0.8944531679153442\n",
      "Iter #2224128:  Learning rate = 0.002037:   Batch Loss = 0.260858, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3377110958099365, Accuracy = 0.9019300937652588\n",
      "Iter #2226176:  Learning rate = 0.002037:   Batch Loss = 0.317778, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3232342302799225, Accuracy = 0.9057555198669434\n",
      "Iter #2228224:  Learning rate = 0.002037:   Batch Loss = 0.348668, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3165104389190674, Accuracy = 0.9078420996665955\n",
      "Iter #2230272:  Learning rate = 0.002037:   Batch Loss = 0.243966, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3157961666584015, Accuracy = 0.9083637595176697\n",
      "Iter #2232320:  Learning rate = 0.002037:   Batch Loss = 0.321600, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30696600675582886, Accuracy = 0.9114936590194702\n",
      "Iter #2234368:  Learning rate = 0.002037:   Batch Loss = 0.298979, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.347674161195755, Accuracy = 0.8948009014129639\n",
      "Iter #2236416:  Learning rate = 0.002037:   Batch Loss = 0.222467, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33091774582862854, Accuracy = 0.9092331528663635\n",
      "Iter #2238464:  Learning rate = 0.002037:   Batch Loss = 0.244414, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32557475566864014, Accuracy = 0.9043644666671753\n",
      "Iter #2240512:  Learning rate = 0.002037:   Batch Loss = 0.244431, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32184913754463196, Accuracy = 0.8988001942634583\n",
      "Iter #2242560:  Learning rate = 0.002037:   Batch Loss = 0.248258, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34771430492401123, Accuracy = 0.8951486945152283\n",
      "Iter #2244608:  Learning rate = 0.002037:   Batch Loss = 0.249269, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32147032022476196, Accuracy = 0.9064510464668274\n",
      "Iter #2246656:  Learning rate = 0.002037:   Batch Loss = 0.214297, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3194228410720825, Accuracy = 0.9061033129692078\n",
      "Iter #2248704:  Learning rate = 0.002037:   Batch Loss = 0.290568, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36412736773490906, Accuracy = 0.89393150806427\n",
      "Iter #2250752:  Learning rate = 0.002037:   Batch Loss = 0.249914, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33821073174476624, Accuracy = 0.8996696472167969\n",
      "Iter #2252800:  Learning rate = 0.002037:   Batch Loss = 0.209875, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3361790180206299, Accuracy = 0.9033211469650269\n",
      "Iter #2254848:  Learning rate = 0.002037:   Batch Loss = 0.303744, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3384521007537842, Accuracy = 0.9007129073143005\n",
      "Iter #2256896:  Learning rate = 0.002037:   Batch Loss = 0.281247, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34238407015800476, Accuracy = 0.9003651738166809\n",
      "Iter #2258944:  Learning rate = 0.002037:   Batch Loss = 0.284291, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3299122154712677, Accuracy = 0.9022778868675232\n",
      "Iter #2260992:  Learning rate = 0.002037:   Batch Loss = 0.296075, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33175989985466003, Accuracy = 0.8984524607658386\n",
      "Iter #2263040:  Learning rate = 0.002037:   Batch Loss = 0.283902, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3487342894077301, Accuracy = 0.8977569341659546\n",
      "Iter #2265088:  Learning rate = 0.002037:   Batch Loss = 0.302844, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3207087814807892, Accuracy = 0.9052338600158691\n",
      "Iter #2267136:  Learning rate = 0.002037:   Batch Loss = 0.253235, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3347964286804199, Accuracy = 0.903147280216217\n",
      "Iter #2269184:  Learning rate = 0.002037:   Batch Loss = 0.247729, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32330936193466187, Accuracy = 0.910276472568512\n",
      "Iter #2271232:  Learning rate = 0.002037:   Batch Loss = 0.263039, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32694587111473083, Accuracy = 0.9034950733184814\n",
      "Iter #2273280:  Learning rate = 0.002037:   Batch Loss = 0.239877, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3408328890800476, Accuracy = 0.9001912474632263\n",
      "Iter #2275328:  Learning rate = 0.002037:   Batch Loss = 0.295846, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32562583684921265, Accuracy = 0.9067988395690918\n",
      "Iter #2277376:  Learning rate = 0.002037:   Batch Loss = 0.282976, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34126120805740356, Accuracy = 0.9022778868675232\n",
      "Iter #2279424:  Learning rate = 0.002037:   Batch Loss = 0.251724, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32727301120758057, Accuracy = 0.903147280216217\n",
      "Iter #2281472:  Learning rate = 0.002037:   Batch Loss = 0.280862, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32556742429733276, Accuracy = 0.9033211469650269\n",
      "Iter #2283520:  Learning rate = 0.002037:   Batch Loss = 0.242167, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3404388427734375, Accuracy = 0.9019300937652588\n",
      "Iter #2285568:  Learning rate = 0.002037:   Batch Loss = 0.294394, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33752623200416565, Accuracy = 0.9036689400672913\n",
      "Iter #2287616:  Learning rate = 0.002037:   Batch Loss = 0.289465, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3225290775299072, Accuracy = 0.910276472568512\n",
      "Iter #2289664:  Learning rate = 0.002037:   Batch Loss = 0.282538, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34679505228996277, Accuracy = 0.8928881883621216\n",
      "Iter #2291712:  Learning rate = 0.002037:   Batch Loss = 0.237886, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3362368643283844, Accuracy = 0.9003651738166809\n",
      "Iter #2293760:  Learning rate = 0.002037:   Batch Loss = 0.281661, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.45579925179481506, Accuracy = 0.8528951406478882\n",
      "Iter #2295808:  Learning rate = 0.002037:   Batch Loss = 0.427084, Accuracy = 0.875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.348558634519577, Accuracy = 0.9008867740631104\n",
      "Iter #2297856:  Learning rate = 0.002037:   Batch Loss = 0.292247, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3522995710372925, Accuracy = 0.8974091410636902\n",
      "Iter #2299904:  Learning rate = 0.002037:   Batch Loss = 0.328537, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34685128927230835, Accuracy = 0.8975830078125\n",
      "Iter #2301952:  Learning rate = 0.001955:   Batch Loss = 0.285476, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3396599292755127, Accuracy = 0.8989741206169128\n",
      "Iter #2304000:  Learning rate = 0.001955:   Batch Loss = 0.305736, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38926708698272705, Accuracy = 0.8866283893585205\n",
      "Iter #2306048:  Learning rate = 0.001955:   Batch Loss = 0.276649, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3300570249557495, Accuracy = 0.9033211469650269\n",
      "Iter #2308096:  Learning rate = 0.001955:   Batch Loss = 0.242993, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3357454538345337, Accuracy = 0.9001912474632263\n",
      "Iter #2310144:  Learning rate = 0.001955:   Batch Loss = 0.263311, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3244365155696869, Accuracy = 0.9038428068161011\n",
      "Iter #2312192:  Learning rate = 0.001955:   Batch Loss = 0.292074, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.323397159576416, Accuracy = 0.9066249132156372\n",
      "Iter #2314240:  Learning rate = 0.001955:   Batch Loss = 0.286119, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33907821774482727, Accuracy = 0.9034950733184814\n",
      "Iter #2316288:  Learning rate = 0.001955:   Batch Loss = 0.302997, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3199416697025299, Accuracy = 0.9071465730667114\n",
      "Iter #2318336:  Learning rate = 0.001955:   Batch Loss = 0.276777, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3153389096260071, Accuracy = 0.9094070792198181\n",
      "Iter #2320384:  Learning rate = 0.001955:   Batch Loss = 0.276641, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3179069757461548, Accuracy = 0.9038428068161011\n",
      "Iter #2322432:  Learning rate = 0.001955:   Batch Loss = 0.303548, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3287705183029175, Accuracy = 0.9048861265182495\n",
      "Iter #2324480:  Learning rate = 0.001955:   Batch Loss = 0.285849, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35247060656547546, Accuracy = 0.894627034664154\n",
      "Iter #2326528:  Learning rate = 0.001955:   Batch Loss = 0.219697, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3298908472061157, Accuracy = 0.9014084339141846\n",
      "Iter #2328576:  Learning rate = 0.001955:   Batch Loss = 0.218960, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32990390062332153, Accuracy = 0.9010607004165649\n",
      "Iter #2330624:  Learning rate = 0.001955:   Batch Loss = 0.341484, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34173744916915894, Accuracy = 0.8934098482131958\n",
      "Iter #2332672:  Learning rate = 0.001955:   Batch Loss = 0.227320, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3758181929588318, Accuracy = 0.8868023157119751\n",
      "Iter #2334720:  Learning rate = 0.001955:   Batch Loss = 0.290734, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3642534613609314, Accuracy = 0.896887481212616\n",
      "Iter #2336768:  Learning rate = 0.001955:   Batch Loss = 0.285250, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3511490821838379, Accuracy = 0.8954964280128479\n",
      "Iter #2338816:  Learning rate = 0.001955:   Batch Loss = 0.474356, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3587914705276489, Accuracy = 0.8911493420600891\n",
      "Iter #2340864:  Learning rate = 0.001955:   Batch Loss = 0.297028, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3939249515533447, Accuracy = 0.8859328627586365\n",
      "Iter #2342912:  Learning rate = 0.001955:   Batch Loss = 0.292589, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3263591229915619, Accuracy = 0.9033211469650269\n",
      "Iter #2344960:  Learning rate = 0.001955:   Batch Loss = 0.283111, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31920647621154785, Accuracy = 0.9097548127174377\n",
      "Iter #2347008:  Learning rate = 0.001955:   Batch Loss = 0.423168, Accuracy = 0.8828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3341195583343506, Accuracy = 0.8998435139656067\n",
      "Iter #2349056:  Learning rate = 0.001955:   Batch Loss = 0.287679, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36537355184555054, Accuracy = 0.8921926617622375\n",
      "Iter #2351104:  Learning rate = 0.001955:   Batch Loss = 0.326673, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3435795307159424, Accuracy = 0.8941053748130798\n",
      "Iter #2353152:  Learning rate = 0.001955:   Batch Loss = 0.300602, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3317669630050659, Accuracy = 0.9010607004165649\n",
      "Iter #2355200:  Learning rate = 0.001955:   Batch Loss = 0.301335, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3225422501564026, Accuracy = 0.9083637595176697\n",
      "Iter #2357248:  Learning rate = 0.001955:   Batch Loss = 0.260783, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3308345079421997, Accuracy = 0.9055816531181335\n",
      "Iter #2359296:  Learning rate = 0.001955:   Batch Loss = 0.270032, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32218727469444275, Accuracy = 0.9111458659172058\n",
      "Iter #2361344:  Learning rate = 0.001955:   Batch Loss = 0.295661, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35551804304122925, Accuracy = 0.8967136144638062\n",
      "Iter #2363392:  Learning rate = 0.001955:   Batch Loss = 0.208450, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32953059673309326, Accuracy = 0.9036689400672913\n",
      "Iter #2365440:  Learning rate = 0.001955:   Batch Loss = 0.473994, Accuracy = 0.8671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3244095742702484, Accuracy = 0.9097548127174377\n",
      "Iter #2367488:  Learning rate = 0.001955:   Batch Loss = 0.331641, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4675569236278534, Accuracy = 0.8650669455528259\n",
      "Iter #2369536:  Learning rate = 0.001955:   Batch Loss = 0.296440, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36703401803970337, Accuracy = 0.8866283893585205\n",
      "Iter #2371584:  Learning rate = 0.001955:   Batch Loss = 0.255088, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32729580998420715, Accuracy = 0.9022778868675232\n",
      "Iter #2373632:  Learning rate = 0.001955:   Batch Loss = 0.282928, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3430957794189453, Accuracy = 0.9003651738166809\n",
      "Iter #2375680:  Learning rate = 0.001955:   Batch Loss = 0.244702, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33286577463150024, Accuracy = 0.9015823602676392\n",
      "Iter #2377728:  Learning rate = 0.001955:   Batch Loss = 0.252927, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33963799476623535, Accuracy = 0.9001912474632263\n",
      "Iter #2379776:  Learning rate = 0.001955:   Batch Loss = 0.318298, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3444919288158417, Accuracy = 0.9021039605140686\n",
      "Iter #2381824:  Learning rate = 0.001955:   Batch Loss = 0.224969, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3305498957633972, Accuracy = 0.9052338600158691\n",
      "Iter #2383872:  Learning rate = 0.001955:   Batch Loss = 0.282518, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34577125310897827, Accuracy = 0.8986263275146484\n",
      "Iter #2385920:  Learning rate = 0.001955:   Batch Loss = 0.255666, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.323272168636322, Accuracy = 0.9134063720703125\n",
      "Iter #2387968:  Learning rate = 0.001955:   Batch Loss = 0.336560, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3591773808002472, Accuracy = 0.8967136144638062\n",
      "Iter #2390016:  Learning rate = 0.001955:   Batch Loss = 0.278747, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3301122188568115, Accuracy = 0.9094070792198181\n",
      "Iter #2392064:  Learning rate = 0.001955:   Batch Loss = 0.266378, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3338080048561096, Accuracy = 0.902451753616333\n",
      "Iter #2394112:  Learning rate = 0.001955:   Batch Loss = 0.313976, Accuracy = 0.88671875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31506213545799255, Accuracy = 0.9094070792198181\n",
      "Iter #2396160:  Learning rate = 0.001955:   Batch Loss = 0.238599, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32928332686424255, Accuracy = 0.9059293866157532\n",
      "Iter #2398208:  Learning rate = 0.001955:   Batch Loss = 0.290891, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3281015455722809, Accuracy = 0.902451753616333\n",
      "Iter #2400256:  Learning rate = 0.001877:   Batch Loss = 0.246782, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31691670417785645, Accuracy = 0.9106242656707764\n",
      "Iter #2402304:  Learning rate = 0.001877:   Batch Loss = 0.254736, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3302675485610962, Accuracy = 0.9059293866157532\n",
      "Iter #2404352:  Learning rate = 0.001877:   Batch Loss = 0.269086, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3264350891113281, Accuracy = 0.9040166735649109\n",
      "Iter #2406400:  Learning rate = 0.001877:   Batch Loss = 0.224946, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3288938105106354, Accuracy = 0.9101026058197021\n",
      "Iter #2408448:  Learning rate = 0.001877:   Batch Loss = 0.267348, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3336372971534729, Accuracy = 0.9048861265182495\n",
      "Iter #2410496:  Learning rate = 0.001877:   Batch Loss = 0.291025, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34411969780921936, Accuracy = 0.8958442211151123\n",
      "Iter #2412544:  Learning rate = 0.001877:   Batch Loss = 0.252664, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3224778175354004, Accuracy = 0.9092331528663635\n",
      "Iter #2414592:  Learning rate = 0.001877:   Batch Loss = 0.233949, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.330881804227829, Accuracy = 0.9050599932670593\n",
      "Iter #2416640:  Learning rate = 0.001877:   Batch Loss = 0.244293, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3543798625469208, Accuracy = 0.8954964280128479\n",
      "Iter #2418688:  Learning rate = 0.001877:   Batch Loss = 0.269585, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32834592461586, Accuracy = 0.9101026058197021\n",
      "Iter #2420736:  Learning rate = 0.001877:   Batch Loss = 0.276363, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3641843795776367, Accuracy = 0.8953225612640381\n",
      "Iter #2422784:  Learning rate = 0.001877:   Batch Loss = 0.242537, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37839069962501526, Accuracy = 0.8857589960098267\n",
      "Iter #2424832:  Learning rate = 0.001877:   Batch Loss = 0.260105, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3565577268600464, Accuracy = 0.8989741206169128\n",
      "Iter #2426880:  Learning rate = 0.001877:   Batch Loss = 0.275395, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3225087523460388, Accuracy = 0.9050599932670593\n",
      "Iter #2428928:  Learning rate = 0.001877:   Batch Loss = 0.322215, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3110806345939636, Accuracy = 0.9104503393173218\n",
      "Iter #2430976:  Learning rate = 0.001877:   Batch Loss = 0.239081, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3024040460586548, Accuracy = 0.9144496321678162\n",
      "Iter #2433024:  Learning rate = 0.001877:   Batch Loss = 0.309244, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.322101891040802, Accuracy = 0.9076682329177856\n",
      "Iter #2435072:  Learning rate = 0.001877:   Batch Loss = 0.239252, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3247462213039398, Accuracy = 0.9095809459686279\n",
      "Iter #2437120:  Learning rate = 0.001877:   Batch Loss = 0.302284, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31789740920066833, Accuracy = 0.9073204398155212\n",
      "Iter #2439168:  Learning rate = 0.001877:   Batch Loss = 0.222709, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3365127146244049, Accuracy = 0.9008867740631104\n",
      "Iter #2441216:  Learning rate = 0.001877:   Batch Loss = 0.244360, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.317179799079895, Accuracy = 0.910276472568512\n",
      "Iter #2443264:  Learning rate = 0.001877:   Batch Loss = 0.261656, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3202977478504181, Accuracy = 0.90801602602005\n",
      "Iter #2445312:  Learning rate = 0.001877:   Batch Loss = 0.261176, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3381323516368866, Accuracy = 0.9027994871139526\n",
      "Iter #2447360:  Learning rate = 0.001877:   Batch Loss = 0.237171, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31722915172576904, Accuracy = 0.9097548127174377\n",
      "Iter #2449408:  Learning rate = 0.001877:   Batch Loss = 0.248088, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3172075152397156, Accuracy = 0.9066249132156372\n",
      "Iter #2451456:  Learning rate = 0.001877:   Batch Loss = 0.276790, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3086135983467102, Accuracy = 0.9097548127174377\n",
      "Iter #2453504:  Learning rate = 0.001877:   Batch Loss = 0.241859, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3210953176021576, Accuracy = 0.9092331528663635\n",
      "Iter #2455552:  Learning rate = 0.001877:   Batch Loss = 0.239247, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3404429256916046, Accuracy = 0.8958442211151123\n",
      "Iter #2457600:  Learning rate = 0.001877:   Batch Loss = 0.276732, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3250408172607422, Accuracy = 0.9045383334159851\n",
      "Iter #2459648:  Learning rate = 0.001877:   Batch Loss = 0.266005, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3389962613582611, Accuracy = 0.8988001942634583\n",
      "Iter #2461696:  Learning rate = 0.001877:   Batch Loss = 0.260677, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.374172568321228, Accuracy = 0.8934098482131958\n",
      "Iter #2463744:  Learning rate = 0.001877:   Batch Loss = 0.279964, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33561161160469055, Accuracy = 0.8994957208633423\n",
      "Iter #2465792:  Learning rate = 0.001877:   Batch Loss = 0.284290, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3128708600997925, Accuracy = 0.9064510464668274\n",
      "Iter #2467840:  Learning rate = 0.001877:   Batch Loss = 0.267442, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3201374411582947, Accuracy = 0.9050599932670593\n",
      "Iter #2469888:  Learning rate = 0.001877:   Batch Loss = 0.247474, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31046509742736816, Accuracy = 0.9120153188705444\n",
      "Iter #2471936:  Learning rate = 0.001877:   Batch Loss = 0.249966, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31857043504714966, Accuracy = 0.9083637595176697\n",
      "Iter #2473984:  Learning rate = 0.001877:   Batch Loss = 0.271697, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30883917212486267, Accuracy = 0.9130585789680481\n",
      "Iter #2476032:  Learning rate = 0.001877:   Batch Loss = 0.307675, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35888153314590454, Accuracy = 0.9048861265182495\n",
      "Iter #2478080:  Learning rate = 0.001877:   Batch Loss = 0.316357, Accuracy = 0.87890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3173658549785614, Accuracy = 0.9061033129692078\n",
      "Iter #2480128:  Learning rate = 0.001877:   Batch Loss = 0.223995, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36539146304130554, Accuracy = 0.9000173807144165\n",
      "Iter #2482176:  Learning rate = 0.001877:   Batch Loss = 0.196639, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33234351873397827, Accuracy = 0.9052338600158691\n",
      "Iter #2484224:  Learning rate = 0.001877:   Batch Loss = 0.273047, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3258262574672699, Accuracy = 0.9012345671653748\n",
      "Iter #2486272:  Learning rate = 0.001877:   Batch Loss = 0.220390, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3095075190067291, Accuracy = 0.9107981324195862\n",
      "Iter #2488320:  Learning rate = 0.001877:   Batch Loss = 0.264331, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3224889039993286, Accuracy = 0.901756227016449\n",
      "Iter #2490368:  Learning rate = 0.001877:   Batch Loss = 0.250148, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3142712712287903, Accuracy = 0.9104503393173218\n",
      "Iter #2492416:  Learning rate = 0.001877:   Batch Loss = 0.268783, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3364030122756958, Accuracy = 0.9055816531181335\n",
      "Iter #2494464:  Learning rate = 0.001877:   Batch Loss = 0.293212, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30293601751327515, Accuracy = 0.9137541055679321\n",
      "Iter #2496512:  Learning rate = 0.001877:   Batch Loss = 0.340005, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3200192153453827, Accuracy = 0.9041905999183655\n",
      "Iter #2498560:  Learning rate = 0.001877:   Batch Loss = 0.230915, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3252723515033722, Accuracy = 0.9059293866157532\n",
      "Iter #2500608:  Learning rate = 0.001802:   Batch Loss = 0.237558, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3067033588886261, Accuracy = 0.9114936590194702\n",
      "Iter #2502656:  Learning rate = 0.001802:   Batch Loss = 0.244808, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3127983510494232, Accuracy = 0.9074943661689758\n",
      "Iter #2504704:  Learning rate = 0.001802:   Batch Loss = 0.287336, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3107677698135376, Accuracy = 0.9147974252700806\n",
      "Iter #2506752:  Learning rate = 0.001802:   Batch Loss = 0.244538, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30474939942359924, Accuracy = 0.9111458659172058\n",
      "Iter #2508800:  Learning rate = 0.001802:   Batch Loss = 0.253116, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31547820568084717, Accuracy = 0.9081898927688599\n",
      "Iter #2510848:  Learning rate = 0.001802:   Batch Loss = 0.230740, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31317654252052307, Accuracy = 0.9147974252700806\n",
      "Iter #2512896:  Learning rate = 0.001802:   Batch Loss = 0.218328, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3312601149082184, Accuracy = 0.9038428068161011\n",
      "Iter #2514944:  Learning rate = 0.001802:   Batch Loss = 0.253472, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3230740427970886, Accuracy = 0.905407726764679\n",
      "Iter #2516992:  Learning rate = 0.001802:   Batch Loss = 0.255573, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32622256875038147, Accuracy = 0.9073204398155212\n",
      "Iter #2519040:  Learning rate = 0.001802:   Batch Loss = 0.260063, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33187615871429443, Accuracy = 0.9033211469650269\n",
      "Iter #2521088:  Learning rate = 0.001802:   Batch Loss = 0.246390, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30727821588516235, Accuracy = 0.9147974252700806\n",
      "Iter #2523136:  Learning rate = 0.001802:   Batch Loss = 0.273135, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33248215913772583, Accuracy = 0.9094070792198181\n",
      "Iter #2525184:  Learning rate = 0.001802:   Batch Loss = 0.235833, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34881216287612915, Accuracy = 0.8970614075660706\n",
      "Iter #2527232:  Learning rate = 0.001802:   Batch Loss = 0.282662, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34051501750946045, Accuracy = 0.8996696472167969\n",
      "Iter #2529280:  Learning rate = 0.001802:   Batch Loss = 0.264108, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34436070919036865, Accuracy = 0.8993218541145325\n",
      "Iter #2531328:  Learning rate = 0.001802:   Batch Loss = 0.294474, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32157063484191895, Accuracy = 0.9062771797180176\n",
      "Iter #2533376:  Learning rate = 0.001802:   Batch Loss = 0.278144, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33297115564346313, Accuracy = 0.9026256203651428\n",
      "Iter #2535424:  Learning rate = 0.001802:   Batch Loss = 0.232327, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3039129376411438, Accuracy = 0.9104503393173218\n",
      "Iter #2537472:  Learning rate = 0.001802:   Batch Loss = 0.269762, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31044894456863403, Accuracy = 0.9099286794662476\n",
      "Iter #2539520:  Learning rate = 0.001802:   Batch Loss = 0.244620, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3215201199054718, Accuracy = 0.9101026058197021\n",
      "Iter #2541568:  Learning rate = 0.001802:   Batch Loss = 0.330734, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3314405679702759, Accuracy = 0.9097548127174377\n",
      "Iter #2543616:  Learning rate = 0.001802:   Batch Loss = 0.201469, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31366685032844543, Accuracy = 0.910276472568512\n",
      "Iter #2545664:  Learning rate = 0.001802:   Batch Loss = 0.250008, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32226303219795227, Accuracy = 0.9036689400672913\n",
      "Iter #2547712:  Learning rate = 0.001802:   Batch Loss = 0.248682, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3470101058483124, Accuracy = 0.89393150806427\n",
      "Iter #2549760:  Learning rate = 0.001802:   Batch Loss = 0.210221, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3318997025489807, Accuracy = 0.9064510464668274\n",
      "Iter #2551808:  Learning rate = 0.001802:   Batch Loss = 0.227428, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30487060546875, Accuracy = 0.910276472568512\n",
      "Iter #2553856:  Learning rate = 0.001802:   Batch Loss = 0.212759, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3140062987804413, Accuracy = 0.9135802388191223\n",
      "Iter #2555904:  Learning rate = 0.001802:   Batch Loss = 0.314924, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30878952145576477, Accuracy = 0.9088854193687439\n",
      "Iter #2557952:  Learning rate = 0.001802:   Batch Loss = 0.259356, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32998690009117126, Accuracy = 0.8994957208633423\n",
      "Iter #2560000:  Learning rate = 0.001802:   Batch Loss = 0.273327, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3199039101600647, Accuracy = 0.9127108454704285\n",
      "Iter #2562048:  Learning rate = 0.001802:   Batch Loss = 0.282265, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32688191533088684, Accuracy = 0.903147280216217\n",
      "Iter #2564096:  Learning rate = 0.001802:   Batch Loss = 0.293331, Accuracy = 0.8984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34850335121154785, Accuracy = 0.8970614075660706\n",
      "Iter #2566144:  Learning rate = 0.001802:   Batch Loss = 0.248760, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3252716064453125, Accuracy = 0.9052338600158691\n",
      "Iter #2568192:  Learning rate = 0.001802:   Batch Loss = 0.293044, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3193286657333374, Accuracy = 0.9057555198669434\n",
      "Iter #2570240:  Learning rate = 0.001802:   Batch Loss = 0.251328, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.38617825508117676, Accuracy = 0.887671709060669\n",
      "Iter #2572288:  Learning rate = 0.001802:   Batch Loss = 0.254612, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.37308502197265625, Accuracy = 0.8937575817108154\n",
      "Iter #2574336:  Learning rate = 0.001802:   Batch Loss = 0.261926, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3756752908229828, Accuracy = 0.8935837149620056\n",
      "Iter #2576384:  Learning rate = 0.001802:   Batch Loss = 0.229469, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3275917172431946, Accuracy = 0.9014084339141846\n",
      "Iter #2578432:  Learning rate = 0.001802:   Batch Loss = 0.305007, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32994526624679565, Accuracy = 0.9045383334159851\n",
      "Iter #2580480:  Learning rate = 0.001802:   Batch Loss = 0.281106, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31811419129371643, Accuracy = 0.9113197922706604\n",
      "Iter #2582528:  Learning rate = 0.001802:   Batch Loss = 0.201837, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30757591128349304, Accuracy = 0.9067988395690918\n",
      "Iter #2584576:  Learning rate = 0.001802:   Batch Loss = 0.276223, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.310973584651947, Accuracy = 0.9121891856193542\n",
      "Iter #2586624:  Learning rate = 0.001802:   Batch Loss = 0.275568, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32098808884620667, Accuracy = 0.90801602602005\n",
      "Iter #2588672:  Learning rate = 0.001802:   Batch Loss = 0.211944, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3284476101398468, Accuracy = 0.9045383334159851\n",
      "Iter #2590720:  Learning rate = 0.001802:   Batch Loss = 0.219028, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3161769211292267, Accuracy = 0.9128847122192383\n",
      "Iter #2592768:  Learning rate = 0.001802:   Batch Loss = 0.255071, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29680800437927246, Accuracy = 0.9081898927688599\n",
      "Iter #2594816:  Learning rate = 0.001802:   Batch Loss = 0.230630, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30871468782424927, Accuracy = 0.9083637595176697\n",
      "Iter #2596864:  Learning rate = 0.001802:   Batch Loss = 0.245095, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3055894374847412, Accuracy = 0.9121891856193542\n",
      "Iter #2598912:  Learning rate = 0.001802:   Batch Loss = 0.262848, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31412991881370544, Accuracy = 0.9097548127174377\n",
      "Iter #2600960:  Learning rate = 0.001730:   Batch Loss = 0.258293, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30860617756843567, Accuracy = 0.9135802388191223\n",
      "Iter #2603008:  Learning rate = 0.001730:   Batch Loss = 0.197581, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3089727759361267, Accuracy = 0.9137541055679321\n",
      "Iter #2605056:  Learning rate = 0.001730:   Batch Loss = 0.215028, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31131279468536377, Accuracy = 0.9107981324195862\n",
      "Iter #2607104:  Learning rate = 0.001730:   Batch Loss = 0.220382, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3411610722541809, Accuracy = 0.9015823602676392\n",
      "Iter #2609152:  Learning rate = 0.001730:   Batch Loss = 0.196212, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31645503640174866, Accuracy = 0.9097548127174377\n",
      "Iter #2611200:  Learning rate = 0.001730:   Batch Loss = 0.199787, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3192720413208008, Accuracy = 0.9047122001647949\n",
      "Iter #2613248:  Learning rate = 0.001730:   Batch Loss = 0.272838, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30475613474845886, Accuracy = 0.9111458659172058\n",
      "Iter #2615296:  Learning rate = 0.001730:   Batch Loss = 0.216386, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30200526118278503, Accuracy = 0.9154929518699646\n",
      "Iter #2617344:  Learning rate = 0.001730:   Batch Loss = 0.268669, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33712804317474365, Accuracy = 0.9076682329177856\n",
      "Iter #2619392:  Learning rate = 0.001730:   Batch Loss = 0.287956, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31396549940109253, Accuracy = 0.9111458659172058\n",
      "Iter #2621440:  Learning rate = 0.001730:   Batch Loss = 0.266701, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3157529830932617, Accuracy = 0.9094070792198181\n",
      "Iter #2623488:  Learning rate = 0.001730:   Batch Loss = 0.253601, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30372387170791626, Accuracy = 0.9114936590194702\n",
      "Iter #2625536:  Learning rate = 0.001730:   Batch Loss = 0.323386, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3275281488895416, Accuracy = 0.9097548127174377\n",
      "Iter #2627584:  Learning rate = 0.001730:   Batch Loss = 0.266475, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32680436968803406, Accuracy = 0.9087115526199341\n",
      "Iter #2629632:  Learning rate = 0.001730:   Batch Loss = 0.235699, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3081565201282501, Accuracy = 0.9064510464668274\n",
      "Iter #2631680:  Learning rate = 0.001730:   Batch Loss = 0.216960, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33730268478393555, Accuracy = 0.9043644666671753\n",
      "Iter #2633728:  Learning rate = 0.001730:   Batch Loss = 0.227523, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31366196274757385, Accuracy = 0.9118413925170898\n",
      "Iter #2635776:  Learning rate = 0.001730:   Batch Loss = 0.262684, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31848058104515076, Accuracy = 0.9085376262664795\n",
      "Iter #2637824:  Learning rate = 0.001730:   Batch Loss = 0.244380, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3378793001174927, Accuracy = 0.8981046676635742\n",
      "Iter #2639872:  Learning rate = 0.001730:   Batch Loss = 0.256384, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31103789806365967, Accuracy = 0.9095809459686279\n",
      "Iter #2641920:  Learning rate = 0.001730:   Batch Loss = 0.238842, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.297006219625473, Accuracy = 0.9128847122192383\n",
      "Iter #2643968:  Learning rate = 0.001730:   Batch Loss = 0.231172, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3011928200721741, Accuracy = 0.9094070792198181\n",
      "Iter #2646016:  Learning rate = 0.001730:   Batch Loss = 0.270466, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3129749298095703, Accuracy = 0.9125369787216187\n",
      "Iter #2648064:  Learning rate = 0.001730:   Batch Loss = 0.202928, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3153288662433624, Accuracy = 0.9130585789680481\n",
      "Iter #2650112:  Learning rate = 0.001730:   Batch Loss = 0.293415, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31051120162010193, Accuracy = 0.9078420996665955\n",
      "Iter #2652160:  Learning rate = 0.001730:   Batch Loss = 0.207876, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31109848618507385, Accuracy = 0.9104503393173218\n",
      "Iter #2654208:  Learning rate = 0.001730:   Batch Loss = 0.260252, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3301963210105896, Accuracy = 0.9097548127174377\n",
      "Iter #2656256:  Learning rate = 0.001730:   Batch Loss = 0.210913, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3130311667919159, Accuracy = 0.9099286794662476\n",
      "Iter #2658304:  Learning rate = 0.001730:   Batch Loss = 0.302878, Accuracy = 0.90234375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33709239959716797, Accuracy = 0.8989741206169128\n",
      "Iter #2660352:  Learning rate = 0.001730:   Batch Loss = 0.262993, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3112393319606781, Accuracy = 0.90801602602005\n",
      "Iter #2662400:  Learning rate = 0.001730:   Batch Loss = 0.291007, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33005663752555847, Accuracy = 0.9092331528663635\n",
      "Iter #2664448:  Learning rate = 0.001730:   Batch Loss = 0.315665, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33582067489624023, Accuracy = 0.9026256203651428\n",
      "Iter #2666496:  Learning rate = 0.001730:   Batch Loss = 0.276063, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30646267533302307, Accuracy = 0.9149712920188904\n",
      "Iter #2668544:  Learning rate = 0.001730:   Batch Loss = 0.176000, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3085057735443115, Accuracy = 0.9137541055679321\n",
      "Iter #2670592:  Learning rate = 0.001730:   Batch Loss = 0.269792, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3074914515018463, Accuracy = 0.9085376262664795\n",
      "Iter #2672640:  Learning rate = 0.001730:   Batch Loss = 0.252589, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31572750210762024, Accuracy = 0.903147280216217\n",
      "Iter #2674688:  Learning rate = 0.001730:   Batch Loss = 0.351848, Accuracy = 0.890625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3197416365146637, Accuracy = 0.9061033129692078\n",
      "Iter #2676736:  Learning rate = 0.001730:   Batch Loss = 0.226708, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3245599865913391, Accuracy = 0.910276472568512\n",
      "Iter #2678784:  Learning rate = 0.001730:   Batch Loss = 0.235863, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31011340022087097, Accuracy = 0.9118413925170898\n",
      "Iter #2680832:  Learning rate = 0.001730:   Batch Loss = 0.234547, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30874359607696533, Accuracy = 0.9114936590194702\n",
      "Iter #2682880:  Learning rate = 0.001730:   Batch Loss = 0.229431, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3751457929611206, Accuracy = 0.9022778868675232\n",
      "Iter #2684928:  Learning rate = 0.001730:   Batch Loss = 0.266317, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.4136996865272522, Accuracy = 0.8874978423118591\n",
      "Iter #2686976:  Learning rate = 0.001730:   Batch Loss = 0.275137, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3269178867340088, Accuracy = 0.9066249132156372\n",
      "Iter #2689024:  Learning rate = 0.001730:   Batch Loss = 0.278836, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3526466488838196, Accuracy = 0.9012345671653748\n",
      "Iter #2691072:  Learning rate = 0.001730:   Batch Loss = 0.255389, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3250761032104492, Accuracy = 0.9047122001647949\n",
      "Iter #2693120:  Learning rate = 0.001730:   Batch Loss = 0.217142, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31099846959114075, Accuracy = 0.9104503393173218\n",
      "Iter #2695168:  Learning rate = 0.001730:   Batch Loss = 0.204486, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3064132332801819, Accuracy = 0.9062771797180176\n",
      "Iter #2697216:  Learning rate = 0.001730:   Batch Loss = 0.249779, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31609344482421875, Accuracy = 0.9125369787216187\n",
      "Iter #2699264:  Learning rate = 0.001730:   Batch Loss = 0.236987, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31372153759002686, Accuracy = 0.9088854193687439\n",
      "Iter #2701312:  Learning rate = 0.001661:   Batch Loss = 0.237952, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30075693130493164, Accuracy = 0.9141018986701965\n",
      "Iter #2703360:  Learning rate = 0.001661:   Batch Loss = 0.222684, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3090898394584656, Accuracy = 0.9085376262664795\n",
      "Iter #2705408:  Learning rate = 0.001661:   Batch Loss = 0.254730, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32168006896972656, Accuracy = 0.9066249132156372\n",
      "Iter #2707456:  Learning rate = 0.001661:   Batch Loss = 0.263532, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3179924488067627, Accuracy = 0.9088854193687439\n",
      "Iter #2709504:  Learning rate = 0.001661:   Batch Loss = 0.210077, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2956538498401642, Accuracy = 0.9161884784698486\n",
      "Iter #2711552:  Learning rate = 0.001661:   Batch Loss = 0.219036, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31293511390686035, Accuracy = 0.9071465730667114\n",
      "Iter #2713600:  Learning rate = 0.001661:   Batch Loss = 0.293829, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3036581873893738, Accuracy = 0.919492244720459\n",
      "Iter #2715648:  Learning rate = 0.001661:   Batch Loss = 0.236406, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2925451397895813, Accuracy = 0.9174056649208069\n",
      "Iter #2717696:  Learning rate = 0.001661:   Batch Loss = 0.235135, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3046327829360962, Accuracy = 0.9130585789680481\n",
      "Iter #2719744:  Learning rate = 0.001661:   Batch Loss = 0.197791, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32111844420433044, Accuracy = 0.9033211469650269\n",
      "Iter #2721792:  Learning rate = 0.001661:   Batch Loss = 0.270135, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3005155622959137, Accuracy = 0.9168840050697327\n",
      "Iter #2723840:  Learning rate = 0.001661:   Batch Loss = 0.209013, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33512306213378906, Accuracy = 0.9066249132156372\n",
      "Iter #2725888:  Learning rate = 0.001661:   Batch Loss = 0.288491, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3062856197357178, Accuracy = 0.9132325053215027\n",
      "Iter #2727936:  Learning rate = 0.001661:   Batch Loss = 0.305742, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30567818880081177, Accuracy = 0.9095809459686279\n",
      "Iter #2729984:  Learning rate = 0.001661:   Batch Loss = 0.234480, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3169400095939636, Accuracy = 0.9071465730667114\n",
      "Iter #2732032:  Learning rate = 0.001661:   Batch Loss = 0.224559, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30087095499038696, Accuracy = 0.9095809459686279\n",
      "Iter #2734080:  Learning rate = 0.001661:   Batch Loss = 0.242262, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31908974051475525, Accuracy = 0.9052338600158691\n",
      "Iter #2736128:  Learning rate = 0.001661:   Batch Loss = 0.206686, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31200361251831055, Accuracy = 0.9048861265182495\n",
      "Iter #2738176:  Learning rate = 0.001661:   Batch Loss = 0.265888, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2909969091415405, Accuracy = 0.9163623452186584\n",
      "Iter #2740224:  Learning rate = 0.001661:   Batch Loss = 0.197203, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3101975917816162, Accuracy = 0.9088854193687439\n",
      "Iter #2742272:  Learning rate = 0.001661:   Batch Loss = 0.263097, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3034513592720032, Accuracy = 0.9123630523681641\n",
      "Iter #2744320:  Learning rate = 0.001661:   Batch Loss = 0.240768, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3019740581512451, Accuracy = 0.9144496321678162\n",
      "Iter #2746368:  Learning rate = 0.001661:   Batch Loss = 0.242876, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3005645275115967, Accuracy = 0.9139280319213867\n",
      "Iter #2748416:  Learning rate = 0.001661:   Batch Loss = 0.249056, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31268808245658875, Accuracy = 0.9057555198669434\n",
      "Iter #2750464:  Learning rate = 0.001661:   Batch Loss = 0.248514, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3068522810935974, Accuracy = 0.9099286794662476\n",
      "Iter #2752512:  Learning rate = 0.001661:   Batch Loss = 0.199423, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2941640615463257, Accuracy = 0.9147974252700806\n",
      "Iter #2754560:  Learning rate = 0.001661:   Batch Loss = 0.252854, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3070254325866699, Accuracy = 0.9160146117210388\n",
      "Iter #2756608:  Learning rate = 0.001661:   Batch Loss = 0.241312, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32060688734054565, Accuracy = 0.9071465730667114\n",
      "Iter #2758656:  Learning rate = 0.001661:   Batch Loss = 0.243363, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34194260835647583, Accuracy = 0.9052338600158691\n",
      "Iter #2760704:  Learning rate = 0.001661:   Batch Loss = 0.212707, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3433396816253662, Accuracy = 0.9055816531181335\n",
      "Iter #2762752:  Learning rate = 0.001661:   Batch Loss = 0.225721, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3246119022369385, Accuracy = 0.9088854193687439\n",
      "Iter #2764800:  Learning rate = 0.001661:   Batch Loss = 0.270813, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33156317472457886, Accuracy = 0.9057555198669434\n",
      "Iter #2766848:  Learning rate = 0.001661:   Batch Loss = 0.246817, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3339608907699585, Accuracy = 0.9055816531181335\n",
      "Iter #2768896:  Learning rate = 0.001661:   Batch Loss = 0.202017, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2996981739997864, Accuracy = 0.9146235585212708\n",
      "Iter #2770944:  Learning rate = 0.001661:   Batch Loss = 0.257311, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3027699887752533, Accuracy = 0.9106242656707764\n",
      "Iter #2772992:  Learning rate = 0.001661:   Batch Loss = 0.284019, Accuracy = 0.90625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31533846259117126, Accuracy = 0.9121891856193542\n",
      "Iter #2775040:  Learning rate = 0.001661:   Batch Loss = 0.228815, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3081769645214081, Accuracy = 0.910276472568512\n",
      "Iter #2777088:  Learning rate = 0.001661:   Batch Loss = 0.233828, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.320661723613739, Accuracy = 0.9078420996665955\n",
      "Iter #2779136:  Learning rate = 0.001661:   Batch Loss = 0.209601, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3251138925552368, Accuracy = 0.9055816531181335\n",
      "Iter #2781184:  Learning rate = 0.001661:   Batch Loss = 0.260685, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29875868558883667, Accuracy = 0.9184489846229553\n",
      "Iter #2783232:  Learning rate = 0.001661:   Batch Loss = 0.227309, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3121167719364166, Accuracy = 0.9043644666671753\n",
      "Iter #2785280:  Learning rate = 0.001661:   Batch Loss = 0.242591, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2997157871723175, Accuracy = 0.9153190851211548\n",
      "Iter #2787328:  Learning rate = 0.001661:   Batch Loss = 0.268744, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29905831813812256, Accuracy = 0.9193183779716492\n",
      "Iter #2789376:  Learning rate = 0.001661:   Batch Loss = 0.253589, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30164140462875366, Accuracy = 0.9149712920188904\n",
      "Iter #2791424:  Learning rate = 0.001661:   Batch Loss = 0.240306, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.298686683177948, Accuracy = 0.9144496321678162\n",
      "Iter #2793472:  Learning rate = 0.001661:   Batch Loss = 0.234811, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31204134225845337, Accuracy = 0.9107981324195862\n",
      "Iter #2795520:  Learning rate = 0.001661:   Batch Loss = 0.264172, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30076369643211365, Accuracy = 0.9161884784698486\n",
      "Iter #2797568:  Learning rate = 0.001661:   Batch Loss = 0.232844, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3101298213005066, Accuracy = 0.9137541055679321\n",
      "Iter #2799616:  Learning rate = 0.001661:   Batch Loss = 0.208744, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30085140466690063, Accuracy = 0.9120153188705444\n",
      "Iter #2801664:  Learning rate = 0.001594:   Batch Loss = 0.213081, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31044837832450867, Accuracy = 0.9107981324195862\n",
      "Iter #2803712:  Learning rate = 0.001594:   Batch Loss = 0.232717, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3050733208656311, Accuracy = 0.9168840050697327\n",
      "Iter #2805760:  Learning rate = 0.001594:   Batch Loss = 0.260907, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2986929416656494, Accuracy = 0.9134063720703125\n",
      "Iter #2807808:  Learning rate = 0.001594:   Batch Loss = 0.245654, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3049439787864685, Accuracy = 0.9114936590194702\n",
      "Iter #2809856:  Learning rate = 0.001594:   Batch Loss = 0.245443, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33120250701904297, Accuracy = 0.9029734134674072\n",
      "Iter #2811904:  Learning rate = 0.001594:   Batch Loss = 0.248135, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30153048038482666, Accuracy = 0.9134063720703125\n",
      "Iter #2813952:  Learning rate = 0.001594:   Batch Loss = 0.262692, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2951083779335022, Accuracy = 0.9181011915206909\n",
      "Iter #2816000:  Learning rate = 0.001594:   Batch Loss = 0.228510, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31446850299835205, Accuracy = 0.9123630523681641\n",
      "Iter #2818048:  Learning rate = 0.001594:   Batch Loss = 0.211635, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2876932919025421, Accuracy = 0.9167101383209229\n",
      "Iter #2820096:  Learning rate = 0.001594:   Batch Loss = 0.266918, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29583802819252014, Accuracy = 0.9111458659172058\n",
      "Iter #2822144:  Learning rate = 0.001594:   Batch Loss = 0.234774, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29927700757980347, Accuracy = 0.9144496321678162\n",
      "Iter #2824192:  Learning rate = 0.001594:   Batch Loss = 0.245109, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2925800085067749, Accuracy = 0.9160146117210388\n",
      "Iter #2826240:  Learning rate = 0.001594:   Batch Loss = 0.200525, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31774234771728516, Accuracy = 0.9092331528663635\n",
      "Iter #2828288:  Learning rate = 0.001594:   Batch Loss = 0.203962, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2975705862045288, Accuracy = 0.920187771320343\n",
      "Iter #2830336:  Learning rate = 0.001594:   Batch Loss = 0.230673, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3041669726371765, Accuracy = 0.916536271572113\n",
      "Iter #2832384:  Learning rate = 0.001594:   Batch Loss = 0.232350, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3288877308368683, Accuracy = 0.9137541055679321\n",
      "Iter #2834432:  Learning rate = 0.001594:   Batch Loss = 0.231565, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36199185252189636, Accuracy = 0.889062762260437\n",
      "Iter #2836480:  Learning rate = 0.001594:   Batch Loss = 0.236354, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31326231360435486, Accuracy = 0.910276472568512\n",
      "Iter #2838528:  Learning rate = 0.001594:   Batch Loss = 0.267526, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3020426332950592, Accuracy = 0.9111458659172058\n",
      "Iter #2840576:  Learning rate = 0.001594:   Batch Loss = 0.269750, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31086745858192444, Accuracy = 0.9071465730667114\n",
      "Iter #2842624:  Learning rate = 0.001594:   Batch Loss = 0.241669, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3336649239063263, Accuracy = 0.8972352743148804\n",
      "Iter #2844672:  Learning rate = 0.001594:   Batch Loss = 0.260789, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3090857267379761, Accuracy = 0.910971999168396\n",
      "Iter #2846720:  Learning rate = 0.001594:   Batch Loss = 0.242689, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3673432171344757, Accuracy = 0.8993218541145325\n",
      "Iter #2848768:  Learning rate = 0.001594:   Batch Loss = 0.268352, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3062317669391632, Accuracy = 0.910971999168396\n",
      "Iter #2850816:  Learning rate = 0.001594:   Batch Loss = 0.236204, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3037481904029846, Accuracy = 0.9073204398155212\n",
      "Iter #2852864:  Learning rate = 0.001594:   Batch Loss = 0.226873, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.352435827255249, Accuracy = 0.8923665285110474\n",
      "Iter #2854912:  Learning rate = 0.001594:   Batch Loss = 0.287033, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.35378819704055786, Accuracy = 0.8866283893585205\n",
      "Iter #2856960:  Learning rate = 0.001594:   Batch Loss = 0.242922, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29465779662132263, Accuracy = 0.916536271572113\n",
      "Iter #2859008:  Learning rate = 0.001594:   Batch Loss = 0.281122, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2969368100166321, Accuracy = 0.9141018986701965\n",
      "Iter #2861056:  Learning rate = 0.001594:   Batch Loss = 0.230549, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3057246506214142, Accuracy = 0.9094070792198181\n",
      "Iter #2863104:  Learning rate = 0.001594:   Batch Loss = 0.239803, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30571264028549194, Accuracy = 0.915145218372345\n",
      "Iter #2865152:  Learning rate = 0.001594:   Batch Loss = 0.283056, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29554784297943115, Accuracy = 0.9163623452186584\n",
      "Iter #2867200:  Learning rate = 0.001594:   Batch Loss = 0.290700, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3143652677536011, Accuracy = 0.9147974252700806\n",
      "Iter #2869248:  Learning rate = 0.001594:   Batch Loss = 0.283519, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3210725784301758, Accuracy = 0.9052338600158691\n",
      "Iter #2871296:  Learning rate = 0.001594:   Batch Loss = 0.226041, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3173462748527527, Accuracy = 0.9146235585212708\n",
      "Iter #2873344:  Learning rate = 0.001594:   Batch Loss = 0.196978, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3270511329174042, Accuracy = 0.9029734134674072\n",
      "Iter #2875392:  Learning rate = 0.001594:   Batch Loss = 0.231234, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2863633632659912, Accuracy = 0.9233176708221436\n",
      "Iter #2877440:  Learning rate = 0.001594:   Batch Loss = 0.214608, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2919902205467224, Accuracy = 0.9198400378227234\n",
      "Iter #2879488:  Learning rate = 0.001594:   Batch Loss = 0.260853, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2993042469024658, Accuracy = 0.9137541055679321\n",
      "Iter #2881536:  Learning rate = 0.001594:   Batch Loss = 0.217146, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30558574199676514, Accuracy = 0.9092331528663635\n",
      "Iter #2883584:  Learning rate = 0.001594:   Batch Loss = 0.270386, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2900089919567108, Accuracy = 0.9135802388191223\n",
      "Iter #2885632:  Learning rate = 0.001594:   Batch Loss = 0.192231, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29454684257507324, Accuracy = 0.9099286794662476\n",
      "Iter #2887680:  Learning rate = 0.001594:   Batch Loss = 0.292815, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31437447667121887, Accuracy = 0.9074943661689758\n",
      "Iter #2889728:  Learning rate = 0.001594:   Batch Loss = 0.246029, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2989879846572876, Accuracy = 0.9104503393173218\n",
      "Iter #2891776:  Learning rate = 0.001594:   Batch Loss = 0.220756, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28520023822784424, Accuracy = 0.915145218372345\n",
      "Iter #2893824:  Learning rate = 0.001594:   Batch Loss = 0.221709, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30602407455444336, Accuracy = 0.9170579314231873\n",
      "Iter #2895872:  Learning rate = 0.001594:   Batch Loss = 0.243713, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29020455479621887, Accuracy = 0.9111458659172058\n",
      "Iter #2897920:  Learning rate = 0.001594:   Batch Loss = 0.228364, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28397515416145325, Accuracy = 0.9207094311714172\n",
      "Iter #2899968:  Learning rate = 0.001594:   Batch Loss = 0.241133, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3226564824581146, Accuracy = 0.903147280216217\n",
      "Iter #2902016:  Learning rate = 0.001531:   Batch Loss = 0.234283, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2918202877044678, Accuracy = 0.9146235585212708\n",
      "Iter #2904064:  Learning rate = 0.001531:   Batch Loss = 0.210602, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2967512607574463, Accuracy = 0.9106242656707764\n",
      "Iter #2906112:  Learning rate = 0.001531:   Batch Loss = 0.255487, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3111776113510132, Accuracy = 0.9059293866157532\n",
      "Iter #2908160:  Learning rate = 0.001531:   Batch Loss = 0.295532, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28844183683395386, Accuracy = 0.9175795316696167\n",
      "Iter #2910208:  Learning rate = 0.001531:   Batch Loss = 0.264609, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3080989122390747, Accuracy = 0.9094070792198181\n",
      "Iter #2912256:  Learning rate = 0.001531:   Batch Loss = 0.256508, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30620911717414856, Accuracy = 0.9106242656707764\n",
      "Iter #2914304:  Learning rate = 0.001531:   Batch Loss = 0.217393, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2837144732475281, Accuracy = 0.9182750582695007\n",
      "Iter #2916352:  Learning rate = 0.001531:   Batch Loss = 0.213972, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29603493213653564, Accuracy = 0.9156668186187744\n",
      "Iter #2918400:  Learning rate = 0.001531:   Batch Loss = 0.219904, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29938650131225586, Accuracy = 0.91166752576828\n",
      "Iter #2920448:  Learning rate = 0.001531:   Batch Loss = 0.197214, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2980171740055084, Accuracy = 0.9177534580230713\n",
      "Iter #2922496:  Learning rate = 0.001531:   Batch Loss = 0.247464, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.311826229095459, Accuracy = 0.9066249132156372\n",
      "Iter #2924544:  Learning rate = 0.001531:   Batch Loss = 0.172323, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31213289499282837, Accuracy = 0.910971999168396\n",
      "Iter #2926592:  Learning rate = 0.001531:   Batch Loss = 0.216179, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3037562072277069, Accuracy = 0.9141018986701965\n",
      "Iter #2928640:  Learning rate = 0.001531:   Batch Loss = 0.236788, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2958376407623291, Accuracy = 0.9146235585212708\n",
      "Iter #2930688:  Learning rate = 0.001531:   Batch Loss = 0.226128, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2870633602142334, Accuracy = 0.9156668186187744\n",
      "Iter #2932736:  Learning rate = 0.001531:   Batch Loss = 0.200732, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2912379503250122, Accuracy = 0.9156668186187744\n",
      "Iter #2934784:  Learning rate = 0.001531:   Batch Loss = 0.236183, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3289799094200134, Accuracy = 0.9014084339141846\n",
      "Iter #2936832:  Learning rate = 0.001531:   Batch Loss = 0.193648, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28865379095077515, Accuracy = 0.9179273247718811\n",
      "Iter #2938880:  Learning rate = 0.001531:   Batch Loss = 0.226426, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2905416488647461, Accuracy = 0.9172317981719971\n",
      "Iter #2940928:  Learning rate = 0.001531:   Batch Loss = 0.230568, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28471922874450684, Accuracy = 0.9229699373245239\n",
      "Iter #2942976:  Learning rate = 0.001531:   Batch Loss = 0.238216, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2851962149143219, Accuracy = 0.9215788841247559\n",
      "Iter #2945024:  Learning rate = 0.001531:   Batch Loss = 0.211004, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.301259309053421, Accuracy = 0.915840744972229\n",
      "Iter #2947072:  Learning rate = 0.001531:   Batch Loss = 0.230122, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3286965787410736, Accuracy = 0.9090592861175537\n",
      "Iter #2949120:  Learning rate = 0.001531:   Batch Loss = 0.248084, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2870402932167053, Accuracy = 0.9193183779716492\n",
      "Iter #2951168:  Learning rate = 0.001531:   Batch Loss = 0.267914, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2946354150772095, Accuracy = 0.9175795316696167\n",
      "Iter #2953216:  Learning rate = 0.001531:   Batch Loss = 0.239885, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.299990177154541, Accuracy = 0.9088854193687439\n",
      "Iter #2955264:  Learning rate = 0.001531:   Batch Loss = 0.250616, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30047357082366943, Accuracy = 0.9186228513717651\n",
      "Iter #2957312:  Learning rate = 0.001531:   Batch Loss = 0.227542, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30571550130844116, Accuracy = 0.9092331528663635\n",
      "Iter #2959360:  Learning rate = 0.001531:   Batch Loss = 0.267779, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3434857130050659, Accuracy = 0.8961919546127319\n",
      "Iter #2961408:  Learning rate = 0.001531:   Batch Loss = 0.208640, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32218441367149353, Accuracy = 0.8977569341659546\n",
      "Iter #2963456:  Learning rate = 0.001531:   Batch Loss = 0.275731, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3063775300979614, Accuracy = 0.9161884784698486\n",
      "Iter #2965504:  Learning rate = 0.001531:   Batch Loss = 0.188556, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3049650490283966, Accuracy = 0.9114936590194702\n",
      "Iter #2967552:  Learning rate = 0.001531:   Batch Loss = 0.243969, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2981720566749573, Accuracy = 0.9114936590194702\n",
      "Iter #2969600:  Learning rate = 0.001531:   Batch Loss = 0.199196, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29793432354927063, Accuracy = 0.9146235585212708\n",
      "Iter #2971648:  Learning rate = 0.001531:   Batch Loss = 0.241029, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2937459647655487, Accuracy = 0.9147974252700806\n",
      "Iter #2973696:  Learning rate = 0.001531:   Batch Loss = 0.247114, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2968381345272064, Accuracy = 0.9111458659172058\n",
      "Iter #2975744:  Learning rate = 0.001531:   Batch Loss = 0.204545, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29388412833213806, Accuracy = 0.9175795316696167\n",
      "Iter #2977792:  Learning rate = 0.001531:   Batch Loss = 0.274533, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3102583587169647, Accuracy = 0.9120153188705444\n",
      "Iter #2979840:  Learning rate = 0.001531:   Batch Loss = 0.180940, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2801395654678345, Accuracy = 0.9189705848693848\n",
      "Iter #2981888:  Learning rate = 0.001531:   Batch Loss = 0.227887, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.351759135723114, Accuracy = 0.8941053748130798\n",
      "Iter #2983936:  Learning rate = 0.001531:   Batch Loss = 0.235994, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2997707724571228, Accuracy = 0.9123630523681641\n",
      "Iter #2985984:  Learning rate = 0.001531:   Batch Loss = 0.259574, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31370997428894043, Accuracy = 0.9135802388191223\n",
      "Iter #2988032:  Learning rate = 0.001531:   Batch Loss = 0.230260, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31715214252471924, Accuracy = 0.9099286794662476\n",
      "Iter #2990080:  Learning rate = 0.001531:   Batch Loss = 0.228733, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28647124767303467, Accuracy = 0.9175795316696167\n",
      "Iter #2992128:  Learning rate = 0.001531:   Batch Loss = 0.239358, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2904190123081207, Accuracy = 0.9149712920188904\n",
      "Iter #2994176:  Learning rate = 0.001531:   Batch Loss = 0.226238, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28744709491729736, Accuracy = 0.9184489846229553\n",
      "Iter #2996224:  Learning rate = 0.001531:   Batch Loss = 0.181605, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2785418629646301, Accuracy = 0.9175795316696167\n",
      "Iter #2998272:  Learning rate = 0.001531:   Batch Loss = 0.214902, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28540024161338806, Accuracy = 0.9163623452186584\n",
      "Iter #3000320:  Learning rate = 0.001469:   Batch Loss = 0.229804, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27424365282058716, Accuracy = 0.9210572242736816\n",
      "Iter #3002368:  Learning rate = 0.001469:   Batch Loss = 0.212030, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2859644889831543, Accuracy = 0.9168840050697327\n",
      "Iter #3004416:  Learning rate = 0.001469:   Batch Loss = 0.258720, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28634580969810486, Accuracy = 0.918796718120575\n",
      "Iter #3006464:  Learning rate = 0.001469:   Batch Loss = 0.189385, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29766690731048584, Accuracy = 0.9132325053215027\n",
      "Iter #3008512:  Learning rate = 0.001469:   Batch Loss = 0.229726, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2899109721183777, Accuracy = 0.9203616976737976\n",
      "Iter #3010560:  Learning rate = 0.001469:   Batch Loss = 0.289259, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3034995496273041, Accuracy = 0.9106242656707764\n",
      "Iter #3012608:  Learning rate = 0.001469:   Batch Loss = 0.239040, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30504268407821655, Accuracy = 0.9111458659172058\n",
      "Iter #3014656:  Learning rate = 0.001469:   Batch Loss = 0.225252, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3379853069782257, Accuracy = 0.8948009014129639\n",
      "Iter #3016704:  Learning rate = 0.001469:   Batch Loss = 0.277092, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29238826036453247, Accuracy = 0.915145218372345\n",
      "Iter #3018752:  Learning rate = 0.001469:   Batch Loss = 0.227610, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2946332097053528, Accuracy = 0.9123630523681641\n",
      "Iter #3020800:  Learning rate = 0.001469:   Batch Loss = 0.241324, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2822566032409668, Accuracy = 0.9184489846229553\n",
      "Iter #3022848:  Learning rate = 0.001469:   Batch Loss = 0.242441, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28601473569869995, Accuracy = 0.9189705848693848\n",
      "Iter #3024896:  Learning rate = 0.001469:   Batch Loss = 0.251021, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30239757895469666, Accuracy = 0.910971999168396\n",
      "Iter #3026944:  Learning rate = 0.001469:   Batch Loss = 0.232797, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2886844873428345, Accuracy = 0.9198400378227234\n",
      "Iter #3028992:  Learning rate = 0.001469:   Batch Loss = 0.143539, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3057599663734436, Accuracy = 0.9121891856193542\n",
      "Iter #3031040:  Learning rate = 0.001469:   Batch Loss = 0.291129, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2780972123146057, Accuracy = 0.918796718120575\n",
      "Iter #3033088:  Learning rate = 0.001469:   Batch Loss = 0.205183, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2731696665287018, Accuracy = 0.9264475703239441\n",
      "Iter #3035136:  Learning rate = 0.001469:   Batch Loss = 0.221919, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28265881538391113, Accuracy = 0.9181011915206909\n",
      "Iter #3037184:  Learning rate = 0.001469:   Batch Loss = 0.221287, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28110864758491516, Accuracy = 0.923665463924408\n",
      "Iter #3039232:  Learning rate = 0.001469:   Batch Loss = 0.233946, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27121439576148987, Accuracy = 0.9241871237754822\n",
      "Iter #3041280:  Learning rate = 0.001469:   Batch Loss = 0.314715, Accuracy = 0.89453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2777955234050751, Accuracy = 0.9212310910224915\n",
      "Iter #3043328:  Learning rate = 0.001469:   Batch Loss = 0.202696, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30535224080085754, Accuracy = 0.9130585789680481\n",
      "Iter #3045376:  Learning rate = 0.001469:   Batch Loss = 0.229877, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29095083475112915, Accuracy = 0.9146235585212708\n",
      "Iter #3047424:  Learning rate = 0.001469:   Batch Loss = 0.233696, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2813860774040222, Accuracy = 0.9255781769752502\n",
      "Iter #3049472:  Learning rate = 0.001469:   Batch Loss = 0.239367, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28929758071899414, Accuracy = 0.9203616976737976\n",
      "Iter #3051520:  Learning rate = 0.001469:   Batch Loss = 0.213343, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28506943583488464, Accuracy = 0.9245348572731018\n",
      "Iter #3053568:  Learning rate = 0.001469:   Batch Loss = 0.181055, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2813771963119507, Accuracy = 0.9226221442222595\n",
      "Iter #3055616:  Learning rate = 0.001469:   Batch Loss = 0.239903, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.285998672246933, Accuracy = 0.9170579314231873\n",
      "Iter #3057664:  Learning rate = 0.001469:   Batch Loss = 0.214120, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28908541798591614, Accuracy = 0.9146235585212708\n",
      "Iter #3059712:  Learning rate = 0.001469:   Batch Loss = 0.239385, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2943647503852844, Accuracy = 0.9134063720703125\n",
      "Iter #3061760:  Learning rate = 0.001469:   Batch Loss = 0.246828, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31714677810668945, Accuracy = 0.910276472568512\n",
      "Iter #3063808:  Learning rate = 0.001469:   Batch Loss = 0.212863, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28625041246414185, Accuracy = 0.9214049577713013\n",
      "Iter #3065856:  Learning rate = 0.001469:   Batch Loss = 0.221991, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30407509207725525, Accuracy = 0.9144496321678162\n",
      "Iter #3067904:  Learning rate = 0.001469:   Batch Loss = 0.238234, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30330508947372437, Accuracy = 0.9207094311714172\n",
      "Iter #3069952:  Learning rate = 0.001469:   Batch Loss = 0.224117, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28383928537368774, Accuracy = 0.9142757654190063\n",
      "Iter #3072000:  Learning rate = 0.001469:   Batch Loss = 0.199491, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29254987835884094, Accuracy = 0.9120153188705444\n",
      "Iter #3074048:  Learning rate = 0.001469:   Batch Loss = 0.180303, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2841079831123352, Accuracy = 0.9200139045715332\n",
      "Iter #3076096:  Learning rate = 0.001469:   Batch Loss = 0.200594, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28912147879600525, Accuracy = 0.9240131974220276\n",
      "Iter #3078144:  Learning rate = 0.001469:   Batch Loss = 0.249742, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2883124351501465, Accuracy = 0.9207094311714172\n",
      "Iter #3080192:  Learning rate = 0.001469:   Batch Loss = 0.298528, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30339473485946655, Accuracy = 0.9184489846229553\n",
      "Iter #3082240:  Learning rate = 0.001469:   Batch Loss = 0.210863, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28021183609962463, Accuracy = 0.925056517124176\n",
      "Iter #3084288:  Learning rate = 0.001469:   Batch Loss = 0.188561, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31981372833251953, Accuracy = 0.9191445112228394\n",
      "Iter #3086336:  Learning rate = 0.001469:   Batch Loss = 0.207157, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28171566128730774, Accuracy = 0.9215788841247559\n",
      "Iter #3088384:  Learning rate = 0.001469:   Batch Loss = 0.218112, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2748044729232788, Accuracy = 0.916536271572113\n",
      "Iter #3090432:  Learning rate = 0.001469:   Batch Loss = 0.264688, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3054181933403015, Accuracy = 0.9130585789680481\n",
      "Iter #3092480:  Learning rate = 0.001469:   Batch Loss = 0.232307, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2918027639389038, Accuracy = 0.9189705848693848\n",
      "Iter #3094528:  Learning rate = 0.001469:   Batch Loss = 0.224021, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30901652574539185, Accuracy = 0.9092331528663635\n",
      "Iter #3096576:  Learning rate = 0.001469:   Batch Loss = 0.202223, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2911452054977417, Accuracy = 0.9132325053215027\n",
      "Iter #3098624:  Learning rate = 0.001469:   Batch Loss = 0.218434, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3031380772590637, Accuracy = 0.9189705848693848\n",
      "Iter #3100672:  Learning rate = 0.001411:   Batch Loss = 0.291694, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3658544421195984, Accuracy = 0.896887481212616\n",
      "Iter #3102720:  Learning rate = 0.001411:   Batch Loss = 0.262793, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3042910695075989, Accuracy = 0.9101026058197021\n",
      "Iter #3104768:  Learning rate = 0.001411:   Batch Loss = 0.213781, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3008042573928833, Accuracy = 0.9146235585212708\n",
      "Iter #3106816:  Learning rate = 0.001411:   Batch Loss = 0.275790, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31181421875953674, Accuracy = 0.9088854193687439\n",
      "Iter #3108864:  Learning rate = 0.001411:   Batch Loss = 0.256734, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2886055111885071, Accuracy = 0.9141018986701965\n",
      "Iter #3110912:  Learning rate = 0.001411:   Batch Loss = 0.268267, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27426251769065857, Accuracy = 0.9247087240219116\n",
      "Iter #3112960:  Learning rate = 0.001411:   Batch Loss = 0.205814, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.317998468875885, Accuracy = 0.9113197922706604\n",
      "Iter #3115008:  Learning rate = 0.001411:   Batch Loss = 0.232406, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2653164267539978, Accuracy = 0.9252303838729858\n",
      "Iter #3117056:  Learning rate = 0.001411:   Batch Loss = 0.217641, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29694175720214844, Accuracy = 0.9227960109710693\n",
      "Iter #3119104:  Learning rate = 0.001411:   Batch Loss = 0.251699, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29554033279418945, Accuracy = 0.9240131974220276\n",
      "Iter #3121152:  Learning rate = 0.001411:   Batch Loss = 0.226509, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3001890778541565, Accuracy = 0.919492244720459\n",
      "Iter #3123200:  Learning rate = 0.001411:   Batch Loss = 0.230243, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31120115518569946, Accuracy = 0.9137541055679321\n",
      "Iter #3125248:  Learning rate = 0.001411:   Batch Loss = 0.224092, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2856472134590149, Accuracy = 0.9177534580230713\n",
      "Iter #3127296:  Learning rate = 0.001411:   Batch Loss = 0.195708, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2828572988510132, Accuracy = 0.919492244720459\n",
      "Iter #3129344:  Learning rate = 0.001411:   Batch Loss = 0.237821, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28303468227386475, Accuracy = 0.9214049577713013\n",
      "Iter #3131392:  Learning rate = 0.001411:   Batch Loss = 0.240590, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2769308090209961, Accuracy = 0.9234915375709534\n",
      "Iter #3133440:  Learning rate = 0.001411:   Batch Loss = 0.177643, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2743954062461853, Accuracy = 0.9260998368263245\n",
      "Iter #3135488:  Learning rate = 0.001411:   Batch Loss = 0.212083, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26902684569358826, Accuracy = 0.9292296767234802\n",
      "Iter #3137536:  Learning rate = 0.001411:   Batch Loss = 0.216401, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2834553122520447, Accuracy = 0.9257520437240601\n",
      "Iter #3139584:  Learning rate = 0.001411:   Batch Loss = 0.231927, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26620030403137207, Accuracy = 0.9274908900260925\n",
      "Iter #3141632:  Learning rate = 0.001411:   Batch Loss = 0.244839, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28931257128715515, Accuracy = 0.9172317981719971\n",
      "Iter #3143680:  Learning rate = 0.001411:   Batch Loss = 0.205957, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31013578176498413, Accuracy = 0.9088854193687439\n",
      "Iter #3145728:  Learning rate = 0.001411:   Batch Loss = 0.221302, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28116104006767273, Accuracy = 0.9184489846229553\n",
      "Iter #3147776:  Learning rate = 0.001411:   Batch Loss = 0.286778, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2773398756980896, Accuracy = 0.9259259104728699\n",
      "Iter #3149824:  Learning rate = 0.001411:   Batch Loss = 0.266981, Accuracy = 0.91015625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3426370322704315, Accuracy = 0.9074943661689758\n",
      "Iter #3151872:  Learning rate = 0.001411:   Batch Loss = 0.278706, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3398958444595337, Accuracy = 0.9127108454704285\n",
      "Iter #3153920:  Learning rate = 0.001411:   Batch Loss = 0.216888, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.370571494102478, Accuracy = 0.9007129073143005\n",
      "Iter #3155968:  Learning rate = 0.001411:   Batch Loss = 0.239966, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3350031077861786, Accuracy = 0.9055816531181335\n",
      "Iter #3158016:  Learning rate = 0.001411:   Batch Loss = 0.240753, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2700544595718384, Accuracy = 0.9290558099746704\n",
      "Iter #3160064:  Learning rate = 0.001411:   Batch Loss = 0.232749, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.281705766916275, Accuracy = 0.9210572242736816\n",
      "Iter #3162112:  Learning rate = 0.001411:   Batch Loss = 0.229665, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2687813639640808, Accuracy = 0.9260998368263245\n",
      "Iter #3164160:  Learning rate = 0.001411:   Batch Loss = 0.257751, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2924001216888428, Accuracy = 0.920187771320343\n",
      "Iter #3166208:  Learning rate = 0.001411:   Batch Loss = 0.202346, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29586243629455566, Accuracy = 0.9181011915206909\n",
      "Iter #3168256:  Learning rate = 0.001411:   Batch Loss = 0.201244, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27197280526161194, Accuracy = 0.9320118427276611\n",
      "Iter #3170304:  Learning rate = 0.001411:   Batch Loss = 0.181219, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28202661871910095, Accuracy = 0.9278386235237122\n",
      "Iter #3172352:  Learning rate = 0.001411:   Batch Loss = 0.190779, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2676563262939453, Accuracy = 0.9316640496253967\n",
      "Iter #3174400:  Learning rate = 0.001411:   Batch Loss = 0.165085, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2799219787120819, Accuracy = 0.9172317981719971\n",
      "Iter #3176448:  Learning rate = 0.001411:   Batch Loss = 0.185349, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28107911348342896, Accuracy = 0.9203616976737976\n",
      "Iter #3178496:  Learning rate = 0.001411:   Batch Loss = 0.198176, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2768692970275879, Accuracy = 0.9227960109710693\n",
      "Iter #3180544:  Learning rate = 0.001411:   Batch Loss = 0.226731, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2879945635795593, Accuracy = 0.9221004843711853\n",
      "Iter #3182592:  Learning rate = 0.001411:   Batch Loss = 0.229825, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30754488706588745, Accuracy = 0.9147974252700806\n",
      "Iter #3184640:  Learning rate = 0.001411:   Batch Loss = 0.198310, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2725816071033478, Accuracy = 0.9222744107246399\n",
      "Iter #3186688:  Learning rate = 0.001411:   Batch Loss = 0.239300, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26961442828178406, Accuracy = 0.9240131974220276\n",
      "Iter #3188736:  Learning rate = 0.001411:   Batch Loss = 0.222236, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29570305347442627, Accuracy = 0.9248826503753662\n",
      "Iter #3190784:  Learning rate = 0.001411:   Batch Loss = 0.264711, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2901761531829834, Accuracy = 0.9181011915206909\n",
      "Iter #3192832:  Learning rate = 0.001411:   Batch Loss = 0.251173, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27002763748168945, Accuracy = 0.9248826503753662\n",
      "Iter #3194880:  Learning rate = 0.001411:   Batch Loss = 0.197346, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2811641991138458, Accuracy = 0.9240131974220276\n",
      "Iter #3196928:  Learning rate = 0.001411:   Batch Loss = 0.221367, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2868591547012329, Accuracy = 0.9219266176223755\n",
      "Iter #3198976:  Learning rate = 0.001411:   Batch Loss = 0.242382, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28035083413124084, Accuracy = 0.9233176708221436\n",
      "Iter #3201024:  Learning rate = 0.001354:   Batch Loss = 0.177766, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2791665196418762, Accuracy = 0.9273169636726379\n",
      "Iter #3203072:  Learning rate = 0.001354:   Batch Loss = 0.190787, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3046666085720062, Accuracy = 0.9175795316696167\n",
      "Iter #3205120:  Learning rate = 0.001354:   Batch Loss = 0.182161, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28040289878845215, Accuracy = 0.9252303838729858\n",
      "Iter #3207168:  Learning rate = 0.001354:   Batch Loss = 0.207031, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27582985162734985, Accuracy = 0.9254042506217957\n",
      "Iter #3209216:  Learning rate = 0.001354:   Batch Loss = 0.221112, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28787070512771606, Accuracy = 0.9179273247718811\n",
      "Iter #3211264:  Learning rate = 0.001354:   Batch Loss = 0.182903, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28222736716270447, Accuracy = 0.9234915375709534\n",
      "Iter #3213312:  Learning rate = 0.001354:   Batch Loss = 0.184349, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27911505103111267, Accuracy = 0.9184489846229553\n",
      "Iter #3215360:  Learning rate = 0.001354:   Batch Loss = 0.227770, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2857615649700165, Accuracy = 0.9217527508735657\n",
      "Iter #3217408:  Learning rate = 0.001354:   Batch Loss = 0.213999, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2644016146659851, Accuracy = 0.9313163161277771\n",
      "Iter #3219456:  Learning rate = 0.001354:   Batch Loss = 0.220854, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2784491777420044, Accuracy = 0.9266214370727539\n",
      "Iter #3221504:  Learning rate = 0.001354:   Batch Loss = 0.208889, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2928810715675354, Accuracy = 0.9229699373245239\n",
      "Iter #3223552:  Learning rate = 0.001354:   Batch Loss = 0.215514, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2672387957572937, Accuracy = 0.9285341501235962\n",
      "Iter #3225600:  Learning rate = 0.001354:   Batch Loss = 0.212127, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.276576966047287, Accuracy = 0.9300991296768188\n",
      "Iter #3227648:  Learning rate = 0.001354:   Batch Loss = 0.191836, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28940549492836, Accuracy = 0.9217527508735657\n",
      "Iter #3229696:  Learning rate = 0.001354:   Batch Loss = 0.189564, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2833442687988281, Accuracy = 0.9212310910224915\n",
      "Iter #3231744:  Learning rate = 0.001354:   Batch Loss = 0.226833, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27842387557029724, Accuracy = 0.9212310910224915\n",
      "Iter #3233792:  Learning rate = 0.001354:   Batch Loss = 0.232177, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2721360921859741, Accuracy = 0.9267953634262085\n",
      "Iter #3235840:  Learning rate = 0.001354:   Batch Loss = 0.190513, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25722306966781616, Accuracy = 0.9281864166259766\n",
      "Iter #3237888:  Learning rate = 0.001354:   Batch Loss = 0.216211, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27846020460128784, Accuracy = 0.9227960109710693\n",
      "Iter #3239936:  Learning rate = 0.001354:   Batch Loss = 0.203975, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27074137330055237, Accuracy = 0.9260998368263245\n",
      "Iter #3241984:  Learning rate = 0.001354:   Batch Loss = 0.254988, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3183733820915222, Accuracy = 0.9041905999183655\n",
      "Iter #3244032:  Learning rate = 0.001354:   Batch Loss = 0.181625, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3025722801685333, Accuracy = 0.90801602602005\n",
      "Iter #3246080:  Learning rate = 0.001354:   Batch Loss = 0.235329, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28789347410202026, Accuracy = 0.9153190851211548\n",
      "Iter #3248128:  Learning rate = 0.001354:   Batch Loss = 0.159211, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28534209728240967, Accuracy = 0.9168840050697327\n",
      "Iter #3250176:  Learning rate = 0.001354:   Batch Loss = 0.187996, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27048200368881226, Accuracy = 0.9221004843711853\n",
      "Iter #3252224:  Learning rate = 0.001354:   Batch Loss = 0.220723, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2757069170475006, Accuracy = 0.920883297920227\n",
      "Iter #3254272:  Learning rate = 0.001354:   Batch Loss = 0.242375, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27769508957862854, Accuracy = 0.9231438040733337\n",
      "Iter #3256320:  Learning rate = 0.001354:   Batch Loss = 0.205580, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28312140703201294, Accuracy = 0.920883297920227\n",
      "Iter #3258368:  Learning rate = 0.001354:   Batch Loss = 0.229155, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26907700300216675, Accuracy = 0.9255781769752502\n",
      "Iter #3260416:  Learning rate = 0.001354:   Batch Loss = 0.235752, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27912843227386475, Accuracy = 0.920883297920227\n",
      "Iter #3262464:  Learning rate = 0.001354:   Batch Loss = 0.212747, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2785724103450775, Accuracy = 0.923665463924408\n",
      "Iter #3264512:  Learning rate = 0.001354:   Batch Loss = 0.208281, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27789759635925293, Accuracy = 0.919492244720459\n",
      "Iter #3266560:  Learning rate = 0.001354:   Batch Loss = 0.185255, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29409143328666687, Accuracy = 0.9163623452186584\n",
      "Iter #3268608:  Learning rate = 0.001354:   Batch Loss = 0.223532, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3042899966239929, Accuracy = 0.9147974252700806\n",
      "Iter #3270656:  Learning rate = 0.001354:   Batch Loss = 0.203670, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2718776762485504, Accuracy = 0.9276647567749023\n",
      "Iter #3272704:  Learning rate = 0.001354:   Batch Loss = 0.178162, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2991713285446167, Accuracy = 0.9147974252700806\n",
      "Iter #3274752:  Learning rate = 0.001354:   Batch Loss = 0.276284, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28583866357803345, Accuracy = 0.9182750582695007\n",
      "Iter #3276800:  Learning rate = 0.001354:   Batch Loss = 0.178220, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26646479964256287, Accuracy = 0.9281864166259766\n",
      "Iter #3278848:  Learning rate = 0.001354:   Batch Loss = 0.200937, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30254578590393066, Accuracy = 0.920187771320343\n",
      "Iter #3280896:  Learning rate = 0.001354:   Batch Loss = 0.245075, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29074057936668396, Accuracy = 0.9205355644226074\n",
      "Iter #3282944:  Learning rate = 0.001354:   Batch Loss = 0.217967, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.34319546818733215, Accuracy = 0.9067988395690918\n",
      "Iter #3284992:  Learning rate = 0.001354:   Batch Loss = 0.190001, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2922537624835968, Accuracy = 0.9221004843711853\n",
      "Iter #3287040:  Learning rate = 0.001354:   Batch Loss = 0.195758, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28862595558166504, Accuracy = 0.9229699373245239\n",
      "Iter #3289088:  Learning rate = 0.001354:   Batch Loss = 0.175548, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26334261894226074, Accuracy = 0.9311423897743225\n",
      "Iter #3291136:  Learning rate = 0.001354:   Batch Loss = 0.173952, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2786344289779663, Accuracy = 0.9274908900260925\n",
      "Iter #3293184:  Learning rate = 0.001354:   Batch Loss = 0.181199, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29360511898994446, Accuracy = 0.915145218372345\n",
      "Iter #3295232:  Learning rate = 0.001354:   Batch Loss = 0.230691, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2681416869163513, Accuracy = 0.9273169636726379\n",
      "Iter #3297280:  Learning rate = 0.001354:   Batch Loss = 0.254871, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2616844177246094, Accuracy = 0.9264475703239441\n",
      "Iter #3299328:  Learning rate = 0.001354:   Batch Loss = 0.161095, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26960304379463196, Accuracy = 0.9292296767234802\n",
      "Iter #3301376:  Learning rate = 0.001300:   Batch Loss = 0.228965, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27777746319770813, Accuracy = 0.928012490272522\n",
      "Iter #3303424:  Learning rate = 0.001300:   Batch Loss = 0.163105, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2823196053504944, Accuracy = 0.9175795316696167\n",
      "Iter #3305472:  Learning rate = 0.001300:   Batch Loss = 0.225893, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2704688608646393, Accuracy = 0.9238393306732178\n",
      "Iter #3307520:  Learning rate = 0.001300:   Batch Loss = 0.178447, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2756635844707489, Accuracy = 0.916536271572113\n",
      "Iter #3309568:  Learning rate = 0.001300:   Batch Loss = 0.187539, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31065958738327026, Accuracy = 0.9184489846229553\n",
      "Iter #3311616:  Learning rate = 0.001300:   Batch Loss = 0.214787, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33901408314704895, Accuracy = 0.9071465730667114\n",
      "Iter #3313664:  Learning rate = 0.001300:   Batch Loss = 0.212830, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.308460533618927, Accuracy = 0.9147974252700806\n",
      "Iter #3315712:  Learning rate = 0.001300:   Batch Loss = 0.258257, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27903252840042114, Accuracy = 0.9285341501235962\n",
      "Iter #3317760:  Learning rate = 0.001300:   Batch Loss = 0.243034, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29198044538497925, Accuracy = 0.9203616976737976\n",
      "Iter #3319808:  Learning rate = 0.001300:   Batch Loss = 0.182559, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27079474925994873, Accuracy = 0.928012490272522\n",
      "Iter #3321856:  Learning rate = 0.001300:   Batch Loss = 0.193987, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26894116401672363, Accuracy = 0.9271430969238281\n",
      "Iter #3323904:  Learning rate = 0.001300:   Batch Loss = 0.208704, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2614074945449829, Accuracy = 0.9285341501235962\n",
      "Iter #3325952:  Learning rate = 0.001300:   Batch Loss = 0.179202, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25911611318588257, Accuracy = 0.9292296767234802\n",
      "Iter #3328000:  Learning rate = 0.001300:   Batch Loss = 0.192977, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2558419108390808, Accuracy = 0.9339245557785034\n",
      "Iter #3330048:  Learning rate = 0.001300:   Batch Loss = 0.242585, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27207040786743164, Accuracy = 0.9252303838729858\n",
      "Iter #3332096:  Learning rate = 0.001300:   Batch Loss = 0.214013, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27782124280929565, Accuracy = 0.9267953634262085\n",
      "Iter #3334144:  Learning rate = 0.001300:   Batch Loss = 0.207424, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31326764822006226, Accuracy = 0.9156668186187744\n",
      "Iter #3336192:  Learning rate = 0.001300:   Batch Loss = 0.210475, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3017111122608185, Accuracy = 0.9161884784698486\n",
      "Iter #3338240:  Learning rate = 0.001300:   Batch Loss = 0.195722, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2627926170825958, Accuracy = 0.9285341501235962\n",
      "Iter #3340288:  Learning rate = 0.001300:   Batch Loss = 0.196041, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26349905133247375, Accuracy = 0.9269692301750183\n",
      "Iter #3342336:  Learning rate = 0.001300:   Batch Loss = 0.209961, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2590830326080322, Accuracy = 0.9323595762252808\n",
      "Iter #3344384:  Learning rate = 0.001300:   Batch Loss = 0.209796, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26843997836112976, Accuracy = 0.9290558099746704\n",
      "Iter #3346432:  Learning rate = 0.001300:   Batch Loss = 0.200535, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27138665318489075, Accuracy = 0.9262737035751343\n",
      "Iter #3348480:  Learning rate = 0.001300:   Batch Loss = 0.189630, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26892170310020447, Accuracy = 0.9292296767234802\n",
      "Iter #3350528:  Learning rate = 0.001300:   Batch Loss = 0.235485, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2691939175128937, Accuracy = 0.9264475703239441\n",
      "Iter #3352576:  Learning rate = 0.001300:   Batch Loss = 0.195382, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.275402307510376, Accuracy = 0.9294036030769348\n",
      "Iter #3354624:  Learning rate = 0.001300:   Batch Loss = 0.174805, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2830011546611786, Accuracy = 0.924360990524292\n",
      "Iter #3356672:  Learning rate = 0.001300:   Batch Loss = 0.211446, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26417219638824463, Accuracy = 0.9325334429740906\n",
      "Iter #3358720:  Learning rate = 0.001300:   Batch Loss = 0.208891, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.31007125973701477, Accuracy = 0.9182750582695007\n",
      "Iter #3360768:  Learning rate = 0.001300:   Batch Loss = 0.210909, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30141690373420715, Accuracy = 0.9200139045715332\n",
      "Iter #3362816:  Learning rate = 0.001300:   Batch Loss = 0.192911, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2825983166694641, Accuracy = 0.9212310910224915\n",
      "Iter #3364864:  Learning rate = 0.001300:   Batch Loss = 0.215779, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2719690799713135, Accuracy = 0.9238393306732178\n",
      "Iter #3366912:  Learning rate = 0.001300:   Batch Loss = 0.210184, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3139856457710266, Accuracy = 0.9078420996665955\n",
      "Iter #3368960:  Learning rate = 0.001300:   Batch Loss = 0.204530, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2821591794490814, Accuracy = 0.9168840050697327\n",
      "Iter #3371008:  Learning rate = 0.001300:   Batch Loss = 0.169590, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28155088424682617, Accuracy = 0.9203616976737976\n",
      "Iter #3373056:  Learning rate = 0.001300:   Batch Loss = 0.197459, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2617388069629669, Accuracy = 0.9255781769752502\n",
      "Iter #3375104:  Learning rate = 0.001300:   Batch Loss = 0.174906, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2687543034553528, Accuracy = 0.9288819432258606\n",
      "Iter #3377152:  Learning rate = 0.001300:   Batch Loss = 0.238469, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26978686451911926, Accuracy = 0.9297513365745544\n",
      "Iter #3379200:  Learning rate = 0.001300:   Batch Loss = 0.194169, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25744369626045227, Accuracy = 0.9269692301750183\n",
      "Iter #3381248:  Learning rate = 0.001300:   Batch Loss = 0.210154, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3048423230648041, Accuracy = 0.9088854193687439\n",
      "Iter #3383296:  Learning rate = 0.001300:   Batch Loss = 0.225112, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26983770728111267, Accuracy = 0.9226221442222595\n",
      "Iter #3385344:  Learning rate = 0.001300:   Batch Loss = 0.217240, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2715437710285187, Accuracy = 0.9274908900260925\n",
      "Iter #3387392:  Learning rate = 0.001300:   Batch Loss = 0.170054, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27839261293411255, Accuracy = 0.9278386235237122\n",
      "Iter #3389440:  Learning rate = 0.001300:   Batch Loss = 0.187141, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2770858407020569, Accuracy = 0.9257520437240601\n",
      "Iter #3391488:  Learning rate = 0.001300:   Batch Loss = 0.200090, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26038917899131775, Accuracy = 0.9351417422294617\n",
      "Iter #3393536:  Learning rate = 0.001300:   Batch Loss = 0.265391, Accuracy = 0.9296875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2713305354118347, Accuracy = 0.9255781769752502\n",
      "Iter #3395584:  Learning rate = 0.001300:   Batch Loss = 0.215197, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2670242488384247, Accuracy = 0.9295774698257446\n",
      "Iter #3397632:  Learning rate = 0.001300:   Batch Loss = 0.191778, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2702012360095978, Accuracy = 0.9307946562767029\n",
      "Iter #3399680:  Learning rate = 0.001300:   Batch Loss = 0.191228, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2772957682609558, Accuracy = 0.9311423897743225\n",
      "Iter #3401728:  Learning rate = 0.001248:   Batch Loss = 0.155297, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2930159270763397, Accuracy = 0.9262737035751343\n",
      "Iter #3403776:  Learning rate = 0.001248:   Batch Loss = 0.189460, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3059754967689514, Accuracy = 0.9172317981719971\n",
      "Iter #3405824:  Learning rate = 0.001248:   Batch Loss = 0.180171, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2788185477256775, Accuracy = 0.9193183779716492\n",
      "Iter #3407872:  Learning rate = 0.001248:   Batch Loss = 0.153389, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2704918384552002, Accuracy = 0.9264475703239441\n",
      "Iter #3409920:  Learning rate = 0.001248:   Batch Loss = 0.236836, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27906233072280884, Accuracy = 0.9273169636726379\n",
      "Iter #3411968:  Learning rate = 0.001248:   Batch Loss = 0.181340, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26867103576660156, Accuracy = 0.9294036030769348\n",
      "Iter #3414016:  Learning rate = 0.001248:   Batch Loss = 0.198640, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2680091857910156, Accuracy = 0.934272289276123\n",
      "Iter #3416064:  Learning rate = 0.001248:   Batch Loss = 0.149562, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3453960120677948, Accuracy = 0.9090592861175537\n",
      "Iter #3418112:  Learning rate = 0.001248:   Batch Loss = 0.190707, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3197307586669922, Accuracy = 0.9139280319213867\n",
      "Iter #3420160:  Learning rate = 0.001248:   Batch Loss = 0.274541, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.43503135442733765, Accuracy = 0.8848896026611328\n",
      "Iter #3422208:  Learning rate = 0.001248:   Batch Loss = 0.225434, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.3553594946861267, Accuracy = 0.8975830078125\n",
      "Iter #3424256:  Learning rate = 0.001248:   Batch Loss = 0.235472, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29716604948043823, Accuracy = 0.9215788841247559\n",
      "Iter #3426304:  Learning rate = 0.001248:   Batch Loss = 0.194515, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27001941204071045, Accuracy = 0.9222744107246399\n",
      "Iter #3428352:  Learning rate = 0.001248:   Batch Loss = 0.252667, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28998100757598877, Accuracy = 0.9191445112228394\n",
      "Iter #3430400:  Learning rate = 0.001248:   Batch Loss = 0.181845, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2644021809101105, Accuracy = 0.9254042506217957\n",
      "Iter #3432448:  Learning rate = 0.001248:   Batch Loss = 0.222780, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25711116194725037, Accuracy = 0.9245348572731018\n",
      "Iter #3434496:  Learning rate = 0.001248:   Batch Loss = 0.206375, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2627992331981659, Accuracy = 0.932185709476471\n",
      "Iter #3436544:  Learning rate = 0.001248:   Batch Loss = 0.210436, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25842803716659546, Accuracy = 0.933576762676239\n",
      "Iter #3438592:  Learning rate = 0.001248:   Batch Loss = 0.160930, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2660631537437439, Accuracy = 0.9273169636726379\n",
      "Iter #3440640:  Learning rate = 0.001248:   Batch Loss = 0.174410, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26669907569885254, Accuracy = 0.9255781769752502\n",
      "Iter #3442688:  Learning rate = 0.001248:   Batch Loss = 0.220046, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25628727674484253, Accuracy = 0.9327073693275452\n",
      "Iter #3444736:  Learning rate = 0.001248:   Batch Loss = 0.184597, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26711001992225647, Accuracy = 0.9278386235237122\n",
      "Iter #3446784:  Learning rate = 0.001248:   Batch Loss = 0.208843, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2662668824195862, Accuracy = 0.9247087240219116\n",
      "Iter #3448832:  Learning rate = 0.001248:   Batch Loss = 0.207739, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25388050079345703, Accuracy = 0.9332290291786194\n",
      "Iter #3450880:  Learning rate = 0.001248:   Batch Loss = 0.224938, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26066142320632935, Accuracy = 0.9313163161277771\n",
      "Iter #3452928:  Learning rate = 0.001248:   Batch Loss = 0.207835, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27538567781448364, Accuracy = 0.9274908900260925\n",
      "Iter #3454976:  Learning rate = 0.001248:   Batch Loss = 0.197154, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2562564015388489, Accuracy = 0.934272289276123\n",
      "Iter #3457024:  Learning rate = 0.001248:   Batch Loss = 0.188434, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26847803592681885, Accuracy = 0.9294036030769348\n",
      "Iter #3459072:  Learning rate = 0.001248:   Batch Loss = 0.204390, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26639607548713684, Accuracy = 0.9260998368263245\n",
      "Iter #3461120:  Learning rate = 0.001248:   Batch Loss = 0.184822, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26290756464004517, Accuracy = 0.9339245557785034\n",
      "Iter #3463168:  Learning rate = 0.001248:   Batch Loss = 0.204370, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2613230049610138, Accuracy = 0.9370543956756592\n",
      "Iter #3465216:  Learning rate = 0.001248:   Batch Loss = 0.168463, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2877858877182007, Accuracy = 0.9292296767234802\n",
      "Iter #3467264:  Learning rate = 0.001248:   Batch Loss = 0.206795, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25840407609939575, Accuracy = 0.9297513365745544\n",
      "Iter #3469312:  Learning rate = 0.001248:   Batch Loss = 0.210281, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25667184591293335, Accuracy = 0.9290558099746704\n",
      "Iter #3471360:  Learning rate = 0.001248:   Batch Loss = 0.240509, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2629814147949219, Accuracy = 0.9260998368263245\n",
      "Iter #3473408:  Learning rate = 0.001248:   Batch Loss = 0.207050, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2677464783191681, Accuracy = 0.9295774698257446\n",
      "Iter #3475456:  Learning rate = 0.001248:   Batch Loss = 0.217652, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2508007287979126, Accuracy = 0.9347939491271973\n",
      "Iter #3477504:  Learning rate = 0.001248:   Batch Loss = 0.214440, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2629624903202057, Accuracy = 0.9288819432258606\n",
      "Iter #3479552:  Learning rate = 0.001248:   Batch Loss = 0.235287, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25596004724502563, Accuracy = 0.9318379163742065\n",
      "Iter #3481600:  Learning rate = 0.001248:   Batch Loss = 0.211954, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26882991194725037, Accuracy = 0.9299252033233643\n",
      "Iter #3483648:  Learning rate = 0.001248:   Batch Loss = 0.168720, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27413755655288696, Accuracy = 0.9248826503753662\n",
      "Iter #3485696:  Learning rate = 0.001248:   Batch Loss = 0.296661, Accuracy = 0.9140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30460670590400696, Accuracy = 0.9146235585212708\n",
      "Iter #3487744:  Learning rate = 0.001248:   Batch Loss = 0.233989, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2760976552963257, Accuracy = 0.9290558099746704\n",
      "Iter #3489792:  Learning rate = 0.001248:   Batch Loss = 0.239735, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26573482155799866, Accuracy = 0.9226221442222595\n",
      "Iter #3491840:  Learning rate = 0.001248:   Batch Loss = 0.239664, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28269726037979126, Accuracy = 0.9177534580230713\n",
      "Iter #3493888:  Learning rate = 0.001248:   Batch Loss = 0.223995, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26620981097221375, Accuracy = 0.9278386235237122\n",
      "Iter #3495936:  Learning rate = 0.001248:   Batch Loss = 0.176964, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26222893595695496, Accuracy = 0.9300991296768188\n",
      "Iter #3497984:  Learning rate = 0.001248:   Batch Loss = 0.171952, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.270114004611969, Accuracy = 0.9247087240219116\n",
      "Iter #3500032:  Learning rate = 0.001198:   Batch Loss = 0.196002, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2585733234882355, Accuracy = 0.9283602833747864\n",
      "Iter #3502080:  Learning rate = 0.001198:   Batch Loss = 0.192768, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2590404450893402, Accuracy = 0.9323595762252808\n",
      "Iter #3504128:  Learning rate = 0.001198:   Batch Loss = 0.199585, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2617483139038086, Accuracy = 0.9346200823783875\n",
      "Iter #3506176:  Learning rate = 0.001198:   Batch Loss = 0.191622, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2587149143218994, Accuracy = 0.9325334429740906\n",
      "Iter #3508224:  Learning rate = 0.001198:   Batch Loss = 0.169610, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26455822587013245, Accuracy = 0.9274908900260925\n",
      "Iter #3510272:  Learning rate = 0.001198:   Batch Loss = 0.224585, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2674834132194519, Accuracy = 0.9285341501235962\n",
      "Iter #3512320:  Learning rate = 0.001198:   Batch Loss = 0.156458, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26739048957824707, Accuracy = 0.9323595762252808\n",
      "Iter #3514368:  Learning rate = 0.001198:   Batch Loss = 0.157626, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2580570578575134, Accuracy = 0.9344461560249329\n",
      "Iter #3516416:  Learning rate = 0.001198:   Batch Loss = 0.138952, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24984578788280487, Accuracy = 0.9393149018287659\n",
      "Iter #3518464:  Learning rate = 0.001198:   Batch Loss = 0.237380, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26683202385902405, Accuracy = 0.932185709476471\n",
      "Iter #3520512:  Learning rate = 0.001198:   Batch Loss = 0.145430, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25714340806007385, Accuracy = 0.9295774698257446\n",
      "Iter #3522560:  Learning rate = 0.001198:   Batch Loss = 0.191336, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25336727499961853, Accuracy = 0.9302729964256287\n",
      "Iter #3524608:  Learning rate = 0.001198:   Batch Loss = 0.160540, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25417816638946533, Accuracy = 0.9323595762252808\n",
      "Iter #3526656:  Learning rate = 0.001198:   Batch Loss = 0.188501, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2579449415206909, Accuracy = 0.9290558099746704\n",
      "Iter #3528704:  Learning rate = 0.001198:   Batch Loss = 0.173255, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2518327832221985, Accuracy = 0.9356633424758911\n",
      "Iter #3530752:  Learning rate = 0.001198:   Batch Loss = 0.195877, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2465222179889679, Accuracy = 0.938445508480072\n",
      "Iter #3532800:  Learning rate = 0.001198:   Batch Loss = 0.213660, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2507554888725281, Accuracy = 0.9408798217773438\n",
      "Iter #3534848:  Learning rate = 0.001198:   Batch Loss = 0.235881, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28812530636787415, Accuracy = 0.9255781769752502\n",
      "Iter #3536896:  Learning rate = 0.001198:   Batch Loss = 0.166197, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2776696979999542, Accuracy = 0.9294036030769348\n",
      "Iter #3538944:  Learning rate = 0.001198:   Batch Loss = 0.160043, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2834777534008026, Accuracy = 0.9276647567749023\n",
      "Iter #3540992:  Learning rate = 0.001198:   Batch Loss = 0.208267, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26070305705070496, Accuracy = 0.9318379163742065\n",
      "Iter #3543040:  Learning rate = 0.001198:   Batch Loss = 0.188236, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27383890748023987, Accuracy = 0.9221004843711853\n",
      "Iter #3545088:  Learning rate = 0.001198:   Batch Loss = 0.170111, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25827494263648987, Accuracy = 0.9358372688293457\n",
      "Iter #3547136:  Learning rate = 0.001198:   Batch Loss = 0.153353, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.251811146736145, Accuracy = 0.9370543956756592\n",
      "Iter #3549184:  Learning rate = 0.001198:   Batch Loss = 0.186177, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2518784999847412, Accuracy = 0.9307946562767029\n",
      "Iter #3551232:  Learning rate = 0.001198:   Batch Loss = 0.157118, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2472335696220398, Accuracy = 0.9403582215309143\n",
      "Iter #3553280:  Learning rate = 0.001198:   Batch Loss = 0.168141, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25374290347099304, Accuracy = 0.9339245557785034\n",
      "Iter #3555328:  Learning rate = 0.001198:   Batch Loss = 0.179828, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26590678095817566, Accuracy = 0.9304468631744385\n",
      "Iter #3557376:  Learning rate = 0.001198:   Batch Loss = 0.162338, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2666378319263458, Accuracy = 0.9311423897743225\n",
      "Iter #3559424:  Learning rate = 0.001198:   Batch Loss = 0.153719, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25449123978614807, Accuracy = 0.9300991296768188\n",
      "Iter #3561472:  Learning rate = 0.001198:   Batch Loss = 0.152905, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24656789004802704, Accuracy = 0.9380977153778076\n",
      "Iter #3563520:  Learning rate = 0.001198:   Batch Loss = 0.180490, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25137394666671753, Accuracy = 0.9386193752288818\n",
      "Iter #3565568:  Learning rate = 0.001198:   Batch Loss = 0.142996, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28314080834388733, Accuracy = 0.9332290291786194\n",
      "Iter #3567616:  Learning rate = 0.001198:   Batch Loss = 0.261835, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2494591474533081, Accuracy = 0.9374021887779236\n",
      "Iter #3569664:  Learning rate = 0.001198:   Batch Loss = 0.270044, Accuracy = 0.921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2596699595451355, Accuracy = 0.9323595762252808\n",
      "Iter #3571712:  Learning rate = 0.001198:   Batch Loss = 0.202444, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25449761748313904, Accuracy = 0.934272289276123\n",
      "Iter #3573760:  Learning rate = 0.001198:   Batch Loss = 0.163423, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28523704409599304, Accuracy = 0.9290558099746704\n",
      "Iter #3575808:  Learning rate = 0.001198:   Batch Loss = 0.170681, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2980634272098541, Accuracy = 0.9267953634262085\n",
      "Iter #3577856:  Learning rate = 0.001198:   Batch Loss = 0.145723, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2743700444698334, Accuracy = 0.9281864166259766\n",
      "Iter #3579904:  Learning rate = 0.001198:   Batch Loss = 0.202359, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2809349596500397, Accuracy = 0.9287080764770508\n",
      "Iter #3581952:  Learning rate = 0.001198:   Batch Loss = 0.159541, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24029918015003204, Accuracy = 0.9375760555267334\n",
      "Iter #3584000:  Learning rate = 0.001198:   Batch Loss = 0.197418, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2730238437652588, Accuracy = 0.9274908900260925\n",
      "Iter #3586048:  Learning rate = 0.001198:   Batch Loss = 0.188162, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2522345185279846, Accuracy = 0.9340984225273132\n",
      "Iter #3588096:  Learning rate = 0.001198:   Batch Loss = 0.196805, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24960890412330627, Accuracy = 0.9337506294250488\n",
      "Iter #3590144:  Learning rate = 0.001198:   Batch Loss = 0.203499, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25064873695373535, Accuracy = 0.9346200823783875\n",
      "Iter #3592192:  Learning rate = 0.001198:   Batch Loss = 0.196640, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2555982172489166, Accuracy = 0.9351417422294617\n",
      "Iter #3594240:  Learning rate = 0.001198:   Batch Loss = 0.169078, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2512300908565521, Accuracy = 0.9387932419776917\n",
      "Iter #3596288:  Learning rate = 0.001198:   Batch Loss = 0.210851, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2611621916294098, Accuracy = 0.9334028959274292\n",
      "Iter #3598336:  Learning rate = 0.001198:   Batch Loss = 0.189579, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.32503950595855713, Accuracy = 0.9040166735649109\n",
      "Iter #3600384:  Learning rate = 0.001150:   Batch Loss = 0.195902, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27828776836395264, Accuracy = 0.9287080764770508\n",
      "Iter #3602432:  Learning rate = 0.001150:   Batch Loss = 0.201798, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2533343732357025, Accuracy = 0.9340984225273132\n",
      "Iter #3604480:  Learning rate = 0.001150:   Batch Loss = 0.202234, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25440049171447754, Accuracy = 0.9351417422294617\n",
      "Iter #3606528:  Learning rate = 0.001150:   Batch Loss = 0.186917, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2639094889163971, Accuracy = 0.9320118427276611\n",
      "Iter #3608576:  Learning rate = 0.001150:   Batch Loss = 0.175511, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2485635131597519, Accuracy = 0.9393149018287659\n",
      "Iter #3610624:  Learning rate = 0.001150:   Batch Loss = 0.148389, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28034621477127075, Accuracy = 0.9266214370727539\n",
      "Iter #3612672:  Learning rate = 0.001150:   Batch Loss = 0.191874, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2690458297729492, Accuracy = 0.9323595762252808\n",
      "Iter #3614720:  Learning rate = 0.001150:   Batch Loss = 0.171520, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2628488838672638, Accuracy = 0.9358372688293457\n",
      "Iter #3616768:  Learning rate = 0.001150:   Batch Loss = 0.176027, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2719773054122925, Accuracy = 0.9325334429740906\n",
      "Iter #3618816:  Learning rate = 0.001150:   Batch Loss = 0.198950, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26263758540153503, Accuracy = 0.9387932419776917\n",
      "Iter #3620864:  Learning rate = 0.001150:   Batch Loss = 0.167549, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2647313177585602, Accuracy = 0.9339245557785034\n",
      "Iter #3622912:  Learning rate = 0.001150:   Batch Loss = 0.205493, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2737685739994049, Accuracy = 0.9292296767234802\n",
      "Iter #3624960:  Learning rate = 0.001150:   Batch Loss = 0.191174, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2691819965839386, Accuracy = 0.9339245557785034\n",
      "Iter #3627008:  Learning rate = 0.001150:   Batch Loss = 0.201588, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24639934301376343, Accuracy = 0.9417492747306824\n",
      "Iter #3629056:  Learning rate = 0.001150:   Batch Loss = 0.161431, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26258283853530884, Accuracy = 0.9318379163742065\n",
      "Iter #3631104:  Learning rate = 0.001150:   Batch Loss = 0.177277, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26049840450286865, Accuracy = 0.9398365616798401\n",
      "Iter #3633152:  Learning rate = 0.001150:   Batch Loss = 0.174337, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25770702958106995, Accuracy = 0.9361850023269653\n",
      "Iter #3635200:  Learning rate = 0.001150:   Batch Loss = 0.154874, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2611583471298218, Accuracy = 0.9339245557785034\n",
      "Iter #3637248:  Learning rate = 0.001150:   Batch Loss = 0.169026, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27840283513069153, Accuracy = 0.9210572242736816\n",
      "Iter #3639296:  Learning rate = 0.001150:   Batch Loss = 0.195911, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.30183422565460205, Accuracy = 0.9269692301750183\n",
      "Iter #3641344:  Learning rate = 0.001150:   Batch Loss = 0.181953, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2752833962440491, Accuracy = 0.9306207895278931\n",
      "Iter #3643392:  Learning rate = 0.001150:   Batch Loss = 0.242736, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2665138244628906, Accuracy = 0.9274908900260925\n",
      "Iter #3645440:  Learning rate = 0.001150:   Batch Loss = 0.191572, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27693992853164673, Accuracy = 0.9259259104728699\n",
      "Iter #3647488:  Learning rate = 0.001150:   Batch Loss = 0.187921, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28068122267723083, Accuracy = 0.9248826503753662\n",
      "Iter #3649536:  Learning rate = 0.001150:   Batch Loss = 0.176521, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2644803822040558, Accuracy = 0.9287080764770508\n",
      "Iter #3651584:  Learning rate = 0.001150:   Batch Loss = 0.246323, Accuracy = 0.92578125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2613285481929779, Accuracy = 0.9269692301750183\n",
      "Iter #3653632:  Learning rate = 0.001150:   Batch Loss = 0.194099, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26588141918182373, Accuracy = 0.9314901828765869\n",
      "Iter #3655680:  Learning rate = 0.001150:   Batch Loss = 0.190981, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2662861943244934, Accuracy = 0.9283602833747864\n",
      "Iter #3657728:  Learning rate = 0.001150:   Batch Loss = 0.201853, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2666527032852173, Accuracy = 0.9297513365745544\n",
      "Iter #3659776:  Learning rate = 0.001150:   Batch Loss = 0.201777, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27964603900909424, Accuracy = 0.9292296767234802\n",
      "Iter #3661824:  Learning rate = 0.001150:   Batch Loss = 0.184887, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.263577938079834, Accuracy = 0.9320118427276611\n",
      "Iter #3663872:  Learning rate = 0.001150:   Batch Loss = 0.216909, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25535669922828674, Accuracy = 0.9358372688293457\n",
      "Iter #3665920:  Learning rate = 0.001150:   Batch Loss = 0.177311, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2793233394622803, Accuracy = 0.9205355644226074\n",
      "Iter #3667968:  Learning rate = 0.001150:   Batch Loss = 0.164786, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2643365263938904, Accuracy = 0.9259259104728699\n",
      "Iter #3670016:  Learning rate = 0.001150:   Batch Loss = 0.204153, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26401227712631226, Accuracy = 0.9292296767234802\n",
      "Iter #3672064:  Learning rate = 0.001150:   Batch Loss = 0.176774, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24663987755775452, Accuracy = 0.9410537481307983\n",
      "Iter #3674112:  Learning rate = 0.001150:   Batch Loss = 0.187623, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2635039687156677, Accuracy = 0.9295774698257446\n",
      "Iter #3676160:  Learning rate = 0.001150:   Batch Loss = 0.178725, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25362297892570496, Accuracy = 0.933576762676239\n",
      "Iter #3678208:  Learning rate = 0.001150:   Batch Loss = 0.174829, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2571026086807251, Accuracy = 0.9330551028251648\n",
      "Iter #3680256:  Learning rate = 0.001150:   Batch Loss = 0.160466, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2483532726764679, Accuracy = 0.9400104284286499\n",
      "Iter #3682304:  Learning rate = 0.001150:   Batch Loss = 0.174782, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25968554615974426, Accuracy = 0.9311423897743225\n",
      "Iter #3684352:  Learning rate = 0.001150:   Batch Loss = 0.194197, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2520917057991028, Accuracy = 0.9370543956756592\n",
      "Iter #3686400:  Learning rate = 0.001150:   Batch Loss = 0.194209, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25275591015815735, Accuracy = 0.9306207895278931\n",
      "Iter #3688448:  Learning rate = 0.001150:   Batch Loss = 0.200285, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27771270275115967, Accuracy = 0.9252303838729858\n",
      "Iter #3690496:  Learning rate = 0.001150:   Batch Loss = 0.143259, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25083017349243164, Accuracy = 0.9396626949310303\n",
      "Iter #3692544:  Learning rate = 0.001150:   Batch Loss = 0.211991, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24667662382125854, Accuracy = 0.9386193752288818\n",
      "Iter #3694592:  Learning rate = 0.001150:   Batch Loss = 0.151116, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25944942235946655, Accuracy = 0.9358372688293457\n",
      "Iter #3696640:  Learning rate = 0.001150:   Batch Loss = 0.170930, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24802269041538239, Accuracy = 0.9361850023269653\n",
      "Iter #3698688:  Learning rate = 0.001150:   Batch Loss = 0.175301, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2501876652240753, Accuracy = 0.9368805289268494\n",
      "Iter #3700736:  Learning rate = 0.001104:   Batch Loss = 0.175531, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28432223200798035, Accuracy = 0.9271430969238281\n",
      "Iter #3702784:  Learning rate = 0.001104:   Batch Loss = 0.179605, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28440797328948975, Accuracy = 0.9215788841247559\n",
      "Iter #3704832:  Learning rate = 0.001104:   Batch Loss = 0.182537, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24623316526412964, Accuracy = 0.9361850023269653\n",
      "Iter #3706880:  Learning rate = 0.001104:   Batch Loss = 0.224517, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2600138187408447, Accuracy = 0.9351417422294617\n",
      "Iter #3708928:  Learning rate = 0.001104:   Batch Loss = 0.171637, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26461726427078247, Accuracy = 0.9340984225273132\n",
      "Iter #3710976:  Learning rate = 0.001104:   Batch Loss = 0.234906, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2569570243358612, Accuracy = 0.9325334429740906\n",
      "Iter #3713024:  Learning rate = 0.001104:   Batch Loss = 0.217340, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2522928714752197, Accuracy = 0.9351417422294617\n",
      "Iter #3715072:  Learning rate = 0.001104:   Batch Loss = 0.196643, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2584024667739868, Accuracy = 0.9294036030769348\n",
      "Iter #3717120:  Learning rate = 0.001104:   Batch Loss = 0.169806, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2718304395675659, Accuracy = 0.9266214370727539\n",
      "Iter #3719168:  Learning rate = 0.001104:   Batch Loss = 0.151722, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26290878653526306, Accuracy = 0.9344461560249329\n",
      "Iter #3721216:  Learning rate = 0.001104:   Batch Loss = 0.198541, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2604442834854126, Accuracy = 0.9386193752288818\n",
      "Iter #3723264:  Learning rate = 0.001104:   Batch Loss = 0.165561, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2624874711036682, Accuracy = 0.9318379163742065\n",
      "Iter #3725312:  Learning rate = 0.001104:   Batch Loss = 0.192801, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2680577337741852, Accuracy = 0.9323595762252808\n",
      "Iter #3727360:  Learning rate = 0.001104:   Batch Loss = 0.172184, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2574825584888458, Accuracy = 0.9311423897743225\n",
      "Iter #3729408:  Learning rate = 0.001104:   Batch Loss = 0.222743, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24715939164161682, Accuracy = 0.9380977153778076\n",
      "Iter #3731456:  Learning rate = 0.001104:   Batch Loss = 0.174319, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2717393636703491, Accuracy = 0.9260998368263245\n",
      "Iter #3733504:  Learning rate = 0.001104:   Batch Loss = 0.172566, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23183268308639526, Accuracy = 0.9434880614280701\n",
      "Iter #3735552:  Learning rate = 0.001104:   Batch Loss = 0.152137, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24571093916893005, Accuracy = 0.9417492747306824\n",
      "Iter #3737600:  Learning rate = 0.001104:   Batch Loss = 0.179644, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24090859293937683, Accuracy = 0.9412276148796082\n",
      "Iter #3739648:  Learning rate = 0.001104:   Batch Loss = 0.173327, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24307627975940704, Accuracy = 0.9394887685775757\n",
      "Iter #3741696:  Learning rate = 0.001104:   Batch Loss = 0.123968, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24397945404052734, Accuracy = 0.9401842951774597\n",
      "Iter #3743744:  Learning rate = 0.001104:   Batch Loss = 0.163034, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27406615018844604, Accuracy = 0.9323595762252808\n",
      "Iter #3745792:  Learning rate = 0.001104:   Batch Loss = 0.217320, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2742636799812317, Accuracy = 0.9356633424758911\n",
      "Iter #3747840:  Learning rate = 0.001104:   Batch Loss = 0.210452, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.251975953578949, Accuracy = 0.9354894757270813\n",
      "Iter #3749888:  Learning rate = 0.001104:   Batch Loss = 0.188014, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27173468470573425, Accuracy = 0.9247087240219116\n",
      "Iter #3751936:  Learning rate = 0.001104:   Batch Loss = 0.169268, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24979037046432495, Accuracy = 0.9354894757270813\n",
      "Iter #3753984:  Learning rate = 0.001104:   Batch Loss = 0.161060, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23370125889778137, Accuracy = 0.9452269077301025\n",
      "Iter #3756032:  Learning rate = 0.001104:   Batch Loss = 0.183613, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2575165927410126, Accuracy = 0.9267953634262085\n",
      "Iter #3758080:  Learning rate = 0.001104:   Batch Loss = 0.215081, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24732118844985962, Accuracy = 0.9360111355781555\n",
      "Iter #3760128:  Learning rate = 0.001104:   Batch Loss = 0.211095, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24591122567653656, Accuracy = 0.9370543956756592\n",
      "Iter #3762176:  Learning rate = 0.001104:   Batch Loss = 0.191070, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25160518288612366, Accuracy = 0.9393149018287659\n",
      "Iter #3764224:  Learning rate = 0.001104:   Batch Loss = 0.151992, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2528933882713318, Accuracy = 0.933576762676239\n",
      "Iter #3766272:  Learning rate = 0.001104:   Batch Loss = 0.181044, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25290071964263916, Accuracy = 0.9347939491271973\n",
      "Iter #3768320:  Learning rate = 0.001104:   Batch Loss = 0.175926, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26866692304611206, Accuracy = 0.9339245557785034\n",
      "Iter #3770368:  Learning rate = 0.001104:   Batch Loss = 0.169785, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25327950716018677, Accuracy = 0.9332290291786194\n",
      "Iter #3772416:  Learning rate = 0.001104:   Batch Loss = 0.184837, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2643728256225586, Accuracy = 0.9297513365745544\n",
      "Iter #3774464:  Learning rate = 0.001104:   Batch Loss = 0.178767, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25542163848876953, Accuracy = 0.9382715821266174\n",
      "Iter #3776512:  Learning rate = 0.001104:   Batch Loss = 0.197972, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2524409294128418, Accuracy = 0.9354894757270813\n",
      "Iter #3778560:  Learning rate = 0.001104:   Batch Loss = 0.166460, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25621309876441956, Accuracy = 0.9354894757270813\n",
      "Iter #3780608:  Learning rate = 0.001104:   Batch Loss = 0.127401, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2568802535533905, Accuracy = 0.939141035079956\n",
      "Iter #3782656:  Learning rate = 0.001104:   Batch Loss = 0.201697, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2618909478187561, Accuracy = 0.9318379163742065\n",
      "Iter #3784704:  Learning rate = 0.001104:   Batch Loss = 0.218597, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2434162199497223, Accuracy = 0.9379238486289978\n",
      "Iter #3786752:  Learning rate = 0.001104:   Batch Loss = 0.169888, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24771791696548462, Accuracy = 0.9387932419776917\n",
      "Iter #3788800:  Learning rate = 0.001104:   Batch Loss = 0.160295, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27073565125465393, Accuracy = 0.9316640496253967\n",
      "Iter #3790848:  Learning rate = 0.001104:   Batch Loss = 0.166031, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24170497059822083, Accuracy = 0.937749981880188\n",
      "Iter #3792896:  Learning rate = 0.001104:   Batch Loss = 0.201209, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2531755864620209, Accuracy = 0.9304468631744385\n",
      "Iter #3794944:  Learning rate = 0.001104:   Batch Loss = 0.159978, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2425432801246643, Accuracy = 0.9394887685775757\n",
      "Iter #3796992:  Learning rate = 0.001104:   Batch Loss = 0.160048, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2406555712223053, Accuracy = 0.9422709345817566\n",
      "Iter #3799040:  Learning rate = 0.001104:   Batch Loss = 0.191511, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2462991178035736, Accuracy = 0.9417492747306824\n",
      "Iter #3801088:  Learning rate = 0.001060:   Batch Loss = 0.143590, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2537260949611664, Accuracy = 0.9379238486289978\n",
      "Iter #3803136:  Learning rate = 0.001060:   Batch Loss = 0.184604, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2707913815975189, Accuracy = 0.9360111355781555\n",
      "Iter #3805184:  Learning rate = 0.001060:   Batch Loss = 0.176790, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2629910707473755, Accuracy = 0.9393149018287659\n",
      "Iter #3807232:  Learning rate = 0.001060:   Batch Loss = 0.204025, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2726293206214905, Accuracy = 0.932185709476471\n",
      "Iter #3809280:  Learning rate = 0.001060:   Batch Loss = 0.150246, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.29625147581100464, Accuracy = 0.9245348572731018\n",
      "Iter #3811328:  Learning rate = 0.001060:   Batch Loss = 0.177704, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2543201744556427, Accuracy = 0.9387932419776917\n",
      "Iter #3813376:  Learning rate = 0.001060:   Batch Loss = 0.169594, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23781593143939972, Accuracy = 0.9405320882797241\n",
      "Iter #3815424:  Learning rate = 0.001060:   Batch Loss = 0.184778, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2416914701461792, Accuracy = 0.941401481628418\n",
      "Iter #3817472:  Learning rate = 0.001060:   Batch Loss = 0.175079, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2597026824951172, Accuracy = 0.9370543956756592\n",
      "Iter #3819520:  Learning rate = 0.001060:   Batch Loss = 0.175356, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24412530660629272, Accuracy = 0.9363588690757751\n",
      "Iter #3821568:  Learning rate = 0.001060:   Batch Loss = 0.195371, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2543448805809021, Accuracy = 0.9340984225273132\n",
      "Iter #3823616:  Learning rate = 0.001060:   Batch Loss = 0.174898, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24048015475273132, Accuracy = 0.9408798217773438\n",
      "Iter #3825664:  Learning rate = 0.001060:   Batch Loss = 0.195471, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24981269240379333, Accuracy = 0.9361850023269653\n",
      "Iter #3827712:  Learning rate = 0.001060:   Batch Loss = 0.159857, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25347769260406494, Accuracy = 0.9361850023269653\n",
      "Iter #3829760:  Learning rate = 0.001060:   Batch Loss = 0.159346, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25888317823410034, Accuracy = 0.9415754079818726\n",
      "Iter #3831808:  Learning rate = 0.001060:   Batch Loss = 0.187139, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24243304133415222, Accuracy = 0.9394887685775757\n",
      "Iter #3833856:  Learning rate = 0.001060:   Batch Loss = 0.152766, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2497813105583191, Accuracy = 0.9375760555267334\n",
      "Iter #3835904:  Learning rate = 0.001060:   Batch Loss = 0.203691, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24633105099201202, Accuracy = 0.9361850023269653\n",
      "Iter #3837952:  Learning rate = 0.001060:   Batch Loss = 0.141984, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25017422437667847, Accuracy = 0.9389671087265015\n",
      "Iter #3840000:  Learning rate = 0.001060:   Batch Loss = 0.166381, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25426143407821655, Accuracy = 0.9389671087265015\n",
      "Iter #3842048:  Learning rate = 0.001060:   Batch Loss = 0.187106, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2778359651565552, Accuracy = 0.9229699373245239\n",
      "Iter #3844096:  Learning rate = 0.001060:   Batch Loss = 0.179553, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2516547739505768, Accuracy = 0.9401842951774597\n",
      "Iter #3846144:  Learning rate = 0.001060:   Batch Loss = 0.195522, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27742964029312134, Accuracy = 0.9285341501235962\n",
      "Iter #3848192:  Learning rate = 0.001060:   Batch Loss = 0.171770, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26284000277519226, Accuracy = 0.9344461560249329\n",
      "Iter #3850240:  Learning rate = 0.001060:   Batch Loss = 0.159515, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2463916540145874, Accuracy = 0.9403582215309143\n",
      "Iter #3852288:  Learning rate = 0.001060:   Batch Loss = 0.203214, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.258405476808548, Accuracy = 0.9354894757270813\n",
      "Iter #3854336:  Learning rate = 0.001060:   Batch Loss = 0.193187, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2502196133136749, Accuracy = 0.933576762676239\n",
      "Iter #3856384:  Learning rate = 0.001060:   Batch Loss = 0.166335, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24106043577194214, Accuracy = 0.9419231414794922\n",
      "Iter #3858432:  Learning rate = 0.001060:   Batch Loss = 0.236243, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2492453157901764, Accuracy = 0.9327073693275452\n",
      "Iter #3860480:  Learning rate = 0.001060:   Batch Loss = 0.169638, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23318195343017578, Accuracy = 0.947661280632019\n",
      "Iter #3862528:  Learning rate = 0.001060:   Batch Loss = 0.211255, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24043451249599457, Accuracy = 0.942097008228302\n",
      "Iter #3864576:  Learning rate = 0.001060:   Batch Loss = 0.155590, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26912328600883484, Accuracy = 0.9323595762252808\n",
      "Iter #3866624:  Learning rate = 0.001060:   Batch Loss = 0.151350, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23167863488197327, Accuracy = 0.9450530409812927\n",
      "Iter #3868672:  Learning rate = 0.001060:   Batch Loss = 0.195576, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24106422066688538, Accuracy = 0.9405320882797241\n",
      "Iter #3870720:  Learning rate = 0.001060:   Batch Loss = 0.168520, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25125476717948914, Accuracy = 0.9387932419776917\n",
      "Iter #3872768:  Learning rate = 0.001060:   Batch Loss = 0.192747, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26643139123916626, Accuracy = 0.9267953634262085\n",
      "Iter #3874816:  Learning rate = 0.001060:   Batch Loss = 0.246709, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2506200671195984, Accuracy = 0.9325334429740906\n",
      "Iter #3876864:  Learning rate = 0.001060:   Batch Loss = 0.184339, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2463814914226532, Accuracy = 0.9334028959274292\n",
      "Iter #3878912:  Learning rate = 0.001060:   Batch Loss = 0.182991, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25874805450439453, Accuracy = 0.9327073693275452\n",
      "Iter #3880960:  Learning rate = 0.001060:   Batch Loss = 0.164429, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23757730424404144, Accuracy = 0.9380977153778076\n",
      "Iter #3883008:  Learning rate = 0.001060:   Batch Loss = 0.168619, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23629263043403625, Accuracy = 0.9408798217773438\n",
      "Iter #3885056:  Learning rate = 0.001060:   Batch Loss = 0.190205, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.255929172039032, Accuracy = 0.939141035079956\n",
      "Iter #3887104:  Learning rate = 0.001060:   Batch Loss = 0.149431, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24422617256641388, Accuracy = 0.9419231414794922\n",
      "Iter #3889152:  Learning rate = 0.001060:   Batch Loss = 0.209001, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2404807209968567, Accuracy = 0.9438358545303345\n",
      "Iter #3891200:  Learning rate = 0.001060:   Batch Loss = 0.193984, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25379133224487305, Accuracy = 0.9339245557785034\n",
      "Iter #3893248:  Learning rate = 0.001060:   Batch Loss = 0.203082, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24848921597003937, Accuracy = 0.9368805289268494\n",
      "Iter #3895296:  Learning rate = 0.001060:   Batch Loss = 0.206966, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24266882240772247, Accuracy = 0.938445508480072\n",
      "Iter #3897344:  Learning rate = 0.001060:   Batch Loss = 0.148351, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25154998898506165, Accuracy = 0.9382715821266174\n",
      "Iter #3899392:  Learning rate = 0.001060:   Batch Loss = 0.205151, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2498370260000229, Accuracy = 0.9351417422294617\n",
      "Iter #3901440:  Learning rate = 0.001018:   Batch Loss = 0.174997, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25264880061149597, Accuracy = 0.9356633424758911\n",
      "Iter #3903488:  Learning rate = 0.001018:   Batch Loss = 0.166926, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25818437337875366, Accuracy = 0.9363588690757751\n",
      "Iter #3905536:  Learning rate = 0.001018:   Batch Loss = 0.138389, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24115584790706635, Accuracy = 0.9396626949310303\n",
      "Iter #3907584:  Learning rate = 0.001018:   Batch Loss = 0.180946, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2573350667953491, Accuracy = 0.9387932419776917\n",
      "Iter #3909632:  Learning rate = 0.001018:   Batch Loss = 0.188916, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24322545528411865, Accuracy = 0.9419231414794922\n",
      "Iter #3911680:  Learning rate = 0.001018:   Batch Loss = 0.220763, Accuracy = 0.93359375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23842072486877441, Accuracy = 0.941401481628418\n",
      "Iter #3913728:  Learning rate = 0.001018:   Batch Loss = 0.169766, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24722686409950256, Accuracy = 0.9407059550285339\n",
      "Iter #3915776:  Learning rate = 0.001018:   Batch Loss = 0.165807, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2474883645772934, Accuracy = 0.9387932419776917\n",
      "Iter #3917824:  Learning rate = 0.001018:   Batch Loss = 0.168213, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24016694724559784, Accuracy = 0.946270227432251\n",
      "Iter #3919872:  Learning rate = 0.001018:   Batch Loss = 0.184091, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2527589499950409, Accuracy = 0.9393149018287659\n",
      "Iter #3921920:  Learning rate = 0.001018:   Batch Loss = 0.162926, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24607333540916443, Accuracy = 0.938445508480072\n",
      "Iter #3923968:  Learning rate = 0.001018:   Batch Loss = 0.199370, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2401505410671234, Accuracy = 0.9403582215309143\n",
      "Iter #3926016:  Learning rate = 0.001018:   Batch Loss = 0.159062, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23826837539672852, Accuracy = 0.9447052478790283\n",
      "Iter #3928064:  Learning rate = 0.001018:   Batch Loss = 0.175223, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24731391668319702, Accuracy = 0.9419231414794922\n",
      "Iter #3930112:  Learning rate = 0.001018:   Batch Loss = 0.163114, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24205443263053894, Accuracy = 0.9419231414794922\n",
      "Iter #3932160:  Learning rate = 0.001018:   Batch Loss = 0.155210, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23637348413467407, Accuracy = 0.9433141946792603\n",
      "Iter #3934208:  Learning rate = 0.001018:   Batch Loss = 0.169062, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25888028740882874, Accuracy = 0.9325334429740906\n",
      "Iter #3936256:  Learning rate = 0.001018:   Batch Loss = 0.160895, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25629812479019165, Accuracy = 0.9347939491271973\n",
      "Iter #3938304:  Learning rate = 0.001018:   Batch Loss = 0.168101, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24534077942371368, Accuracy = 0.9367066621780396\n",
      "Iter #3940352:  Learning rate = 0.001018:   Batch Loss = 0.209683, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25532209873199463, Accuracy = 0.9361850023269653\n",
      "Iter #3942400:  Learning rate = 0.001018:   Batch Loss = 0.179652, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25856995582580566, Accuracy = 0.9387932419776917\n",
      "Iter #3944448:  Learning rate = 0.001018:   Batch Loss = 0.191053, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2469775378704071, Accuracy = 0.9393149018287659\n",
      "Iter #3946496:  Learning rate = 0.001018:   Batch Loss = 0.182107, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2490050494670868, Accuracy = 0.934272289276123\n",
      "Iter #3948544:  Learning rate = 0.001018:   Batch Loss = 0.200678, Accuracy = 0.9453125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24178245663642883, Accuracy = 0.9431403279304504\n",
      "Iter #3950592:  Learning rate = 0.001018:   Batch Loss = 0.162399, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2446306049823761, Accuracy = 0.9412276148796082\n",
      "Iter #3952640:  Learning rate = 0.001018:   Batch Loss = 0.161495, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24766309559345245, Accuracy = 0.9386193752288818\n",
      "Iter #3954688:  Learning rate = 0.001018:   Batch Loss = 0.170757, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2632662057876587, Accuracy = 0.9281864166259766\n",
      "Iter #3956736:  Learning rate = 0.001018:   Batch Loss = 0.142805, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23246774077415466, Accuracy = 0.9455747008323669\n",
      "Iter #3958784:  Learning rate = 0.001018:   Batch Loss = 0.181964, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25903037190437317, Accuracy = 0.9387932419776917\n",
      "Iter #3960832:  Learning rate = 0.001018:   Batch Loss = 0.168189, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23512861132621765, Accuracy = 0.9387932419776917\n",
      "Iter #3962880:  Learning rate = 0.001018:   Batch Loss = 0.166625, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2302112728357315, Accuracy = 0.9424448013305664\n",
      "Iter #3964928:  Learning rate = 0.001018:   Batch Loss = 0.148981, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2536899447441101, Accuracy = 0.9309685230255127\n",
      "Iter #3966976:  Learning rate = 0.001018:   Batch Loss = 0.172849, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24258685111999512, Accuracy = 0.9398365616798401\n",
      "Iter #3969024:  Learning rate = 0.001018:   Batch Loss = 0.159803, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2416839301586151, Accuracy = 0.9389671087265015\n",
      "Iter #3971072:  Learning rate = 0.001018:   Batch Loss = 0.200108, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2349221408367157, Accuracy = 0.9450530409812927\n",
      "Iter #3973120:  Learning rate = 0.001018:   Batch Loss = 0.179089, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2541106939315796, Accuracy = 0.9353156089782715\n",
      "Iter #3975168:  Learning rate = 0.001018:   Batch Loss = 0.150210, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23984593152999878, Accuracy = 0.9424448013305664\n",
      "Iter #3977216:  Learning rate = 0.001018:   Batch Loss = 0.171039, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23780658841133118, Accuracy = 0.9454007744789124\n",
      "Iter #3979264:  Learning rate = 0.001018:   Batch Loss = 0.176963, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22985363006591797, Accuracy = 0.9419231414794922\n",
      "Iter #3981312:  Learning rate = 0.001018:   Batch Loss = 0.163889, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23746952414512634, Accuracy = 0.9460963606834412\n",
      "Iter #3983360:  Learning rate = 0.001018:   Batch Loss = 0.154757, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25254026055336, Accuracy = 0.9356633424758911\n",
      "Iter #3985408:  Learning rate = 0.001018:   Batch Loss = 0.149549, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2455047070980072, Accuracy = 0.9363588690757751\n",
      "Iter #3987456:  Learning rate = 0.001018:   Batch Loss = 0.187886, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2439996600151062, Accuracy = 0.9358372688293457\n",
      "Iter #3989504:  Learning rate = 0.001018:   Batch Loss = 0.162070, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2565441131591797, Accuracy = 0.9300991296768188\n",
      "Iter #3991552:  Learning rate = 0.001018:   Batch Loss = 0.147530, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2298659086227417, Accuracy = 0.946965754032135\n",
      "Iter #3993600:  Learning rate = 0.001018:   Batch Loss = 0.197742, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.258450984954834, Accuracy = 0.932881236076355\n",
      "Iter #3995648:  Learning rate = 0.001018:   Batch Loss = 0.202087, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2589065134525299, Accuracy = 0.932881236076355\n",
      "Iter #3997696:  Learning rate = 0.001018:   Batch Loss = 0.173535, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24031516909599304, Accuracy = 0.9419231414794922\n",
      "Iter #3999744:  Learning rate = 0.001018:   Batch Loss = 0.184961, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25401920080184937, Accuracy = 0.9311423897743225\n",
      "Iter #4001792:  Learning rate = 0.000977:   Batch Loss = 0.173024, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2387574017047882, Accuracy = 0.9382715821266174\n",
      "Iter #4003840:  Learning rate = 0.000977:   Batch Loss = 0.162242, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23000574111938477, Accuracy = 0.9466179609298706\n",
      "Iter #4005888:  Learning rate = 0.000977:   Batch Loss = 0.207280, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24211385846138, Accuracy = 0.9440097212791443\n",
      "Iter #4007936:  Learning rate = 0.000977:   Batch Loss = 0.146378, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23827099800109863, Accuracy = 0.9467918872833252\n",
      "Iter #4009984:  Learning rate = 0.000977:   Batch Loss = 0.188604, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2616909146308899, Accuracy = 0.932185709476471\n",
      "Iter #4012032:  Learning rate = 0.000977:   Batch Loss = 0.146655, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24612122774124146, Accuracy = 0.9361850023269653\n",
      "Iter #4014080:  Learning rate = 0.000977:   Batch Loss = 0.184007, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.261153906583786, Accuracy = 0.9332290291786194\n",
      "Iter #4016128:  Learning rate = 0.000977:   Batch Loss = 0.150047, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24868157505989075, Accuracy = 0.9375760555267334\n",
      "Iter #4018176:  Learning rate = 0.000977:   Batch Loss = 0.177877, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2308429479598999, Accuracy = 0.9452269077301025\n",
      "Iter #4020224:  Learning rate = 0.000977:   Batch Loss = 0.187449, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23771853744983673, Accuracy = 0.9426186680793762\n",
      "Iter #4022272:  Learning rate = 0.000977:   Batch Loss = 0.192209, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.253317654132843, Accuracy = 0.9354894757270813\n",
      "Iter #4024320:  Learning rate = 0.000977:   Batch Loss = 0.162808, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23786339163780212, Accuracy = 0.9447052478790283\n",
      "Iter #4026368:  Learning rate = 0.000977:   Batch Loss = 0.213499, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22994065284729004, Accuracy = 0.9478351473808289\n",
      "Iter #4028416:  Learning rate = 0.000977:   Batch Loss = 0.194497, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23686367273330688, Accuracy = 0.9438358545303345\n",
      "Iter #4030464:  Learning rate = 0.000977:   Batch Loss = 0.175257, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24693866074085236, Accuracy = 0.9401842951774597\n",
      "Iter #4032512:  Learning rate = 0.000977:   Batch Loss = 0.161376, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2501477897167206, Accuracy = 0.9407059550285339\n",
      "Iter #4034560:  Learning rate = 0.000977:   Batch Loss = 0.182529, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2523196339607239, Accuracy = 0.9396626949310303\n",
      "Iter #4036608:  Learning rate = 0.000977:   Batch Loss = 0.183048, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25320690870285034, Accuracy = 0.9340984225273132\n",
      "Iter #4038656:  Learning rate = 0.000977:   Batch Loss = 0.147796, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24253970384597778, Accuracy = 0.9400104284286499\n",
      "Iter #4040704:  Learning rate = 0.000977:   Batch Loss = 0.165113, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24900662899017334, Accuracy = 0.9389671087265015\n",
      "Iter #4042752:  Learning rate = 0.000977:   Batch Loss = 0.169077, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24989469349384308, Accuracy = 0.9361850023269653\n",
      "Iter #4044800:  Learning rate = 0.000977:   Batch Loss = 0.222634, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22365564107894897, Accuracy = 0.9474874138832092\n",
      "Iter #4046848:  Learning rate = 0.000977:   Batch Loss = 0.207311, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23341214656829834, Accuracy = 0.9408798217773438\n",
      "Iter #4048896:  Learning rate = 0.000977:   Batch Loss = 0.156060, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25105586647987366, Accuracy = 0.9302729964256287\n",
      "Iter #4050944:  Learning rate = 0.000977:   Batch Loss = 0.150547, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24241198599338531, Accuracy = 0.9387932419776917\n",
      "Iter #4052992:  Learning rate = 0.000977:   Batch Loss = 0.192918, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23565293848514557, Accuracy = 0.9433141946792603\n",
      "Iter #4055040:  Learning rate = 0.000977:   Batch Loss = 0.202813, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2345854938030243, Accuracy = 0.946965754032135\n",
      "Iter #4057088:  Learning rate = 0.000977:   Batch Loss = 0.205856, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25221967697143555, Accuracy = 0.9415754079818726\n",
      "Iter #4059136:  Learning rate = 0.000977:   Batch Loss = 0.148055, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23495620489120483, Accuracy = 0.9454007744789124\n",
      "Iter #4061184:  Learning rate = 0.000977:   Batch Loss = 0.171342, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23811647295951843, Accuracy = 0.9407059550285339\n",
      "Iter #4063232:  Learning rate = 0.000977:   Batch Loss = 0.160454, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22680959105491638, Accuracy = 0.9459224343299866\n",
      "Iter #4065280:  Learning rate = 0.000977:   Batch Loss = 0.175506, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24556225538253784, Accuracy = 0.9398365616798401\n",
      "Iter #4067328:  Learning rate = 0.000977:   Batch Loss = 0.155987, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26659712195396423, Accuracy = 0.9316640496253967\n",
      "Iter #4069376:  Learning rate = 0.000977:   Batch Loss = 0.200135, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25663983821868896, Accuracy = 0.9386193752288818\n",
      "Iter #4071424:  Learning rate = 0.000977:   Batch Loss = 0.165611, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23615925014019012, Accuracy = 0.9455747008323669\n",
      "Iter #4073472:  Learning rate = 0.000977:   Batch Loss = 0.193334, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.254487544298172, Accuracy = 0.9356633424758911\n",
      "Iter #4075520:  Learning rate = 0.000977:   Batch Loss = 0.163122, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2293592244386673, Accuracy = 0.9452269077301025\n",
      "Iter #4077568:  Learning rate = 0.000977:   Batch Loss = 0.180012, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24496373534202576, Accuracy = 0.9394887685775757\n",
      "Iter #4079616:  Learning rate = 0.000977:   Batch Loss = 0.182315, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23727071285247803, Accuracy = 0.941401481628418\n",
      "Iter #4081664:  Learning rate = 0.000977:   Batch Loss = 0.132000, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24055242538452148, Accuracy = 0.9434880614280701\n",
      "Iter #4083712:  Learning rate = 0.000977:   Batch Loss = 0.175186, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2423575520515442, Accuracy = 0.9422709345817566\n",
      "Iter #4085760:  Learning rate = 0.000977:   Batch Loss = 0.203607, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24580207467079163, Accuracy = 0.9370543956756592\n",
      "Iter #4087808:  Learning rate = 0.000977:   Batch Loss = 0.171108, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2405315339565277, Accuracy = 0.9424448013305664\n",
      "Iter #4089856:  Learning rate = 0.000977:   Batch Loss = 0.167211, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.239914670586586, Accuracy = 0.9434880614280701\n",
      "Iter #4091904:  Learning rate = 0.000977:   Batch Loss = 0.155574, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2335936725139618, Accuracy = 0.9497478604316711\n",
      "Iter #4093952:  Learning rate = 0.000977:   Batch Loss = 0.168561, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24558910727500916, Accuracy = 0.939141035079956\n",
      "Iter #4096000:  Learning rate = 0.000977:   Batch Loss = 0.154762, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23826128244400024, Accuracy = 0.9440097212791443\n",
      "Iter #4098048:  Learning rate = 0.000977:   Batch Loss = 0.146961, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2503032386302948, Accuracy = 0.9405320882797241\n",
      "Iter #4100096:  Learning rate = 0.000938:   Batch Loss = 0.141226, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23903265595436096, Accuracy = 0.9431403279304504\n",
      "Iter #4102144:  Learning rate = 0.000938:   Batch Loss = 0.149703, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2327433079481125, Accuracy = 0.9441836476325989\n",
      "Iter #4104192:  Learning rate = 0.000938:   Batch Loss = 0.187618, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24820834398269653, Accuracy = 0.942097008228302\n",
      "Iter #4106240:  Learning rate = 0.000938:   Batch Loss = 0.168137, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2406611293554306, Accuracy = 0.9434880614280701\n",
      "Iter #4108288:  Learning rate = 0.000938:   Batch Loss = 0.140863, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22705087065696716, Accuracy = 0.9473134875297546\n",
      "Iter #4110336:  Learning rate = 0.000938:   Batch Loss = 0.145701, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22635450959205627, Accuracy = 0.9459224343299866\n",
      "Iter #4112384:  Learning rate = 0.000938:   Batch Loss = 0.158459, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23461058735847473, Accuracy = 0.9426186680793762\n",
      "Iter #4114432:  Learning rate = 0.000938:   Batch Loss = 0.180802, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23871192336082458, Accuracy = 0.9438358545303345\n",
      "Iter #4116480:  Learning rate = 0.000938:   Batch Loss = 0.141266, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2506013810634613, Accuracy = 0.9436619877815247\n",
      "Iter #4118528:  Learning rate = 0.000938:   Batch Loss = 0.167458, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26593348383903503, Accuracy = 0.9367066621780396\n",
      "Iter #4120576:  Learning rate = 0.000938:   Batch Loss = 0.149659, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2389776110649109, Accuracy = 0.9440097212791443\n",
      "Iter #4122624:  Learning rate = 0.000938:   Batch Loss = 0.181951, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2243492603302002, Accuracy = 0.9490523338317871\n",
      "Iter #4124672:  Learning rate = 0.000938:   Batch Loss = 0.146569, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24531544744968414, Accuracy = 0.9454007744789124\n",
      "Iter #4126720:  Learning rate = 0.000938:   Batch Loss = 0.122912, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22994253039360046, Accuracy = 0.9474874138832092\n",
      "Iter #4128768:  Learning rate = 0.000938:   Batch Loss = 0.145642, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23139142990112305, Accuracy = 0.9450530409812927\n",
      "Iter #4130816:  Learning rate = 0.000938:   Batch Loss = 0.180254, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2516661286354065, Accuracy = 0.9389671087265015\n",
      "Iter #4132864:  Learning rate = 0.000938:   Batch Loss = 0.161056, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23734444379806519, Accuracy = 0.941401481628418\n",
      "Iter #4134912:  Learning rate = 0.000938:   Batch Loss = 0.152222, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24922212958335876, Accuracy = 0.9405320882797241\n",
      "Iter #4136960:  Learning rate = 0.000938:   Batch Loss = 0.170841, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24500572681427002, Accuracy = 0.9367066621780396\n",
      "Iter #4139008:  Learning rate = 0.000938:   Batch Loss = 0.178502, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.243552565574646, Accuracy = 0.9429664611816406\n",
      "Iter #4141056:  Learning rate = 0.000938:   Batch Loss = 0.144238, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24441173672676086, Accuracy = 0.9354894757270813\n",
      "Iter #4143104:  Learning rate = 0.000938:   Batch Loss = 0.167916, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2408328652381897, Accuracy = 0.9433141946792603\n",
      "Iter #4145152:  Learning rate = 0.000938:   Batch Loss = 0.166596, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2268417477607727, Accuracy = 0.9504433870315552\n",
      "Iter #4147200:  Learning rate = 0.000938:   Batch Loss = 0.160027, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24400478601455688, Accuracy = 0.9401842951774597\n",
      "Iter #4149248:  Learning rate = 0.000938:   Batch Loss = 0.150019, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22891685366630554, Accuracy = 0.9492262005805969\n",
      "Iter #4151296:  Learning rate = 0.000938:   Batch Loss = 0.136584, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2483915388584137, Accuracy = 0.9380977153778076\n",
      "Iter #4153344:  Learning rate = 0.000938:   Batch Loss = 0.148854, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24144867062568665, Accuracy = 0.9417492747306824\n",
      "Iter #4155392:  Learning rate = 0.000938:   Batch Loss = 0.205222, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2366521805524826, Accuracy = 0.9429664611816406\n",
      "Iter #4157440:  Learning rate = 0.000938:   Batch Loss = 0.210450, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28930234909057617, Accuracy = 0.9207094311714172\n",
      "Iter #4159488:  Learning rate = 0.000938:   Batch Loss = 0.295332, Accuracy = 0.91796875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27260851860046387, Accuracy = 0.9325334429740906\n",
      "Iter #4161536:  Learning rate = 0.000938:   Batch Loss = 0.232242, Accuracy = 0.9375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.36167198419570923, Accuracy = 0.9050599932670593\n",
      "Iter #4163584:  Learning rate = 0.000938:   Batch Loss = 0.163342, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2755967080593109, Accuracy = 0.9264475703239441\n",
      "Iter #4165632:  Learning rate = 0.000938:   Batch Loss = 0.178214, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2275916486978531, Accuracy = 0.9516605734825134\n",
      "Iter #4167680:  Learning rate = 0.000938:   Batch Loss = 0.144633, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2232634276151657, Accuracy = 0.9492262005805969\n",
      "Iter #4169728:  Learning rate = 0.000938:   Batch Loss = 0.134393, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2397216260433197, Accuracy = 0.9382715821266174\n",
      "Iter #4171776:  Learning rate = 0.000938:   Batch Loss = 0.174458, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23817405104637146, Accuracy = 0.9460963606834412\n",
      "Iter #4173824:  Learning rate = 0.000938:   Batch Loss = 0.160966, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2281286120414734, Accuracy = 0.9488784670829773\n",
      "Iter #4175872:  Learning rate = 0.000938:   Batch Loss = 0.167452, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25742632150650024, Accuracy = 0.9327073693275452\n",
      "Iter #4177920:  Learning rate = 0.000938:   Batch Loss = 0.161110, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2662609815597534, Accuracy = 0.932185709476471\n",
      "Iter #4179968:  Learning rate = 0.000938:   Batch Loss = 0.202885, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2554313540458679, Accuracy = 0.9372283220291138\n",
      "Iter #4182016:  Learning rate = 0.000938:   Batch Loss = 0.145260, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24833586812019348, Accuracy = 0.9346200823783875\n",
      "Iter #4184064:  Learning rate = 0.000938:   Batch Loss = 0.183426, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2458811104297638, Accuracy = 0.9382715821266174\n",
      "Iter #4186112:  Learning rate = 0.000938:   Batch Loss = 0.143560, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23128220438957214, Accuracy = 0.9478351473808289\n",
      "Iter #4188160:  Learning rate = 0.000938:   Batch Loss = 0.146571, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28058332204818726, Accuracy = 0.9353156089782715\n",
      "Iter #4190208:  Learning rate = 0.000938:   Batch Loss = 0.139880, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2602663040161133, Accuracy = 0.937749981880188\n",
      "Iter #4192256:  Learning rate = 0.000938:   Batch Loss = 0.191683, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23455290496349335, Accuracy = 0.9431403279304504\n",
      "Iter #4194304:  Learning rate = 0.000938:   Batch Loss = 0.153573, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.252196729183197, Accuracy = 0.9367066621780396\n",
      "Iter #4196352:  Learning rate = 0.000938:   Batch Loss = 0.157867, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23646141588687897, Accuracy = 0.9424448013305664\n",
      "Iter #4198400:  Learning rate = 0.000938:   Batch Loss = 0.170030, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25943973660469055, Accuracy = 0.9386193752288818\n",
      "Iter #4200448:  Learning rate = 0.000900:   Batch Loss = 0.166846, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22822287678718567, Accuracy = 0.9455747008323669\n",
      "Iter #4202496:  Learning rate = 0.000900:   Batch Loss = 0.198694, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23298560082912445, Accuracy = 0.9448791742324829\n",
      "Iter #4204544:  Learning rate = 0.000900:   Batch Loss = 0.186233, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22696413099765778, Accuracy = 0.947661280632019\n",
      "Iter #4206592:  Learning rate = 0.000900:   Batch Loss = 0.177003, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23459729552268982, Accuracy = 0.9422709345817566\n",
      "Iter #4208640:  Learning rate = 0.000900:   Batch Loss = 0.173827, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23083433508872986, Accuracy = 0.9436619877815247\n",
      "Iter #4210688:  Learning rate = 0.000900:   Batch Loss = 0.133575, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22907423973083496, Accuracy = 0.9438358545303345\n",
      "Iter #4212736:  Learning rate = 0.000900:   Batch Loss = 0.150488, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23174360394477844, Accuracy = 0.9483568072319031\n",
      "Iter #4214784:  Learning rate = 0.000900:   Batch Loss = 0.140363, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22865380346775055, Accuracy = 0.9492262005805969\n",
      "Iter #4216832:  Learning rate = 0.000900:   Batch Loss = 0.159830, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23938512802124023, Accuracy = 0.9422709345817566\n",
      "Iter #4218880:  Learning rate = 0.000900:   Batch Loss = 0.146316, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24154171347618103, Accuracy = 0.942097008228302\n",
      "Iter #4220928:  Learning rate = 0.000900:   Batch Loss = 0.124923, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23347264528274536, Accuracy = 0.9443575143814087\n",
      "Iter #4222976:  Learning rate = 0.000900:   Batch Loss = 0.167641, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22894223034381866, Accuracy = 0.9497478604316711\n",
      "Iter #4225024:  Learning rate = 0.000900:   Batch Loss = 0.183027, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23457753658294678, Accuracy = 0.9447052478790283\n",
      "Iter #4227072:  Learning rate = 0.000900:   Batch Loss = 0.125530, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23434966802597046, Accuracy = 0.9467918872833252\n",
      "Iter #4229120:  Learning rate = 0.000900:   Batch Loss = 0.155930, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22219467163085938, Accuracy = 0.9514867067337036\n",
      "Iter #4231168:  Learning rate = 0.000900:   Batch Loss = 0.139235, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23181399703025818, Accuracy = 0.946270227432251\n",
      "Iter #4233216:  Learning rate = 0.000900:   Batch Loss = 0.207229, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22884635627269745, Accuracy = 0.9474874138832092\n",
      "Iter #4235264:  Learning rate = 0.000900:   Batch Loss = 0.145841, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2333185374736786, Accuracy = 0.941401481628418\n",
      "Iter #4237312:  Learning rate = 0.000900:   Batch Loss = 0.168618, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23387694358825684, Accuracy = 0.9434880614280701\n",
      "Iter #4239360:  Learning rate = 0.000900:   Batch Loss = 0.193221, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24006323516368866, Accuracy = 0.9452269077301025\n",
      "Iter #4241408:  Learning rate = 0.000900:   Batch Loss = 0.207355, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23292098939418793, Accuracy = 0.9436619877815247\n",
      "Iter #4243456:  Learning rate = 0.000900:   Batch Loss = 0.127707, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26405176520347595, Accuracy = 0.9360111355781555\n",
      "Iter #4245504:  Learning rate = 0.000900:   Batch Loss = 0.131905, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22678036987781525, Accuracy = 0.9492262005805969\n",
      "Iter #4247552:  Learning rate = 0.000900:   Batch Loss = 0.194981, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22516126930713654, Accuracy = 0.9516605734825134\n",
      "Iter #4249600:  Learning rate = 0.000900:   Batch Loss = 0.203524, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2531808614730835, Accuracy = 0.9375760555267334\n",
      "Iter #4251648:  Learning rate = 0.000900:   Batch Loss = 0.149052, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2418581247329712, Accuracy = 0.9434880614280701\n",
      "Iter #4253696:  Learning rate = 0.000900:   Batch Loss = 0.149179, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.228502094745636, Accuracy = 0.9509650468826294\n",
      "Iter #4255744:  Learning rate = 0.000900:   Batch Loss = 0.126364, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22755393385887146, Accuracy = 0.9466179609298706\n",
      "Iter #4257792:  Learning rate = 0.000900:   Batch Loss = 0.140885, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24302245676517487, Accuracy = 0.9426186680793762\n",
      "Iter #4259840:  Learning rate = 0.000900:   Batch Loss = 0.126160, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23944395780563354, Accuracy = 0.9387932419776917\n",
      "Iter #4261888:  Learning rate = 0.000900:   Batch Loss = 0.150341, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23291495442390442, Accuracy = 0.9448791742324829\n",
      "Iter #4263936:  Learning rate = 0.000900:   Batch Loss = 0.178682, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23516710102558136, Accuracy = 0.9488784670829773\n",
      "Iter #4265984:  Learning rate = 0.000900:   Batch Loss = 0.119808, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23691606521606445, Accuracy = 0.9436619877815247\n",
      "Iter #4268032:  Learning rate = 0.000900:   Batch Loss = 0.140843, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22792892158031464, Accuracy = 0.9467918872833252\n",
      "Iter #4270080:  Learning rate = 0.000900:   Batch Loss = 0.145314, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21944591403007507, Accuracy = 0.9520083665847778\n",
      "Iter #4272128:  Learning rate = 0.000900:   Batch Loss = 0.150541, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2237880825996399, Accuracy = 0.9502695202827454\n",
      "Iter #4274176:  Learning rate = 0.000900:   Batch Loss = 0.138614, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21962061524391174, Accuracy = 0.9495739936828613\n",
      "Iter #4276224:  Learning rate = 0.000900:   Batch Loss = 0.148464, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2318510115146637, Accuracy = 0.9464440941810608\n",
      "Iter #4278272:  Learning rate = 0.000900:   Batch Loss = 0.130146, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24782949686050415, Accuracy = 0.9405320882797241\n",
      "Iter #4280320:  Learning rate = 0.000900:   Batch Loss = 0.148269, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24421760439872742, Accuracy = 0.9440097212791443\n",
      "Iter #4282368:  Learning rate = 0.000900:   Batch Loss = 0.148226, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23490116000175476, Accuracy = 0.9441836476325989\n",
      "Iter #4284416:  Learning rate = 0.000900:   Batch Loss = 0.118747, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26302477717399597, Accuracy = 0.9346200823783875\n",
      "Iter #4286464:  Learning rate = 0.000900:   Batch Loss = 0.161593, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24128705263137817, Accuracy = 0.9447052478790283\n",
      "Iter #4288512:  Learning rate = 0.000900:   Batch Loss = 0.188101, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2490941286087036, Accuracy = 0.9358372688293457\n",
      "Iter #4290560:  Learning rate = 0.000900:   Batch Loss = 0.207241, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23677092790603638, Accuracy = 0.946270227432251\n",
      "Iter #4292608:  Learning rate = 0.000900:   Batch Loss = 0.189952, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23059190809726715, Accuracy = 0.9454007744789124\n",
      "Iter #4294656:  Learning rate = 0.000900:   Batch Loss = 0.167651, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2475493848323822, Accuracy = 0.9370543956756592\n",
      "Iter #4296704:  Learning rate = 0.000900:   Batch Loss = 0.187657, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2744652032852173, Accuracy = 0.9307946562767029\n",
      "Iter #4298752:  Learning rate = 0.000900:   Batch Loss = 0.151768, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2451651394367218, Accuracy = 0.9412276148796082\n",
      "Iter #4300800:  Learning rate = 0.000864:   Batch Loss = 0.200063, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2272091805934906, Accuracy = 0.9464440941810608\n",
      "Iter #4302848:  Learning rate = 0.000864:   Batch Loss = 0.179235, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2493387758731842, Accuracy = 0.9389671087265015\n",
      "Iter #4304896:  Learning rate = 0.000864:   Batch Loss = 0.181430, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26075270771980286, Accuracy = 0.9353156089782715\n",
      "Iter #4306944:  Learning rate = 0.000864:   Batch Loss = 0.152903, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25082629919052124, Accuracy = 0.9407059550285339\n",
      "Iter #4308992:  Learning rate = 0.000864:   Batch Loss = 0.222844, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2281007468700409, Accuracy = 0.9452269077301025\n",
      "Iter #4311040:  Learning rate = 0.000864:   Batch Loss = 0.236211, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25293469429016113, Accuracy = 0.9382715821266174\n",
      "Iter #4313088:  Learning rate = 0.000864:   Batch Loss = 0.141002, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2479049265384674, Accuracy = 0.9431403279304504\n",
      "Iter #4315136:  Learning rate = 0.000864:   Batch Loss = 0.179131, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25409841537475586, Accuracy = 0.9443575143814087\n",
      "Iter #4317184:  Learning rate = 0.000864:   Batch Loss = 0.144204, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25229692459106445, Accuracy = 0.938445508480072\n",
      "Iter #4319232:  Learning rate = 0.000864:   Batch Loss = 0.175616, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22286567091941833, Accuracy = 0.9490523338317871\n",
      "Iter #4321280:  Learning rate = 0.000864:   Batch Loss = 0.160119, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23013971745967865, Accuracy = 0.947661280632019\n",
      "Iter #4323328:  Learning rate = 0.000864:   Batch Loss = 0.152766, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2371234893798828, Accuracy = 0.9434880614280701\n",
      "Iter #4325376:  Learning rate = 0.000864:   Batch Loss = 0.134611, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22155722975730896, Accuracy = 0.9467918872833252\n",
      "Iter #4327424:  Learning rate = 0.000864:   Batch Loss = 0.157691, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2288275510072708, Accuracy = 0.9490523338317871\n",
      "Iter #4329472:  Learning rate = 0.000864:   Batch Loss = 0.126746, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2325703203678131, Accuracy = 0.9448791742324829\n",
      "Iter #4331520:  Learning rate = 0.000864:   Batch Loss = 0.145801, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22658509016036987, Accuracy = 0.9488784670829773\n",
      "Iter #4333568:  Learning rate = 0.000864:   Batch Loss = 0.169068, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2277686893939972, Accuracy = 0.9478351473808289\n",
      "Iter #4335616:  Learning rate = 0.000864:   Batch Loss = 0.204691, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25669437646865845, Accuracy = 0.9353156089782715\n",
      "Iter #4337664:  Learning rate = 0.000864:   Batch Loss = 0.124399, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2193666398525238, Accuracy = 0.949921727180481\n",
      "Iter #4339712:  Learning rate = 0.000864:   Batch Loss = 0.152746, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23592525720596313, Accuracy = 0.9417492747306824\n",
      "Iter #4341760:  Learning rate = 0.000864:   Batch Loss = 0.175356, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.33575254678726196, Accuracy = 0.9123630523681641\n",
      "Iter #4343808:  Learning rate = 0.000864:   Batch Loss = 0.179109, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23827941715717316, Accuracy = 0.9422709345817566\n",
      "Iter #4345856:  Learning rate = 0.000864:   Batch Loss = 0.130757, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22527746856212616, Accuracy = 0.9514867067337036\n",
      "Iter #4347904:  Learning rate = 0.000864:   Batch Loss = 0.192994, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23580622673034668, Accuracy = 0.9467918872833252\n",
      "Iter #4349952:  Learning rate = 0.000864:   Batch Loss = 0.167553, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25333264470100403, Accuracy = 0.941401481628418\n",
      "Iter #4352000:  Learning rate = 0.000864:   Batch Loss = 0.178716, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2359834760427475, Accuracy = 0.9466179609298706\n",
      "Iter #4354048:  Learning rate = 0.000864:   Batch Loss = 0.168062, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23954349756240845, Accuracy = 0.9407059550285339\n",
      "Iter #4356096:  Learning rate = 0.000864:   Batch Loss = 0.206186, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24775445461273193, Accuracy = 0.9394887685775757\n",
      "Iter #4358144:  Learning rate = 0.000864:   Batch Loss = 0.129738, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22260504961013794, Accuracy = 0.9513128399848938\n",
      "Iter #4360192:  Learning rate = 0.000864:   Batch Loss = 0.154707, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23180654644966125, Accuracy = 0.9464440941810608\n",
      "Iter #4362240:  Learning rate = 0.000864:   Batch Loss = 0.193821, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22796499729156494, Accuracy = 0.9481829404830933\n",
      "Iter #4364288:  Learning rate = 0.000864:   Batch Loss = 0.140231, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23167115449905396, Accuracy = 0.9452269077301025\n",
      "Iter #4366336:  Learning rate = 0.000864:   Batch Loss = 0.154092, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24411159753799438, Accuracy = 0.9410537481307983\n",
      "Iter #4368384:  Learning rate = 0.000864:   Batch Loss = 0.154549, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22745686769485474, Accuracy = 0.9487046003341675\n",
      "Iter #4370432:  Learning rate = 0.000864:   Batch Loss = 0.198513, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26276594400405884, Accuracy = 0.9368805289268494\n",
      "Iter #4372480:  Learning rate = 0.000864:   Batch Loss = 0.132053, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22522032260894775, Accuracy = 0.9478351473808289\n",
      "Iter #4374528:  Learning rate = 0.000864:   Batch Loss = 0.174863, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23629945516586304, Accuracy = 0.9434880614280701\n",
      "Iter #4376576:  Learning rate = 0.000864:   Batch Loss = 0.149471, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22531774640083313, Accuracy = 0.9514867067337036\n",
      "Iter #4378624:  Learning rate = 0.000864:   Batch Loss = 0.129582, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22908535599708557, Accuracy = 0.9518344402313232\n",
      "Iter #4380672:  Learning rate = 0.000864:   Batch Loss = 0.142071, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23679706454277039, Accuracy = 0.9488784670829773\n",
      "Iter #4382720:  Learning rate = 0.000864:   Batch Loss = 0.132552, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24427402019500732, Accuracy = 0.9450530409812927\n",
      "Iter #4384768:  Learning rate = 0.000864:   Batch Loss = 0.153648, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2258802056312561, Accuracy = 0.9464440941810608\n",
      "Iter #4386816:  Learning rate = 0.000864:   Batch Loss = 0.130538, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23275373876094818, Accuracy = 0.9426186680793762\n",
      "Iter #4388864:  Learning rate = 0.000864:   Batch Loss = 0.207115, Accuracy = 0.94140625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24111075699329376, Accuracy = 0.9426186680793762\n",
      "Iter #4390912:  Learning rate = 0.000864:   Batch Loss = 0.144393, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23456063866615295, Accuracy = 0.9454007744789124\n",
      "Iter #4392960:  Learning rate = 0.000864:   Batch Loss = 0.168672, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2946765124797821, Accuracy = 0.9254042506217957\n",
      "Iter #4395008:  Learning rate = 0.000864:   Batch Loss = 0.138875, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2547012269496918, Accuracy = 0.9375760555267334\n",
      "Iter #4397056:  Learning rate = 0.000864:   Batch Loss = 0.180752, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2642817795276642, Accuracy = 0.9294036030769348\n",
      "Iter #4399104:  Learning rate = 0.000864:   Batch Loss = 0.159529, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23168402910232544, Accuracy = 0.9454007744789124\n",
      "Iter #4401152:  Learning rate = 0.000830:   Batch Loss = 0.151785, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22235539555549622, Accuracy = 0.9492262005805969\n",
      "Iter #4403200:  Learning rate = 0.000830:   Batch Loss = 0.135729, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2276371717453003, Accuracy = 0.946965754032135\n",
      "Iter #4405248:  Learning rate = 0.000830:   Batch Loss = 0.119538, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22603407502174377, Accuracy = 0.9485306739807129\n",
      "Iter #4407296:  Learning rate = 0.000830:   Batch Loss = 0.203172, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2601860761642456, Accuracy = 0.9379238486289978\n",
      "Iter #4409344:  Learning rate = 0.000830:   Batch Loss = 0.147629, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23097841441631317, Accuracy = 0.9457485675811768\n",
      "Iter #4411392:  Learning rate = 0.000830:   Batch Loss = 0.179651, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24723392724990845, Accuracy = 0.9389671087265015\n",
      "Iter #4413440:  Learning rate = 0.000830:   Batch Loss = 0.152017, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22776231169700623, Accuracy = 0.9502695202827454\n",
      "Iter #4415488:  Learning rate = 0.000830:   Batch Loss = 0.165901, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22628459334373474, Accuracy = 0.9478351473808289\n",
      "Iter #4417536:  Learning rate = 0.000830:   Batch Loss = 0.130261, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24327656626701355, Accuracy = 0.9393149018287659\n",
      "Iter #4419584:  Learning rate = 0.000830:   Batch Loss = 0.114672, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24260644614696503, Accuracy = 0.9447052478790283\n",
      "Iter #4421632:  Learning rate = 0.000830:   Batch Loss = 0.106967, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23827296495437622, Accuracy = 0.9447052478790283\n",
      "Iter #4423680:  Learning rate = 0.000830:   Batch Loss = 0.160106, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2234647572040558, Accuracy = 0.9483568072319031\n",
      "Iter #4425728:  Learning rate = 0.000830:   Batch Loss = 0.155618, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22913973033428192, Accuracy = 0.9492262005805969\n",
      "Iter #4427776:  Learning rate = 0.000830:   Batch Loss = 0.128368, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22740082442760468, Accuracy = 0.9483568072319031\n",
      "Iter #4429824:  Learning rate = 0.000830:   Batch Loss = 0.138698, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23135250806808472, Accuracy = 0.9450530409812927\n",
      "Iter #4431872:  Learning rate = 0.000830:   Batch Loss = 0.146570, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2103562355041504, Accuracy = 0.9532255530357361\n",
      "Iter #4433920:  Learning rate = 0.000830:   Batch Loss = 0.144278, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22921890020370483, Accuracy = 0.9480090141296387\n",
      "Iter #4435968:  Learning rate = 0.000830:   Batch Loss = 0.152993, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23976415395736694, Accuracy = 0.9455747008323669\n",
      "Iter #4438016:  Learning rate = 0.000830:   Batch Loss = 0.139826, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23380878567695618, Accuracy = 0.9443575143814087\n",
      "Iter #4440064:  Learning rate = 0.000830:   Batch Loss = 0.143891, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22279009222984314, Accuracy = 0.9480090141296387\n",
      "Iter #4442112:  Learning rate = 0.000830:   Batch Loss = 0.168747, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23379766941070557, Accuracy = 0.9447052478790283\n",
      "Iter #4444160:  Learning rate = 0.000830:   Batch Loss = 0.141977, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24078631401062012, Accuracy = 0.9455747008323669\n",
      "Iter #4446208:  Learning rate = 0.000830:   Batch Loss = 0.155328, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22665570676326752, Accuracy = 0.9509650468826294\n",
      "Iter #4448256:  Learning rate = 0.000830:   Batch Loss = 0.131079, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2495119869709015, Accuracy = 0.9429664611816406\n",
      "Iter #4450304:  Learning rate = 0.000830:   Batch Loss = 0.147780, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22781997919082642, Accuracy = 0.9527038931846619\n",
      "Iter #4452352:  Learning rate = 0.000830:   Batch Loss = 0.119280, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25932490825653076, Accuracy = 0.9400104284286499\n",
      "Iter #4454400:  Learning rate = 0.000830:   Batch Loss = 0.144567, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23026823997497559, Accuracy = 0.9459224343299866\n",
      "Iter #4456448:  Learning rate = 0.000830:   Batch Loss = 0.164438, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25391241908073425, Accuracy = 0.9313163161277771\n",
      "Iter #4458496:  Learning rate = 0.000830:   Batch Loss = 0.158901, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22210882604122162, Accuracy = 0.949921727180481\n",
      "Iter #4460544:  Learning rate = 0.000830:   Batch Loss = 0.131251, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22924268245697021, Accuracy = 0.9447052478790283\n",
      "Iter #4462592:  Learning rate = 0.000830:   Batch Loss = 0.174676, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21890684962272644, Accuracy = 0.9490523338317871\n",
      "Iter #4464640:  Learning rate = 0.000830:   Batch Loss = 0.117605, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2239362895488739, Accuracy = 0.9527038931846619\n",
      "Iter #4466688:  Learning rate = 0.000830:   Batch Loss = 0.134713, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2553454637527466, Accuracy = 0.9382715821266174\n",
      "Iter #4468736:  Learning rate = 0.000830:   Batch Loss = 0.164246, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25025737285614014, Accuracy = 0.9436619877815247\n",
      "Iter #4470784:  Learning rate = 0.000830:   Batch Loss = 0.178776, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23565834760665894, Accuracy = 0.9488784670829773\n",
      "Iter #4472832:  Learning rate = 0.000830:   Batch Loss = 0.126280, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24244987964630127, Accuracy = 0.937749981880188\n",
      "Iter #4474880:  Learning rate = 0.000830:   Batch Loss = 0.152978, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2555691599845886, Accuracy = 0.9351417422294617\n",
      "Iter #4476928:  Learning rate = 0.000830:   Batch Loss = 0.154638, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24195289611816406, Accuracy = 0.9393149018287659\n",
      "Iter #4478976:  Learning rate = 0.000830:   Batch Loss = 0.155831, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.28856393694877625, Accuracy = 0.9276647567749023\n",
      "Iter #4481024:  Learning rate = 0.000830:   Batch Loss = 0.166062, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2483488768339157, Accuracy = 0.942792534828186\n",
      "Iter #4483072:  Learning rate = 0.000830:   Batch Loss = 0.182230, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23401683568954468, Accuracy = 0.942097008228302\n",
      "Iter #4485120:  Learning rate = 0.000830:   Batch Loss = 0.161395, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22289887070655823, Accuracy = 0.9485306739807129\n",
      "Iter #4487168:  Learning rate = 0.000830:   Batch Loss = 0.121551, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22132064402103424, Accuracy = 0.9506173133850098\n",
      "Iter #4489216:  Learning rate = 0.000830:   Batch Loss = 0.123672, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23184596002101898, Accuracy = 0.9497478604316711\n",
      "Iter #4491264:  Learning rate = 0.000830:   Batch Loss = 0.139449, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22842338681221008, Accuracy = 0.9521822333335876\n",
      "Iter #4493312:  Learning rate = 0.000830:   Batch Loss = 0.146709, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23948365449905396, Accuracy = 0.9445313811302185\n",
      "Iter #4495360:  Learning rate = 0.000830:   Batch Loss = 0.155567, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24291446805000305, Accuracy = 0.9459224343299866\n",
      "Iter #4497408:  Learning rate = 0.000830:   Batch Loss = 0.157735, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2298620641231537, Accuracy = 0.9500956535339355\n",
      "Iter #4499456:  Learning rate = 0.000830:   Batch Loss = 0.164707, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2250557839870453, Accuracy = 0.9488784670829773\n",
      "Iter #4501504:  Learning rate = 0.000796:   Batch Loss = 0.174584, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2556939721107483, Accuracy = 0.9382715821266174\n",
      "Iter #4503552:  Learning rate = 0.000796:   Batch Loss = 0.170240, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24727115035057068, Accuracy = 0.942097008228302\n",
      "Iter #4505600:  Learning rate = 0.000796:   Batch Loss = 0.124440, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21737681329250336, Accuracy = 0.9509650468826294\n",
      "Iter #4507648:  Learning rate = 0.000796:   Batch Loss = 0.127570, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21861594915390015, Accuracy = 0.9509650468826294\n",
      "Iter #4509696:  Learning rate = 0.000796:   Batch Loss = 0.155047, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2214154601097107, Accuracy = 0.9474874138832092\n",
      "Iter #4511744:  Learning rate = 0.000796:   Batch Loss = 0.138468, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2340240180492401, Accuracy = 0.946270227432251\n",
      "Iter #4513792:  Learning rate = 0.000796:   Batch Loss = 0.158914, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2160695195198059, Accuracy = 0.9558337926864624\n",
      "Iter #4515840:  Learning rate = 0.000796:   Batch Loss = 0.120032, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22184592485427856, Accuracy = 0.9506173133850098\n",
      "Iter #4517888:  Learning rate = 0.000796:   Batch Loss = 0.132299, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21816861629486084, Accuracy = 0.9525299668312073\n",
      "Iter #4519936:  Learning rate = 0.000796:   Batch Loss = 0.161237, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22961843013763428, Accuracy = 0.9500956535339355\n",
      "Iter #4521984:  Learning rate = 0.000796:   Batch Loss = 0.141242, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22396615147590637, Accuracy = 0.949921727180481\n",
      "Iter #4524032:  Learning rate = 0.000796:   Batch Loss = 0.165186, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25781890749931335, Accuracy = 0.9306207895278931\n",
      "Iter #4526080:  Learning rate = 0.000796:   Batch Loss = 0.136432, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2231799066066742, Accuracy = 0.9490523338317871\n",
      "Iter #4528128:  Learning rate = 0.000796:   Batch Loss = 0.139653, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22900688648223877, Accuracy = 0.9504433870315552\n",
      "Iter #4530176:  Learning rate = 0.000796:   Batch Loss = 0.129618, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2297579050064087, Accuracy = 0.9460963606834412\n",
      "Iter #4532224:  Learning rate = 0.000796:   Batch Loss = 0.142053, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24365586042404175, Accuracy = 0.9398365616798401\n",
      "Iter #4534272:  Learning rate = 0.000796:   Batch Loss = 0.143110, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2170439064502716, Accuracy = 0.9514867067337036\n",
      "Iter #4536320:  Learning rate = 0.000796:   Batch Loss = 0.161393, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22952161729335785, Accuracy = 0.9511389136314392\n",
      "Iter #4538368:  Learning rate = 0.000796:   Batch Loss = 0.125515, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22300809621810913, Accuracy = 0.9549643397331238\n",
      "Iter #4540416:  Learning rate = 0.000796:   Batch Loss = 0.167494, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25033822655677795, Accuracy = 0.9419231414794922\n",
      "Iter #4542464:  Learning rate = 0.000796:   Batch Loss = 0.177049, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2454572319984436, Accuracy = 0.9415754079818726\n",
      "Iter #4544512:  Learning rate = 0.000796:   Batch Loss = 0.164516, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23787501454353333, Accuracy = 0.9441836476325989\n",
      "Iter #4546560:  Learning rate = 0.000796:   Batch Loss = 0.136913, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23590928316116333, Accuracy = 0.9419231414794922\n",
      "Iter #4548608:  Learning rate = 0.000796:   Batch Loss = 0.128053, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2320900559425354, Accuracy = 0.9471396207809448\n",
      "Iter #4550656:  Learning rate = 0.000796:   Batch Loss = 0.175798, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22072651982307434, Accuracy = 0.9521822333335876\n",
      "Iter #4552704:  Learning rate = 0.000796:   Batch Loss = 0.122453, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22746247053146362, Accuracy = 0.9502695202827454\n",
      "Iter #4554752:  Learning rate = 0.000796:   Batch Loss = 0.135011, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23976236581802368, Accuracy = 0.9497478604316711\n",
      "Iter #4556800:  Learning rate = 0.000796:   Batch Loss = 0.126468, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22476796805858612, Accuracy = 0.949921727180481\n",
      "Iter #4558848:  Learning rate = 0.000796:   Batch Loss = 0.163602, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22760039567947388, Accuracy = 0.9448791742324829\n",
      "Iter #4560896:  Learning rate = 0.000796:   Batch Loss = 0.125123, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22450092434883118, Accuracy = 0.9495739936828613\n",
      "Iter #4562944:  Learning rate = 0.000796:   Batch Loss = 0.177206, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21962425112724304, Accuracy = 0.9507911801338196\n",
      "Iter #4564992:  Learning rate = 0.000796:   Batch Loss = 0.178672, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22098752856254578, Accuracy = 0.9527038931846619\n",
      "Iter #4567040:  Learning rate = 0.000796:   Batch Loss = 0.164791, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23261527717113495, Accuracy = 0.9474874138832092\n",
      "Iter #4569088:  Learning rate = 0.000796:   Batch Loss = 0.160816, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24668526649475098, Accuracy = 0.9403582215309143\n",
      "Iter #4571136:  Learning rate = 0.000796:   Batch Loss = 0.135762, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22449550032615662, Accuracy = 0.9474874138832092\n",
      "Iter #4573184:  Learning rate = 0.000796:   Batch Loss = 0.118874, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23451781272888184, Accuracy = 0.9436619877815247\n",
      "Iter #4575232:  Learning rate = 0.000796:   Batch Loss = 0.151023, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22815746068954468, Accuracy = 0.9434880614280701\n",
      "Iter #4577280:  Learning rate = 0.000796:   Batch Loss = 0.123215, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21428346633911133, Accuracy = 0.9511389136314392\n",
      "Iter #4579328:  Learning rate = 0.000796:   Batch Loss = 0.157112, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21394070982933044, Accuracy = 0.955485999584198\n",
      "Iter #4581376:  Learning rate = 0.000796:   Batch Loss = 0.164652, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2340843379497528, Accuracy = 0.9467918872833252\n",
      "Iter #4583424:  Learning rate = 0.000796:   Batch Loss = 0.155964, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23864340782165527, Accuracy = 0.9434880614280701\n",
      "Iter #4585472:  Learning rate = 0.000796:   Batch Loss = 0.128741, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21707046031951904, Accuracy = 0.9507911801338196\n",
      "Iter #4587520:  Learning rate = 0.000796:   Batch Loss = 0.130089, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.235477477312088, Accuracy = 0.9422709345817566\n",
      "Iter #4589568:  Learning rate = 0.000796:   Batch Loss = 0.139582, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2240808606147766, Accuracy = 0.9495739936828613\n",
      "Iter #4591616:  Learning rate = 0.000796:   Batch Loss = 0.154887, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.218075692653656, Accuracy = 0.9537471532821655\n",
      "Iter #4593664:  Learning rate = 0.000796:   Batch Loss = 0.145762, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21327105164527893, Accuracy = 0.9567031860351562\n",
      "Iter #4595712:  Learning rate = 0.000796:   Batch Loss = 0.133850, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2274284064769745, Accuracy = 0.9459224343299866\n",
      "Iter #4597760:  Learning rate = 0.000796:   Batch Loss = 0.171171, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22752761840820312, Accuracy = 0.946965754032135\n",
      "Iter #4599808:  Learning rate = 0.000796:   Batch Loss = 0.139594, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25328317284584045, Accuracy = 0.9374021887779236\n",
      "Iter #4601856:  Learning rate = 0.000765:   Batch Loss = 0.152049, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2350165843963623, Accuracy = 0.946965754032135\n",
      "Iter #4603904:  Learning rate = 0.000765:   Batch Loss = 0.133041, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22343552112579346, Accuracy = 0.9509650468826294\n",
      "Iter #4605952:  Learning rate = 0.000765:   Batch Loss = 0.140452, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2166566699743271, Accuracy = 0.9528777599334717\n",
      "Iter #4608000:  Learning rate = 0.000765:   Batch Loss = 0.147986, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22386348247528076, Accuracy = 0.9504433870315552\n",
      "Iter #4610048:  Learning rate = 0.000765:   Batch Loss = 0.177457, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22598567605018616, Accuracy = 0.9495739936828613\n",
      "Iter #4612096:  Learning rate = 0.000765:   Batch Loss = 0.154084, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2266230583190918, Accuracy = 0.9487046003341675\n",
      "Iter #4614144:  Learning rate = 0.000765:   Batch Loss = 0.156846, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2229565978050232, Accuracy = 0.9516605734825134\n",
      "Iter #4616192:  Learning rate = 0.000765:   Batch Loss = 0.127604, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23012828826904297, Accuracy = 0.9452269077301025\n",
      "Iter #4618240:  Learning rate = 0.000765:   Batch Loss = 0.147836, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23654595017433167, Accuracy = 0.947661280632019\n",
      "Iter #4620288:  Learning rate = 0.000765:   Batch Loss = 0.125600, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22693970799446106, Accuracy = 0.9504433870315552\n",
      "Iter #4622336:  Learning rate = 0.000765:   Batch Loss = 0.110871, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22124657034873962, Accuracy = 0.9492262005805969\n",
      "Iter #4624384:  Learning rate = 0.000765:   Batch Loss = 0.144081, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22014440596103668, Accuracy = 0.9546166062355042\n",
      "Iter #4626432:  Learning rate = 0.000765:   Batch Loss = 0.123002, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22335758805274963, Accuracy = 0.9514867067337036\n",
      "Iter #4628480:  Learning rate = 0.000765:   Batch Loss = 0.128286, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22546356916427612, Accuracy = 0.9490523338317871\n",
      "Iter #4630528:  Learning rate = 0.000765:   Batch Loss = 0.146967, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24071305990219116, Accuracy = 0.9400104284286499\n",
      "Iter #4632576:  Learning rate = 0.000765:   Batch Loss = 0.180114, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23425595462322235, Accuracy = 0.9433141946792603\n",
      "Iter #4634624:  Learning rate = 0.000765:   Batch Loss = 0.140818, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.223329558968544, Accuracy = 0.9525299668312073\n",
      "Iter #4636672:  Learning rate = 0.000765:   Batch Loss = 0.135841, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23358632624149323, Accuracy = 0.9410537481307983\n",
      "Iter #4638720:  Learning rate = 0.000765:   Batch Loss = 0.133079, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22672387957572937, Accuracy = 0.9474874138832092\n",
      "Iter #4640768:  Learning rate = 0.000765:   Batch Loss = 0.145111, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23218271136283875, Accuracy = 0.9448791742324829\n",
      "Iter #4642816:  Learning rate = 0.000765:   Batch Loss = 0.125303, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22732055187225342, Accuracy = 0.9459224343299866\n",
      "Iter #4644864:  Learning rate = 0.000765:   Batch Loss = 0.140835, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23530703783035278, Accuracy = 0.946965754032135\n",
      "Iter #4646912:  Learning rate = 0.000765:   Batch Loss = 0.141458, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.233016237616539, Accuracy = 0.9495739936828613\n",
      "Iter #4648960:  Learning rate = 0.000765:   Batch Loss = 0.122704, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22973421216011047, Accuracy = 0.9450530409812927\n",
      "Iter #4651008:  Learning rate = 0.000765:   Batch Loss = 0.136765, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23302139341831207, Accuracy = 0.9454007744789124\n",
      "Iter #4653056:  Learning rate = 0.000765:   Batch Loss = 0.154938, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22268933057785034, Accuracy = 0.9474874138832092\n",
      "Iter #4655104:  Learning rate = 0.000765:   Batch Loss = 0.186324, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23461368680000305, Accuracy = 0.9429664611816406\n",
      "Iter #4657152:  Learning rate = 0.000765:   Batch Loss = 0.148493, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22001875936985016, Accuracy = 0.9514867067337036\n",
      "Iter #4659200:  Learning rate = 0.000765:   Batch Loss = 0.158741, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22659839689731598, Accuracy = 0.9466179609298706\n",
      "Iter #4661248:  Learning rate = 0.000765:   Batch Loss = 0.149914, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2211400270462036, Accuracy = 0.9474874138832092\n",
      "Iter #4663296:  Learning rate = 0.000765:   Batch Loss = 0.127848, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2320011407136917, Accuracy = 0.9500956535339355\n",
      "Iter #4665344:  Learning rate = 0.000765:   Batch Loss = 0.136442, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23252862691879272, Accuracy = 0.9434880614280701\n",
      "Iter #4667392:  Learning rate = 0.000765:   Batch Loss = 0.139388, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22447150945663452, Accuracy = 0.9483568072319031\n",
      "Iter #4669440:  Learning rate = 0.000765:   Batch Loss = 0.113345, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2171320617198944, Accuracy = 0.9513128399848938\n",
      "Iter #4671488:  Learning rate = 0.000765:   Batch Loss = 0.124697, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22307062149047852, Accuracy = 0.9523561000823975\n",
      "Iter #4673536:  Learning rate = 0.000765:   Batch Loss = 0.176862, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24107250571250916, Accuracy = 0.9396626949310303\n",
      "Iter #4675584:  Learning rate = 0.000765:   Batch Loss = 0.140605, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24515897035598755, Accuracy = 0.9387932419776917\n",
      "Iter #4677632:  Learning rate = 0.000765:   Batch Loss = 0.138641, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2506857216358185, Accuracy = 0.942792534828186\n",
      "Iter #4679680:  Learning rate = 0.000765:   Batch Loss = 0.146309, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23354633152484894, Accuracy = 0.9485306739807129\n",
      "Iter #4681728:  Learning rate = 0.000765:   Batch Loss = 0.141313, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22903212904930115, Accuracy = 0.9471396207809448\n",
      "Iter #4683776:  Learning rate = 0.000765:   Batch Loss = 0.133606, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22782644629478455, Accuracy = 0.9481829404830933\n",
      "Iter #4685824:  Learning rate = 0.000765:   Batch Loss = 0.126144, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21955686807632446, Accuracy = 0.9500956535339355\n",
      "Iter #4687872:  Learning rate = 0.000765:   Batch Loss = 0.145535, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24074780941009521, Accuracy = 0.9424448013305664\n",
      "Iter #4689920:  Learning rate = 0.000765:   Batch Loss = 0.126377, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22480875253677368, Accuracy = 0.9467918872833252\n",
      "Iter #4691968:  Learning rate = 0.000765:   Batch Loss = 0.128950, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20868231356143951, Accuracy = 0.9575725793838501\n",
      "Iter #4694016:  Learning rate = 0.000765:   Batch Loss = 0.144799, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24234232306480408, Accuracy = 0.9450530409812927\n",
      "Iter #4696064:  Learning rate = 0.000765:   Batch Loss = 0.140287, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2232457995414734, Accuracy = 0.9490523338317871\n",
      "Iter #4698112:  Learning rate = 0.000765:   Batch Loss = 0.139877, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23647058010101318, Accuracy = 0.9431403279304504\n",
      "Iter #4700160:  Learning rate = 0.000734:   Batch Loss = 0.112033, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2298564314842224, Accuracy = 0.9481829404830933\n",
      "Iter #4702208:  Learning rate = 0.000734:   Batch Loss = 0.119500, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23609650135040283, Accuracy = 0.9448791742324829\n",
      "Iter #4704256:  Learning rate = 0.000734:   Batch Loss = 0.155837, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21698608994483948, Accuracy = 0.9521822333335876\n",
      "Iter #4706304:  Learning rate = 0.000734:   Batch Loss = 0.140613, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2459566742181778, Accuracy = 0.9403582215309143\n",
      "Iter #4708352:  Learning rate = 0.000734:   Batch Loss = 0.135003, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23901289701461792, Accuracy = 0.9422709345817566\n",
      "Iter #4710400:  Learning rate = 0.000734:   Batch Loss = 0.145668, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22863635420799255, Accuracy = 0.9509650468826294\n",
      "Iter #4712448:  Learning rate = 0.000734:   Batch Loss = 0.133141, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23235002160072327, Accuracy = 0.9443575143814087\n",
      "Iter #4714496:  Learning rate = 0.000734:   Batch Loss = 0.136706, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2406446635723114, Accuracy = 0.9452269077301025\n",
      "Iter #4716544:  Learning rate = 0.000734:   Batch Loss = 0.127392, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21087002754211426, Accuracy = 0.9540949463844299\n",
      "Iter #4718592:  Learning rate = 0.000734:   Batch Loss = 0.181702, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2206771820783615, Accuracy = 0.9532255530357361\n",
      "Iter #4720640:  Learning rate = 0.000734:   Batch Loss = 0.125899, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21904131770133972, Accuracy = 0.9525299668312073\n",
      "Iter #4722688:  Learning rate = 0.000734:   Batch Loss = 0.119596, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23620906472206116, Accuracy = 0.946965754032135\n",
      "Iter #4724736:  Learning rate = 0.000734:   Batch Loss = 0.136164, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22830475866794586, Accuracy = 0.9474874138832092\n",
      "Iter #4726784:  Learning rate = 0.000734:   Batch Loss = 0.128889, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2328324019908905, Accuracy = 0.9490523338317871\n",
      "Iter #4728832:  Learning rate = 0.000734:   Batch Loss = 0.130990, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24226057529449463, Accuracy = 0.9405320882797241\n",
      "Iter #4730880:  Learning rate = 0.000734:   Batch Loss = 0.204911, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22910207509994507, Accuracy = 0.9464440941810608\n",
      "Iter #4732928:  Learning rate = 0.000734:   Batch Loss = 0.126602, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2326025664806366, Accuracy = 0.9474874138832092\n",
      "Iter #4734976:  Learning rate = 0.000734:   Batch Loss = 0.138851, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22730599343776703, Accuracy = 0.9497478604316711\n",
      "Iter #4737024:  Learning rate = 0.000734:   Batch Loss = 0.114293, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21704763174057007, Accuracy = 0.9549643397331238\n",
      "Iter #4739072:  Learning rate = 0.000734:   Batch Loss = 0.111926, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22460991144180298, Accuracy = 0.9488784670829773\n",
      "Iter #4741120:  Learning rate = 0.000734:   Batch Loss = 0.128246, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2256813794374466, Accuracy = 0.9490523338317871\n",
      "Iter #4743168:  Learning rate = 0.000734:   Batch Loss = 0.133707, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23949691653251648, Accuracy = 0.9441836476325989\n",
      "Iter #4745216:  Learning rate = 0.000734:   Batch Loss = 0.129940, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23051989078521729, Accuracy = 0.946965754032135\n",
      "Iter #4747264:  Learning rate = 0.000734:   Batch Loss = 0.135468, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21782369911670685, Accuracy = 0.9497478604316711\n",
      "Iter #4749312:  Learning rate = 0.000734:   Batch Loss = 0.139844, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21628235280513763, Accuracy = 0.9521822333335876\n",
      "Iter #4751360:  Learning rate = 0.000734:   Batch Loss = 0.162936, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24918994307518005, Accuracy = 0.9405320882797241\n",
      "Iter #4753408:  Learning rate = 0.000734:   Batch Loss = 0.136066, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24659523367881775, Accuracy = 0.942097008228302\n",
      "Iter #4755456:  Learning rate = 0.000734:   Batch Loss = 0.147946, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.219719797372818, Accuracy = 0.9511389136314392\n",
      "Iter #4757504:  Learning rate = 0.000734:   Batch Loss = 0.137864, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2198724001646042, Accuracy = 0.9525299668312073\n",
      "Iter #4759552:  Learning rate = 0.000734:   Batch Loss = 0.163251, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2292916178703308, Accuracy = 0.946270227432251\n",
      "Iter #4761600:  Learning rate = 0.000734:   Batch Loss = 0.134255, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24525614082813263, Accuracy = 0.9440097212791443\n",
      "Iter #4763648:  Learning rate = 0.000734:   Batch Loss = 0.169009, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24253419041633606, Accuracy = 0.9436619877815247\n",
      "Iter #4765696:  Learning rate = 0.000734:   Batch Loss = 0.103891, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2566133141517639, Accuracy = 0.941401481628418\n",
      "Iter #4767744:  Learning rate = 0.000734:   Batch Loss = 0.146523, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2568243741989136, Accuracy = 0.9400104284286499\n",
      "Iter #4769792:  Learning rate = 0.000734:   Batch Loss = 0.152754, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2291642725467682, Accuracy = 0.9483568072319031\n",
      "Iter #4771840:  Learning rate = 0.000734:   Batch Loss = 0.145657, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23925556242465973, Accuracy = 0.9455747008323669\n",
      "Iter #4773888:  Learning rate = 0.000734:   Batch Loss = 0.139206, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2239772081375122, Accuracy = 0.9502695202827454\n",
      "Iter #4775936:  Learning rate = 0.000734:   Batch Loss = 0.130576, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2365417778491974, Accuracy = 0.9450530409812927\n",
      "Iter #4777984:  Learning rate = 0.000734:   Batch Loss = 0.122445, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2239881455898285, Accuracy = 0.9480090141296387\n",
      "Iter #4780032:  Learning rate = 0.000734:   Batch Loss = 0.147301, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2570764124393463, Accuracy = 0.9393149018287659\n",
      "Iter #4782080:  Learning rate = 0.000734:   Batch Loss = 0.135445, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23810605704784393, Accuracy = 0.9454007744789124\n",
      "Iter #4784128:  Learning rate = 0.000734:   Batch Loss = 0.139240, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2366364598274231, Accuracy = 0.9398365616798401\n",
      "Iter #4786176:  Learning rate = 0.000734:   Batch Loss = 0.126804, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23104000091552734, Accuracy = 0.949921727180481\n",
      "Iter #4788224:  Learning rate = 0.000734:   Batch Loss = 0.132145, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24014374613761902, Accuracy = 0.9415754079818726\n",
      "Iter #4790272:  Learning rate = 0.000734:   Batch Loss = 0.150800, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24417799711227417, Accuracy = 0.942792534828186\n",
      "Iter #4792320:  Learning rate = 0.000734:   Batch Loss = 0.170971, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23079444468021393, Accuracy = 0.947661280632019\n",
      "Iter #4794368:  Learning rate = 0.000734:   Batch Loss = 0.145553, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23393744230270386, Accuracy = 0.9485306739807129\n",
      "Iter #4796416:  Learning rate = 0.000734:   Batch Loss = 0.158808, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2199030965566635, Accuracy = 0.9523561000823975\n",
      "Iter #4798464:  Learning rate = 0.000734:   Batch Loss = 0.109426, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23301149904727936, Accuracy = 0.9466179609298706\n",
      "Iter #4800512:  Learning rate = 0.000705:   Batch Loss = 0.130900, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24953211843967438, Accuracy = 0.9372283220291138\n",
      "Iter #4802560:  Learning rate = 0.000705:   Batch Loss = 0.130788, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23311607539653778, Accuracy = 0.9454007744789124\n",
      "Iter #4804608:  Learning rate = 0.000705:   Batch Loss = 0.137173, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21228452026844025, Accuracy = 0.9563553929328918\n",
      "Iter #4806656:  Learning rate = 0.000705:   Batch Loss = 0.130570, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2200792133808136, Accuracy = 0.9537471532821655\n",
      "Iter #4808704:  Learning rate = 0.000705:   Batch Loss = 0.145750, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22304287552833557, Accuracy = 0.9523561000823975\n",
      "Iter #4810752:  Learning rate = 0.000705:   Batch Loss = 0.125765, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23527905344963074, Accuracy = 0.9474874138832092\n",
      "Iter #4812800:  Learning rate = 0.000705:   Batch Loss = 0.142276, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23585614562034607, Accuracy = 0.9466179609298706\n",
      "Iter #4814848:  Learning rate = 0.000705:   Batch Loss = 0.152583, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23308026790618896, Accuracy = 0.9473134875297546\n",
      "Iter #4816896:  Learning rate = 0.000705:   Batch Loss = 0.121975, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22649526596069336, Accuracy = 0.9504433870315552\n",
      "Iter #4818944:  Learning rate = 0.000705:   Batch Loss = 0.138728, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21689337491989136, Accuracy = 0.9500956535339355\n",
      "Iter #4820992:  Learning rate = 0.000705:   Batch Loss = 0.150309, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24176740646362305, Accuracy = 0.9441836476325989\n",
      "Iter #4823040:  Learning rate = 0.000705:   Batch Loss = 0.135737, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23273620009422302, Accuracy = 0.9440097212791443\n",
      "Iter #4825088:  Learning rate = 0.000705:   Batch Loss = 0.137549, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2557312250137329, Accuracy = 0.9434880614280701\n",
      "Iter #4827136:  Learning rate = 0.000705:   Batch Loss = 0.172235, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22207586467266083, Accuracy = 0.9507911801338196\n",
      "Iter #4829184:  Learning rate = 0.000705:   Batch Loss = 0.119164, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23159675300121307, Accuracy = 0.9443575143814087\n",
      "Iter #4831232:  Learning rate = 0.000705:   Batch Loss = 0.131693, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2188955545425415, Accuracy = 0.9513128399848938\n",
      "Iter #4833280:  Learning rate = 0.000705:   Batch Loss = 0.120180, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22167029976844788, Accuracy = 0.9495739936828613\n",
      "Iter #4835328:  Learning rate = 0.000705:   Batch Loss = 0.154710, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22704841196537018, Accuracy = 0.9490523338317871\n",
      "Iter #4837376:  Learning rate = 0.000705:   Batch Loss = 0.115352, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23236197233200073, Accuracy = 0.9506173133850098\n",
      "Iter #4839424:  Learning rate = 0.000705:   Batch Loss = 0.120483, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2362847626209259, Accuracy = 0.947661280632019\n",
      "Iter #4841472:  Learning rate = 0.000705:   Batch Loss = 0.142503, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23661521077156067, Accuracy = 0.9455747008323669\n",
      "Iter #4843520:  Learning rate = 0.000705:   Batch Loss = 0.124395, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24195893108844757, Accuracy = 0.9405320882797241\n",
      "Iter #4845568:  Learning rate = 0.000705:   Batch Loss = 0.137538, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24129578471183777, Accuracy = 0.9429664611816406\n",
      "Iter #4847616:  Learning rate = 0.000705:   Batch Loss = 0.169799, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25843143463134766, Accuracy = 0.9433141946792603\n",
      "Iter #4849664:  Learning rate = 0.000705:   Batch Loss = 0.127796, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2641112506389618, Accuracy = 0.9422709345817566\n",
      "Iter #4851712:  Learning rate = 0.000705:   Batch Loss = 0.167520, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2329511046409607, Accuracy = 0.9460963606834412\n",
      "Iter #4853760:  Learning rate = 0.000705:   Batch Loss = 0.128133, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23076291382312775, Accuracy = 0.9509650468826294\n",
      "Iter #4855808:  Learning rate = 0.000705:   Batch Loss = 0.120272, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21911001205444336, Accuracy = 0.9537471532821655\n",
      "Iter #4857856:  Learning rate = 0.000705:   Batch Loss = 0.136010, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23560336232185364, Accuracy = 0.9481829404830933\n",
      "Iter #4859904:  Learning rate = 0.000705:   Batch Loss = 0.136618, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23355410993099213, Accuracy = 0.9483568072319031\n",
      "Iter #4861952:  Learning rate = 0.000705:   Batch Loss = 0.102169, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23144078254699707, Accuracy = 0.9495739936828613\n",
      "Iter #4864000:  Learning rate = 0.000705:   Batch Loss = 0.144727, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21824461221694946, Accuracy = 0.9532255530357361\n",
      "Iter #4866048:  Learning rate = 0.000705:   Batch Loss = 0.125665, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23670700192451477, Accuracy = 0.9436619877815247\n",
      "Iter #4868096:  Learning rate = 0.000705:   Batch Loss = 0.127035, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22290295362472534, Accuracy = 0.9445313811302185\n",
      "Iter #4870144:  Learning rate = 0.000705:   Batch Loss = 0.150507, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23372435569763184, Accuracy = 0.949921727180481\n",
      "Iter #4872192:  Learning rate = 0.000705:   Batch Loss = 0.139554, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24642109870910645, Accuracy = 0.9433141946792603\n",
      "Iter #4874240:  Learning rate = 0.000705:   Batch Loss = 0.150460, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26182642579078674, Accuracy = 0.9332290291786194\n",
      "Iter #4876288:  Learning rate = 0.000705:   Batch Loss = 0.147201, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2125859409570694, Accuracy = 0.9540949463844299\n",
      "Iter #4878336:  Learning rate = 0.000705:   Batch Loss = 0.166672, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2651066184043884, Accuracy = 0.9375760555267334\n",
      "Iter #4880384:  Learning rate = 0.000705:   Batch Loss = 0.168455, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2800537049770355, Accuracy = 0.9327073693275452\n",
      "Iter #4882432:  Learning rate = 0.000705:   Batch Loss = 0.137848, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2391834855079651, Accuracy = 0.9448791742324829\n",
      "Iter #4884480:  Learning rate = 0.000705:   Batch Loss = 0.109872, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2336980700492859, Accuracy = 0.9450530409812927\n",
      "Iter #4886528:  Learning rate = 0.000705:   Batch Loss = 0.122211, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21441000699996948, Accuracy = 0.9551382660865784\n",
      "Iter #4888576:  Learning rate = 0.000705:   Batch Loss = 0.158715, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21864403784275055, Accuracy = 0.9540949463844299\n",
      "Iter #4890624:  Learning rate = 0.000705:   Batch Loss = 0.131736, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2211696058511734, Accuracy = 0.9521822333335876\n",
      "Iter #4892672:  Learning rate = 0.000705:   Batch Loss = 0.118541, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21494948863983154, Accuracy = 0.9539210796356201\n",
      "Iter #4894720:  Learning rate = 0.000705:   Batch Loss = 0.124121, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2188754379749298, Accuracy = 0.9537471532821655\n",
      "Iter #4896768:  Learning rate = 0.000705:   Batch Loss = 0.124832, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21772903203964233, Accuracy = 0.9527038931846619\n",
      "Iter #4898816:  Learning rate = 0.000705:   Batch Loss = 0.124311, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2233523726463318, Accuracy = 0.9507911801338196\n",
      "Iter #4900864:  Learning rate = 0.000676:   Batch Loss = 0.105838, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25901558995246887, Accuracy = 0.9400104284286499\n",
      "Iter #4902912:  Learning rate = 0.000676:   Batch Loss = 0.150521, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22271841764450073, Accuracy = 0.9494001269340515\n",
      "Iter #4904960:  Learning rate = 0.000676:   Batch Loss = 0.123325, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22647026181221008, Accuracy = 0.946965754032135\n",
      "Iter #4907008:  Learning rate = 0.000676:   Batch Loss = 0.114107, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22541093826293945, Accuracy = 0.9502695202827454\n",
      "Iter #4909056:  Learning rate = 0.000676:   Batch Loss = 0.144389, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22440801560878754, Accuracy = 0.9466179609298706\n",
      "Iter #4911104:  Learning rate = 0.000676:   Batch Loss = 0.134780, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2142825424671173, Accuracy = 0.9537471532821655\n",
      "Iter #4913152:  Learning rate = 0.000676:   Batch Loss = 0.124442, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22118525207042694, Accuracy = 0.9490523338317871\n",
      "Iter #4915200:  Learning rate = 0.000676:   Batch Loss = 0.129247, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22886697947978973, Accuracy = 0.9490523338317871\n",
      "Iter #4917248:  Learning rate = 0.000676:   Batch Loss = 0.136325, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23190253973007202, Accuracy = 0.9490523338317871\n",
      "Iter #4919296:  Learning rate = 0.000676:   Batch Loss = 0.130357, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23507878184318542, Accuracy = 0.9471396207809448\n",
      "Iter #4921344:  Learning rate = 0.000676:   Batch Loss = 0.119931, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2731873095035553, Accuracy = 0.9360111355781555\n",
      "Iter #4923392:  Learning rate = 0.000676:   Batch Loss = 0.121980, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2551007866859436, Accuracy = 0.9445313811302185\n",
      "Iter #4925440:  Learning rate = 0.000676:   Batch Loss = 0.135811, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23046517372131348, Accuracy = 0.9448791742324829\n",
      "Iter #4927488:  Learning rate = 0.000676:   Batch Loss = 0.139655, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23185020685195923, Accuracy = 0.9471396207809448\n",
      "Iter #4929536:  Learning rate = 0.000676:   Batch Loss = 0.151558, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2145470380783081, Accuracy = 0.9516605734825134\n",
      "Iter #4931584:  Learning rate = 0.000676:   Batch Loss = 0.142190, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2184562385082245, Accuracy = 0.954790472984314\n",
      "Iter #4933632:  Learning rate = 0.000676:   Batch Loss = 0.113698, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22776128351688385, Accuracy = 0.9495739936828613\n",
      "Iter #4935680:  Learning rate = 0.000676:   Batch Loss = 0.137181, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2279069423675537, Accuracy = 0.9452269077301025\n",
      "Iter #4937728:  Learning rate = 0.000676:   Batch Loss = 0.142398, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2231273055076599, Accuracy = 0.949921727180481\n",
      "Iter #4939776:  Learning rate = 0.000676:   Batch Loss = 0.170333, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.244562029838562, Accuracy = 0.9429664611816406\n",
      "Iter #4941824:  Learning rate = 0.000676:   Batch Loss = 0.114876, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23450563848018646, Accuracy = 0.9480090141296387\n",
      "Iter #4943872:  Learning rate = 0.000676:   Batch Loss = 0.126134, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22032120823860168, Accuracy = 0.9535732865333557\n",
      "Iter #4945920:  Learning rate = 0.000676:   Batch Loss = 0.106474, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2160484492778778, Accuracy = 0.9509650468826294\n",
      "Iter #4947968:  Learning rate = 0.000676:   Batch Loss = 0.144693, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23505142331123352, Accuracy = 0.9431403279304504\n",
      "Iter #4950016:  Learning rate = 0.000676:   Batch Loss = 0.126063, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2246132642030716, Accuracy = 0.9520083665847778\n",
      "Iter #4952064:  Learning rate = 0.000676:   Batch Loss = 0.138379, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21695443987846375, Accuracy = 0.9518344402313232\n",
      "Iter #4954112:  Learning rate = 0.000676:   Batch Loss = 0.138361, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22531500458717346, Accuracy = 0.9509650468826294\n",
      "Iter #4956160:  Learning rate = 0.000676:   Batch Loss = 0.119245, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22122615575790405, Accuracy = 0.9504433870315552\n",
      "Iter #4958208:  Learning rate = 0.000676:   Batch Loss = 0.133707, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23109889030456543, Accuracy = 0.9483568072319031\n",
      "Iter #4960256:  Learning rate = 0.000676:   Batch Loss = 0.122345, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23150131106376648, Accuracy = 0.9485306739807129\n",
      "Iter #4962304:  Learning rate = 0.000676:   Batch Loss = 0.181210, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21775653958320618, Accuracy = 0.9504433870315552\n",
      "Iter #4964352:  Learning rate = 0.000676:   Batch Loss = 0.143908, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2572588324546814, Accuracy = 0.9398365616798401\n",
      "Iter #4966400:  Learning rate = 0.000676:   Batch Loss = 0.147167, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25245392322540283, Accuracy = 0.9394887685775757\n",
      "Iter #4968448:  Learning rate = 0.000676:   Batch Loss = 0.152279, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23378515243530273, Accuracy = 0.9467918872833252\n",
      "Iter #4970496:  Learning rate = 0.000676:   Batch Loss = 0.112729, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22930997610092163, Accuracy = 0.9438358545303345\n",
      "Iter #4972544:  Learning rate = 0.000676:   Batch Loss = 0.127565, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23114517331123352, Accuracy = 0.9478351473808289\n",
      "Iter #4974592:  Learning rate = 0.000676:   Batch Loss = 0.146383, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22811448574066162, Accuracy = 0.9471396207809448\n",
      "Iter #4976640:  Learning rate = 0.000676:   Batch Loss = 0.153120, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21712833642959595, Accuracy = 0.9530516266822815\n",
      "Iter #4978688:  Learning rate = 0.000676:   Batch Loss = 0.125536, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22834482789039612, Accuracy = 0.9490523338317871\n",
      "Iter #4980736:  Learning rate = 0.000676:   Batch Loss = 0.123600, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2311204969882965, Accuracy = 0.9445313811302185\n",
      "Iter #4982784:  Learning rate = 0.000676:   Batch Loss = 0.146916, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2298985868692398, Accuracy = 0.9481829404830933\n",
      "Iter #4984832:  Learning rate = 0.000676:   Batch Loss = 0.132141, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23107486963272095, Accuracy = 0.9471396207809448\n",
      "Iter #4986880:  Learning rate = 0.000676:   Batch Loss = 0.130294, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2275223731994629, Accuracy = 0.9513128399848938\n",
      "Iter #4988928:  Learning rate = 0.000676:   Batch Loss = 0.125263, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2199653536081314, Accuracy = 0.9487046003341675\n",
      "Iter #4990976:  Learning rate = 0.000676:   Batch Loss = 0.130843, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2230820655822754, Accuracy = 0.9516605734825134\n",
      "Iter #4993024:  Learning rate = 0.000676:   Batch Loss = 0.174181, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2315451204776764, Accuracy = 0.9417492747306824\n",
      "Iter #4995072:  Learning rate = 0.000676:   Batch Loss = 0.119737, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2133328914642334, Accuracy = 0.9539210796356201\n",
      "Iter #4997120:  Learning rate = 0.000676:   Batch Loss = 0.135145, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21576833724975586, Accuracy = 0.9572248458862305\n",
      "Iter #4999168:  Learning rate = 0.000676:   Batch Loss = 0.132669, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21901172399520874, Accuracy = 0.9539210796356201\n",
      "Iter #5001216:  Learning rate = 0.000649:   Batch Loss = 0.174830, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22677236795425415, Accuracy = 0.9480090141296387\n",
      "Iter #5003264:  Learning rate = 0.000649:   Batch Loss = 0.171406, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22488531470298767, Accuracy = 0.9497478604316711\n",
      "Iter #5005312:  Learning rate = 0.000649:   Batch Loss = 0.121960, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24017715454101562, Accuracy = 0.9448791742324829\n",
      "Iter #5007360:  Learning rate = 0.000649:   Batch Loss = 0.132375, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22239401936531067, Accuracy = 0.9518344402313232\n",
      "Iter #5009408:  Learning rate = 0.000649:   Batch Loss = 0.145733, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22592531144618988, Accuracy = 0.9488784670829773\n",
      "Iter #5011456:  Learning rate = 0.000649:   Batch Loss = 0.137546, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21412238478660583, Accuracy = 0.9556598663330078\n",
      "Iter #5013504:  Learning rate = 0.000649:   Batch Loss = 0.136051, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2264237403869629, Accuracy = 0.9494001269340515\n",
      "Iter #5015552:  Learning rate = 0.000649:   Batch Loss = 0.121421, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2247675061225891, Accuracy = 0.9464440941810608\n",
      "Iter #5017600:  Learning rate = 0.000649:   Batch Loss = 0.119704, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24271702766418457, Accuracy = 0.946965754032135\n",
      "Iter #5019648:  Learning rate = 0.000649:   Batch Loss = 0.131383, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21330106258392334, Accuracy = 0.9532255530357361\n",
      "Iter #5021696:  Learning rate = 0.000649:   Batch Loss = 0.150871, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23049774765968323, Accuracy = 0.947661280632019\n",
      "Iter #5023744:  Learning rate = 0.000649:   Batch Loss = 0.134996, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2299177646636963, Accuracy = 0.9504433870315552\n",
      "Iter #5025792:  Learning rate = 0.000649:   Batch Loss = 0.130898, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2273433953523636, Accuracy = 0.9530516266822815\n",
      "Iter #5027840:  Learning rate = 0.000649:   Batch Loss = 0.170816, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2365700900554657, Accuracy = 0.9441836476325989\n",
      "Iter #5029888:  Learning rate = 0.000649:   Batch Loss = 0.101187, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2288501113653183, Accuracy = 0.9528777599334717\n",
      "Iter #5031936:  Learning rate = 0.000649:   Batch Loss = 0.125038, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23219171166419983, Accuracy = 0.9441836476325989\n",
      "Iter #5033984:  Learning rate = 0.000649:   Batch Loss = 0.152821, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24191315472126007, Accuracy = 0.9455747008323669\n",
      "Iter #5036032:  Learning rate = 0.000649:   Batch Loss = 0.129204, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21721100807189941, Accuracy = 0.9488784670829773\n",
      "Iter #5038080:  Learning rate = 0.000649:   Batch Loss = 0.129556, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21584931015968323, Accuracy = 0.9507911801338196\n",
      "Iter #5040128:  Learning rate = 0.000649:   Batch Loss = 0.116994, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22856926918029785, Accuracy = 0.9455747008323669\n",
      "Iter #5042176:  Learning rate = 0.000649:   Batch Loss = 0.142474, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20861956477165222, Accuracy = 0.9570509195327759\n",
      "Iter #5044224:  Learning rate = 0.000649:   Batch Loss = 0.122879, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22293773293495178, Accuracy = 0.9521822333335876\n",
      "Iter #5046272:  Learning rate = 0.000649:   Batch Loss = 0.129893, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21169552206993103, Accuracy = 0.954790472984314\n",
      "Iter #5048320:  Learning rate = 0.000649:   Batch Loss = 0.131374, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2341180145740509, Accuracy = 0.9448791742324829\n",
      "Iter #5050368:  Learning rate = 0.000649:   Batch Loss = 0.159962, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23622387647628784, Accuracy = 0.9443575143814087\n",
      "Iter #5052416:  Learning rate = 0.000649:   Batch Loss = 0.155244, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23304036259651184, Accuracy = 0.9507911801338196\n",
      "Iter #5054464:  Learning rate = 0.000649:   Batch Loss = 0.124906, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21779541671276093, Accuracy = 0.9528777599334717\n",
      "Iter #5056512:  Learning rate = 0.000649:   Batch Loss = 0.129917, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22299383580684662, Accuracy = 0.9513128399848938\n",
      "Iter #5058560:  Learning rate = 0.000649:   Batch Loss = 0.132834, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23001140356063843, Accuracy = 0.9460963606834412\n",
      "Iter #5060608:  Learning rate = 0.000649:   Batch Loss = 0.167005, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24039560556411743, Accuracy = 0.9445313811302185\n",
      "Iter #5062656:  Learning rate = 0.000649:   Batch Loss = 0.109680, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2324942648410797, Accuracy = 0.9473134875297546\n",
      "Iter #5064704:  Learning rate = 0.000649:   Batch Loss = 0.131630, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22241708636283875, Accuracy = 0.9518344402313232\n",
      "Iter #5066752:  Learning rate = 0.000649:   Batch Loss = 0.126113, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23630541563034058, Accuracy = 0.9474874138832092\n",
      "Iter #5068800:  Learning rate = 0.000649:   Batch Loss = 0.185387, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23492372035980225, Accuracy = 0.9454007744789124\n",
      "Iter #5070848:  Learning rate = 0.000649:   Batch Loss = 0.133844, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2473519742488861, Accuracy = 0.9452269077301025\n",
      "Iter #5072896:  Learning rate = 0.000649:   Batch Loss = 0.144660, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22778110206127167, Accuracy = 0.9509650468826294\n",
      "Iter #5074944:  Learning rate = 0.000649:   Batch Loss = 0.136536, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21487560868263245, Accuracy = 0.955485999584198\n",
      "Iter #5076992:  Learning rate = 0.000649:   Batch Loss = 0.138499, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2123270034790039, Accuracy = 0.9532255530357361\n",
      "Iter #5079040:  Learning rate = 0.000649:   Batch Loss = 0.167156, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21721145510673523, Accuracy = 0.9540949463844299\n",
      "Iter #5081088:  Learning rate = 0.000649:   Batch Loss = 0.144231, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21717935800552368, Accuracy = 0.9513128399848938\n",
      "Iter #5083136:  Learning rate = 0.000649:   Batch Loss = 0.139921, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21099358797073364, Accuracy = 0.9537471532821655\n",
      "Iter #5085184:  Learning rate = 0.000649:   Batch Loss = 0.151248, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.26017603278160095, Accuracy = 0.9396626949310303\n",
      "Iter #5087232:  Learning rate = 0.000649:   Batch Loss = 0.162110, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2409447878599167, Accuracy = 0.9471396207809448\n",
      "Iter #5089280:  Learning rate = 0.000649:   Batch Loss = 0.151942, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25091564655303955, Accuracy = 0.9433141946792603\n",
      "Iter #5091328:  Learning rate = 0.000649:   Batch Loss = 0.116629, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22065100073814392, Accuracy = 0.9490523338317871\n",
      "Iter #5093376:  Learning rate = 0.000649:   Batch Loss = 0.138402, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.217220738530159, Accuracy = 0.9511389136314392\n",
      "Iter #5095424:  Learning rate = 0.000649:   Batch Loss = 0.142877, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22381147742271423, Accuracy = 0.9504433870315552\n",
      "Iter #5097472:  Learning rate = 0.000649:   Batch Loss = 0.146776, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24958980083465576, Accuracy = 0.9433141946792603\n",
      "Iter #5099520:  Learning rate = 0.000649:   Batch Loss = 0.152633, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24283336102962494, Accuracy = 0.9455747008323669\n",
      "Iter #5101568:  Learning rate = 0.000623:   Batch Loss = 0.134193, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20605899393558502, Accuracy = 0.9553121328353882\n",
      "Iter #5103616:  Learning rate = 0.000623:   Batch Loss = 0.120746, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21753573417663574, Accuracy = 0.9513128399848938\n",
      "Iter #5105664:  Learning rate = 0.000623:   Batch Loss = 0.107095, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22271178662776947, Accuracy = 0.9487046003341675\n",
      "Iter #5107712:  Learning rate = 0.000623:   Batch Loss = 0.124899, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22931283712387085, Accuracy = 0.9490523338317871\n",
      "Iter #5109760:  Learning rate = 0.000623:   Batch Loss = 0.118506, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2404080480337143, Accuracy = 0.9436619877815247\n",
      "Iter #5111808:  Learning rate = 0.000623:   Batch Loss = 0.164358, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.221433624625206, Accuracy = 0.9506173133850098\n",
      "Iter #5113856:  Learning rate = 0.000623:   Batch Loss = 0.132811, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21876530349254608, Accuracy = 0.9518344402313232\n",
      "Iter #5115904:  Learning rate = 0.000623:   Batch Loss = 0.120165, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2146872729063034, Accuracy = 0.9518344402313232\n",
      "Iter #5117952:  Learning rate = 0.000623:   Batch Loss = 0.136618, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21958070993423462, Accuracy = 0.9525299668312073\n",
      "Iter #5120000:  Learning rate = 0.000623:   Batch Loss = 0.104293, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2233804166316986, Accuracy = 0.949921727180481\n",
      "Iter #5122048:  Learning rate = 0.000623:   Batch Loss = 0.134667, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21108248829841614, Accuracy = 0.9567031860351562\n",
      "Iter #5124096:  Learning rate = 0.000623:   Batch Loss = 0.112458, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22103634476661682, Accuracy = 0.9546166062355042\n",
      "Iter #5126144:  Learning rate = 0.000623:   Batch Loss = 0.120287, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22612187266349792, Accuracy = 0.9459224343299866\n",
      "Iter #5128192:  Learning rate = 0.000623:   Batch Loss = 0.133285, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25203168392181396, Accuracy = 0.9412276148796082\n",
      "Iter #5130240:  Learning rate = 0.000623:   Batch Loss = 0.148352, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24030986428260803, Accuracy = 0.9455747008323669\n",
      "Iter #5132288:  Learning rate = 0.000623:   Batch Loss = 0.113952, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2245820164680481, Accuracy = 0.9494001269340515\n",
      "Iter #5134336:  Learning rate = 0.000623:   Batch Loss = 0.126119, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22393587231636047, Accuracy = 0.9487046003341675\n",
      "Iter #5136384:  Learning rate = 0.000623:   Batch Loss = 0.150923, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.236793652176857, Accuracy = 0.9407059550285339\n",
      "Iter #5138432:  Learning rate = 0.000623:   Batch Loss = 0.188023, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24774879217147827, Accuracy = 0.9454007744789124\n",
      "Iter #5140480:  Learning rate = 0.000623:   Batch Loss = 0.132695, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24378013610839844, Accuracy = 0.9460963606834412\n",
      "Iter #5142528:  Learning rate = 0.000623:   Batch Loss = 0.160058, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21415510773658752, Accuracy = 0.9546166062355042\n",
      "Iter #5144576:  Learning rate = 0.000623:   Batch Loss = 0.133184, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22776386141777039, Accuracy = 0.9464440941810608\n",
      "Iter #5146624:  Learning rate = 0.000623:   Batch Loss = 0.140573, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21192018687725067, Accuracy = 0.9563553929328918\n",
      "Iter #5148672:  Learning rate = 0.000623:   Batch Loss = 0.140767, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2359682321548462, Accuracy = 0.9488784670829773\n",
      "Iter #5150720:  Learning rate = 0.000623:   Batch Loss = 0.142000, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22816546261310577, Accuracy = 0.946270227432251\n",
      "Iter #5152768:  Learning rate = 0.000623:   Batch Loss = 0.160836, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23953509330749512, Accuracy = 0.9460963606834412\n",
      "Iter #5154816:  Learning rate = 0.000623:   Batch Loss = 0.135508, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21857047080993652, Accuracy = 0.954790472984314\n",
      "Iter #5156864:  Learning rate = 0.000623:   Batch Loss = 0.107169, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2386368215084076, Accuracy = 0.9403582215309143\n",
      "Iter #5158912:  Learning rate = 0.000623:   Batch Loss = 0.147417, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2250962257385254, Accuracy = 0.9507911801338196\n",
      "Iter #5160960:  Learning rate = 0.000623:   Batch Loss = 0.111183, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21819141507148743, Accuracy = 0.9546166062355042\n",
      "Iter #5163008:  Learning rate = 0.000623:   Batch Loss = 0.102676, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2304999828338623, Accuracy = 0.946270227432251\n",
      "Iter #5165056:  Learning rate = 0.000623:   Batch Loss = 0.130508, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2198612540960312, Accuracy = 0.9494001269340515\n",
      "Iter #5167104:  Learning rate = 0.000623:   Batch Loss = 0.175893, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24728238582611084, Accuracy = 0.9448791742324829\n",
      "Iter #5169152:  Learning rate = 0.000623:   Batch Loss = 0.121975, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.237568661570549, Accuracy = 0.9403582215309143\n",
      "Iter #5171200:  Learning rate = 0.000623:   Batch Loss = 0.111857, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2301221489906311, Accuracy = 0.9485306739807129\n",
      "Iter #5173248:  Learning rate = 0.000623:   Batch Loss = 0.106064, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22263693809509277, Accuracy = 0.9525299668312073\n",
      "Iter #5175296:  Learning rate = 0.000623:   Batch Loss = 0.165708, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2287628948688507, Accuracy = 0.9520083665847778\n",
      "Iter #5177344:  Learning rate = 0.000623:   Batch Loss = 0.121351, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2491043210029602, Accuracy = 0.9426186680793762\n",
      "Iter #5179392:  Learning rate = 0.000623:   Batch Loss = 0.121338, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24146685004234314, Accuracy = 0.9438358545303345\n",
      "Iter #5181440:  Learning rate = 0.000623:   Batch Loss = 0.194210, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22653505206108093, Accuracy = 0.9518344402313232\n",
      "Iter #5183488:  Learning rate = 0.000623:   Batch Loss = 0.122109, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23100900650024414, Accuracy = 0.9500956535339355\n",
      "Iter #5185536:  Learning rate = 0.000623:   Batch Loss = 0.119604, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23098096251487732, Accuracy = 0.946270227432251\n",
      "Iter #5187584:  Learning rate = 0.000623:   Batch Loss = 0.133370, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23077458143234253, Accuracy = 0.9433141946792603\n",
      "Iter #5189632:  Learning rate = 0.000623:   Batch Loss = 0.167775, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2359369397163391, Accuracy = 0.9487046003341675\n",
      "Iter #5191680:  Learning rate = 0.000623:   Batch Loss = 0.146289, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24349626898765564, Accuracy = 0.9434880614280701\n",
      "Iter #5193728:  Learning rate = 0.000623:   Batch Loss = 0.160140, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23236629366874695, Accuracy = 0.9506173133850098\n",
      "Iter #5195776:  Learning rate = 0.000623:   Batch Loss = 0.120511, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2224368453025818, Accuracy = 0.9525299668312073\n",
      "Iter #5197824:  Learning rate = 0.000623:   Batch Loss = 0.138131, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22808919847011566, Accuracy = 0.9500956535339355\n",
      "Iter #5199872:  Learning rate = 0.000623:   Batch Loss = 0.150588, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24923038482666016, Accuracy = 0.9424448013305664\n",
      "Iter #5201920:  Learning rate = 0.000599:   Batch Loss = 0.119311, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21287913620471954, Accuracy = 0.9549643397331238\n",
      "Iter #5203968:  Learning rate = 0.000599:   Batch Loss = 0.108645, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22270599007606506, Accuracy = 0.9507911801338196\n",
      "Iter #5206016:  Learning rate = 0.000599:   Batch Loss = 0.115238, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22151592373847961, Accuracy = 0.9533994197845459\n",
      "Iter #5208064:  Learning rate = 0.000599:   Batch Loss = 0.138218, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22696055471897125, Accuracy = 0.9497478604316711\n",
      "Iter #5210112:  Learning rate = 0.000599:   Batch Loss = 0.182417, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2612326741218567, Accuracy = 0.9349678158760071\n",
      "Iter #5212160:  Learning rate = 0.000599:   Batch Loss = 0.142672, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23150426149368286, Accuracy = 0.9520083665847778\n",
      "Iter #5214208:  Learning rate = 0.000599:   Batch Loss = 0.156260, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24048638343811035, Accuracy = 0.9455747008323669\n",
      "Iter #5216256:  Learning rate = 0.000599:   Batch Loss = 0.151245, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21398884057998657, Accuracy = 0.9546166062355042\n",
      "Iter #5218304:  Learning rate = 0.000599:   Batch Loss = 0.134715, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23360459506511688, Accuracy = 0.9445313811302185\n",
      "Iter #5220352:  Learning rate = 0.000599:   Batch Loss = 0.156590, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2176346778869629, Accuracy = 0.9539210796356201\n",
      "Iter #5222400:  Learning rate = 0.000599:   Batch Loss = 0.122234, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23742137849330902, Accuracy = 0.947661280632019\n",
      "Iter #5224448:  Learning rate = 0.000599:   Batch Loss = 0.124818, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22443974018096924, Accuracy = 0.9480090141296387\n",
      "Iter #5226496:  Learning rate = 0.000599:   Batch Loss = 0.173797, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23236076533794403, Accuracy = 0.946270227432251\n",
      "Iter #5228544:  Learning rate = 0.000599:   Batch Loss = 0.136599, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2091263234615326, Accuracy = 0.9560076594352722\n",
      "Iter #5230592:  Learning rate = 0.000599:   Batch Loss = 0.145540, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2279670387506485, Accuracy = 0.9521822333335876\n",
      "Iter #5232640:  Learning rate = 0.000599:   Batch Loss = 0.119560, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.27051830291748047, Accuracy = 0.9374021887779236\n",
      "Iter #5234688:  Learning rate = 0.000599:   Batch Loss = 0.134630, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2277757227420807, Accuracy = 0.949921727180481\n",
      "Iter #5236736:  Learning rate = 0.000599:   Batch Loss = 0.128313, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2212907373905182, Accuracy = 0.9471396207809448\n",
      "Iter #5238784:  Learning rate = 0.000599:   Batch Loss = 0.145496, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23234814405441284, Accuracy = 0.9513128399848938\n",
      "Iter #5240832:  Learning rate = 0.000599:   Batch Loss = 0.128384, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22604763507843018, Accuracy = 0.9485306739807129\n",
      "Iter #5242880:  Learning rate = 0.000599:   Batch Loss = 0.195663, Accuracy = 0.94921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24329182505607605, Accuracy = 0.9459224343299866\n",
      "Iter #5244928:  Learning rate = 0.000599:   Batch Loss = 0.132681, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25038373470306396, Accuracy = 0.9368805289268494\n",
      "Iter #5246976:  Learning rate = 0.000599:   Batch Loss = 0.121023, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2456766963005066, Accuracy = 0.9438358545303345\n",
      "Iter #5249024:  Learning rate = 0.000599:   Batch Loss = 0.099268, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2109568864107132, Accuracy = 0.955485999584198\n",
      "Iter #5251072:  Learning rate = 0.000599:   Batch Loss = 0.138634, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2321470081806183, Accuracy = 0.9497478604316711\n",
      "Iter #5253120:  Learning rate = 0.000599:   Batch Loss = 0.147533, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22847387194633484, Accuracy = 0.946965754032135\n",
      "Iter #5255168:  Learning rate = 0.000599:   Batch Loss = 0.141453, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22773009538650513, Accuracy = 0.9528777599334717\n",
      "Iter #5257216:  Learning rate = 0.000599:   Batch Loss = 0.107178, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2404681295156479, Accuracy = 0.9488784670829773\n",
      "Iter #5259264:  Learning rate = 0.000599:   Batch Loss = 0.130203, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2220451384782791, Accuracy = 0.9521822333335876\n",
      "Iter #5261312:  Learning rate = 0.000599:   Batch Loss = 0.114014, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21763110160827637, Accuracy = 0.9553121328353882\n",
      "Iter #5263360:  Learning rate = 0.000599:   Batch Loss = 0.125996, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23812660574913025, Accuracy = 0.9478351473808289\n",
      "Iter #5265408:  Learning rate = 0.000599:   Batch Loss = 0.120451, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21012134850025177, Accuracy = 0.9565293192863464\n",
      "Iter #5267456:  Learning rate = 0.000599:   Batch Loss = 0.129474, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21181455254554749, Accuracy = 0.9551382660865784\n",
      "Iter #5269504:  Learning rate = 0.000599:   Batch Loss = 0.113745, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2222628891468048, Accuracy = 0.9494001269340515\n",
      "Iter #5271552:  Learning rate = 0.000599:   Batch Loss = 0.130913, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22046461701393127, Accuracy = 0.9514867067337036\n",
      "Iter #5273600:  Learning rate = 0.000599:   Batch Loss = 0.138123, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22427722811698914, Accuracy = 0.9532255530357361\n",
      "Iter #5275648:  Learning rate = 0.000599:   Batch Loss = 0.168028, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2265210598707199, Accuracy = 0.9521822333335876\n",
      "Iter #5277696:  Learning rate = 0.000599:   Batch Loss = 0.119587, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23311373591423035, Accuracy = 0.9464440941810608\n",
      "Iter #5279744:  Learning rate = 0.000599:   Batch Loss = 0.116267, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22958418726921082, Accuracy = 0.9466179609298706\n",
      "Iter #5281792:  Learning rate = 0.000599:   Batch Loss = 0.121686, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22677242755889893, Accuracy = 0.9481829404830933\n",
      "Iter #5283840:  Learning rate = 0.000599:   Batch Loss = 0.163286, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23466865718364716, Accuracy = 0.9500956535339355\n",
      "Iter #5285888:  Learning rate = 0.000599:   Batch Loss = 0.122354, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23490308225154877, Accuracy = 0.9438358545303345\n",
      "Iter #5287936:  Learning rate = 0.000599:   Batch Loss = 0.112155, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24053791165351868, Accuracy = 0.9492262005805969\n",
      "Iter #5289984:  Learning rate = 0.000599:   Batch Loss = 0.139687, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22795173525810242, Accuracy = 0.9506173133850098\n",
      "Iter #5292032:  Learning rate = 0.000599:   Batch Loss = 0.140688, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22485876083374023, Accuracy = 0.9535732865333557\n",
      "Iter #5294080:  Learning rate = 0.000599:   Batch Loss = 0.134976, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2150590717792511, Accuracy = 0.9528777599334717\n",
      "Iter #5296128:  Learning rate = 0.000599:   Batch Loss = 0.105001, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21149010956287384, Accuracy = 0.954790472984314\n",
      "Iter #5298176:  Learning rate = 0.000599:   Batch Loss = 0.129714, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22398488223552704, Accuracy = 0.9514867067337036\n",
      "Iter #5300224:  Learning rate = 0.000575:   Batch Loss = 0.111467, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22165465354919434, Accuracy = 0.9520083665847778\n",
      "Iter #5302272:  Learning rate = 0.000575:   Batch Loss = 0.101988, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24104727804660797, Accuracy = 0.9440097212791443\n",
      "Iter #5304320:  Learning rate = 0.000575:   Batch Loss = 0.131099, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22964993119239807, Accuracy = 0.9497478604316711\n",
      "Iter #5306368:  Learning rate = 0.000575:   Batch Loss = 0.127844, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22086304426193237, Accuracy = 0.9518344402313232\n",
      "Iter #5308416:  Learning rate = 0.000575:   Batch Loss = 0.126787, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23133446276187897, Accuracy = 0.9502695202827454\n",
      "Iter #5310464:  Learning rate = 0.000575:   Batch Loss = 0.125579, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22015762329101562, Accuracy = 0.9509650468826294\n",
      "Iter #5312512:  Learning rate = 0.000575:   Batch Loss = 0.131125, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2187025547027588, Accuracy = 0.9511389136314392\n",
      "Iter #5314560:  Learning rate = 0.000575:   Batch Loss = 0.144264, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23134320974349976, Accuracy = 0.9487046003341675\n",
      "Iter #5316608:  Learning rate = 0.000575:   Batch Loss = 0.128401, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22078196704387665, Accuracy = 0.9504433870315552\n",
      "Iter #5318656:  Learning rate = 0.000575:   Batch Loss = 0.140679, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23345139622688293, Accuracy = 0.946965754032135\n",
      "Iter #5320704:  Learning rate = 0.000575:   Batch Loss = 0.124180, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23513460159301758, Accuracy = 0.9466179609298706\n",
      "Iter #5322752:  Learning rate = 0.000575:   Batch Loss = 0.100098, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22302860021591187, Accuracy = 0.9507911801338196\n",
      "Iter #5324800:  Learning rate = 0.000575:   Batch Loss = 0.115025, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21397468447685242, Accuracy = 0.9568770527839661\n",
      "Iter #5326848:  Learning rate = 0.000575:   Batch Loss = 0.113826, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2205754816532135, Accuracy = 0.9460963606834412\n",
      "Iter #5328896:  Learning rate = 0.000575:   Batch Loss = 0.140678, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24222677946090698, Accuracy = 0.9473134875297546\n",
      "Iter #5330944:  Learning rate = 0.000575:   Batch Loss = 0.118714, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2148638665676117, Accuracy = 0.9540949463844299\n",
      "Iter #5332992:  Learning rate = 0.000575:   Batch Loss = 0.149261, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21321210265159607, Accuracy = 0.9556598663330078\n",
      "Iter #5335040:  Learning rate = 0.000575:   Batch Loss = 0.117681, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21184141933918, Accuracy = 0.956181526184082\n",
      "Iter #5337088:  Learning rate = 0.000575:   Batch Loss = 0.119954, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2253655195236206, Accuracy = 0.947661280632019\n",
      "Iter #5339136:  Learning rate = 0.000575:   Batch Loss = 0.112637, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24144117534160614, Accuracy = 0.9455747008323669\n",
      "Iter #5341184:  Learning rate = 0.000575:   Batch Loss = 0.125737, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23722267150878906, Accuracy = 0.9466179609298706\n",
      "Iter #5343232:  Learning rate = 0.000575:   Batch Loss = 0.116893, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23609867691993713, Accuracy = 0.9452269077301025\n",
      "Iter #5345280:  Learning rate = 0.000575:   Batch Loss = 0.129002, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2408493161201477, Accuracy = 0.9467918872833252\n",
      "Iter #5347328:  Learning rate = 0.000575:   Batch Loss = 0.162192, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22626110911369324, Accuracy = 0.9504433870315552\n",
      "Iter #5349376:  Learning rate = 0.000575:   Batch Loss = 0.129884, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21435989439487457, Accuracy = 0.955485999584198\n",
      "Iter #5351424:  Learning rate = 0.000575:   Batch Loss = 0.094588, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23571069538593292, Accuracy = 0.9487046003341675\n",
      "Iter #5353472:  Learning rate = 0.000575:   Batch Loss = 0.155665, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24378761649131775, Accuracy = 0.9450530409812927\n",
      "Iter #5355520:  Learning rate = 0.000575:   Batch Loss = 0.158320, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21623468399047852, Accuracy = 0.9527038931846619\n",
      "Iter #5357568:  Learning rate = 0.000575:   Batch Loss = 0.141315, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22133806347846985, Accuracy = 0.9506173133850098\n",
      "Iter #5359616:  Learning rate = 0.000575:   Batch Loss = 0.131946, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22836031019687653, Accuracy = 0.9483568072319031\n",
      "Iter #5361664:  Learning rate = 0.000575:   Batch Loss = 0.130761, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22596901655197144, Accuracy = 0.9495739936828613\n",
      "Iter #5363712:  Learning rate = 0.000575:   Batch Loss = 0.132221, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22354590892791748, Accuracy = 0.9540949463844299\n",
      "Iter #5365760:  Learning rate = 0.000575:   Batch Loss = 0.143205, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23420894145965576, Accuracy = 0.9518344402313232\n",
      "Iter #5367808:  Learning rate = 0.000575:   Batch Loss = 0.149218, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21937189996242523, Accuracy = 0.9523561000823975\n",
      "Iter #5369856:  Learning rate = 0.000575:   Batch Loss = 0.207548, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2997954785823822, Accuracy = 0.9241871237754822\n",
      "Iter #5371904:  Learning rate = 0.000575:   Batch Loss = 0.150782, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23195907473564148, Accuracy = 0.9440097212791443\n",
      "Iter #5373952:  Learning rate = 0.000575:   Batch Loss = 0.117175, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23784251511096954, Accuracy = 0.9464440941810608\n",
      "Iter #5376000:  Learning rate = 0.000575:   Batch Loss = 0.149239, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22064028680324554, Accuracy = 0.9514867067337036\n",
      "Iter #5378048:  Learning rate = 0.000575:   Batch Loss = 0.140770, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22005029022693634, Accuracy = 0.9473134875297546\n",
      "Iter #5380096:  Learning rate = 0.000575:   Batch Loss = 0.130949, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21274423599243164, Accuracy = 0.9532255530357361\n",
      "Iter #5382144:  Learning rate = 0.000575:   Batch Loss = 0.140826, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21667011082172394, Accuracy = 0.9553121328353882\n",
      "Iter #5384192:  Learning rate = 0.000575:   Batch Loss = 0.157519, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21413180232048035, Accuracy = 0.9553121328353882\n",
      "Iter #5386240:  Learning rate = 0.000575:   Batch Loss = 0.194443, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23306109011173248, Accuracy = 0.9485306739807129\n",
      "Iter #5388288:  Learning rate = 0.000575:   Batch Loss = 0.133924, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23707005381584167, Accuracy = 0.9466179609298706\n",
      "Iter #5390336:  Learning rate = 0.000575:   Batch Loss = 0.115413, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20710313320159912, Accuracy = 0.9549643397331238\n",
      "Iter #5392384:  Learning rate = 0.000575:   Batch Loss = 0.147248, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21777012944221497, Accuracy = 0.9527038931846619\n",
      "Iter #5394432:  Learning rate = 0.000575:   Batch Loss = 0.162564, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21146591007709503, Accuracy = 0.9565293192863464\n",
      "Iter #5396480:  Learning rate = 0.000575:   Batch Loss = 0.156993, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23478959500789642, Accuracy = 0.942792534828186\n",
      "Iter #5398528:  Learning rate = 0.000575:   Batch Loss = 0.128827, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22503551840782166, Accuracy = 0.9516605734825134\n",
      "Iter #5400576:  Learning rate = 0.000552:   Batch Loss = 0.113806, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22538596391677856, Accuracy = 0.9457485675811768\n",
      "Iter #5402624:  Learning rate = 0.000552:   Batch Loss = 0.129335, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22125741839408875, Accuracy = 0.9518344402313232\n",
      "Iter #5404672:  Learning rate = 0.000552:   Batch Loss = 0.114519, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21120589971542358, Accuracy = 0.9553121328353882\n",
      "Iter #5406720:  Learning rate = 0.000552:   Batch Loss = 0.148425, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22077056765556335, Accuracy = 0.9523561000823975\n",
      "Iter #5408768:  Learning rate = 0.000552:   Batch Loss = 0.121427, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23494209349155426, Accuracy = 0.9474874138832092\n",
      "Iter #5410816:  Learning rate = 0.000552:   Batch Loss = 0.147686, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2212599813938141, Accuracy = 0.9488784670829773\n",
      "Iter #5412864:  Learning rate = 0.000552:   Batch Loss = 0.137072, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2208043783903122, Accuracy = 0.9537471532821655\n",
      "Iter #5414912:  Learning rate = 0.000552:   Batch Loss = 0.124370, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21328821778297424, Accuracy = 0.9535732865333557\n",
      "Iter #5416960:  Learning rate = 0.000552:   Batch Loss = 0.121856, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2207728773355484, Accuracy = 0.9533994197845459\n",
      "Iter #5419008:  Learning rate = 0.000552:   Batch Loss = 0.107362, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20511239767074585, Accuracy = 0.9600069522857666\n",
      "Iter #5421056:  Learning rate = 0.000552:   Batch Loss = 0.122167, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21322360634803772, Accuracy = 0.9575725793838501\n",
      "Iter #5423104:  Learning rate = 0.000552:   Batch Loss = 0.171707, Accuracy = 0.953125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21734029054641724, Accuracy = 0.9525299668312073\n",
      "Iter #5425152:  Learning rate = 0.000552:   Batch Loss = 0.132240, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21370625495910645, Accuracy = 0.9520083665847778\n",
      "Iter #5427200:  Learning rate = 0.000552:   Batch Loss = 0.117688, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22089868783950806, Accuracy = 0.9527038931846619\n",
      "Iter #5429248:  Learning rate = 0.000552:   Batch Loss = 0.124993, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2447662353515625, Accuracy = 0.946270227432251\n",
      "Iter #5431296:  Learning rate = 0.000552:   Batch Loss = 0.126623, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22321903705596924, Accuracy = 0.9521822333335876\n",
      "Iter #5433344:  Learning rate = 0.000552:   Batch Loss = 0.132940, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21712565422058105, Accuracy = 0.9525299668312073\n",
      "Iter #5435392:  Learning rate = 0.000552:   Batch Loss = 0.114281, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21537967026233673, Accuracy = 0.9513128399848938\n",
      "Iter #5437440:  Learning rate = 0.000552:   Batch Loss = 0.126563, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2547621428966522, Accuracy = 0.9354894757270813\n",
      "Iter #5439488:  Learning rate = 0.000552:   Batch Loss = 0.148980, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22216886281967163, Accuracy = 0.9504433870315552\n",
      "Iter #5441536:  Learning rate = 0.000552:   Batch Loss = 0.129435, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2172604501247406, Accuracy = 0.9532255530357361\n",
      "Iter #5443584:  Learning rate = 0.000552:   Batch Loss = 0.099317, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23184585571289062, Accuracy = 0.9483568072319031\n",
      "Iter #5445632:  Learning rate = 0.000552:   Batch Loss = 0.136716, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.212171733379364, Accuracy = 0.9527038931846619\n",
      "Iter #5447680:  Learning rate = 0.000552:   Batch Loss = 0.122049, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23005393147468567, Accuracy = 0.9502695202827454\n",
      "Iter #5449728:  Learning rate = 0.000552:   Batch Loss = 0.122872, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.213708758354187, Accuracy = 0.9551382660865784\n",
      "Iter #5451776:  Learning rate = 0.000552:   Batch Loss = 0.119718, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24545039236545563, Accuracy = 0.9448791742324829\n",
      "Iter #5453824:  Learning rate = 0.000552:   Batch Loss = 0.110094, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2164033055305481, Accuracy = 0.9539210796356201\n",
      "Iter #5455872:  Learning rate = 0.000552:   Batch Loss = 0.120790, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2297230064868927, Accuracy = 0.9495739936828613\n",
      "Iter #5457920:  Learning rate = 0.000552:   Batch Loss = 0.111404, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21792753040790558, Accuracy = 0.9521822333335876\n",
      "Iter #5459968:  Learning rate = 0.000552:   Batch Loss = 0.129005, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2286815047264099, Accuracy = 0.9492262005805969\n",
      "Iter #5462016:  Learning rate = 0.000552:   Batch Loss = 0.123100, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23279109597206116, Accuracy = 0.9473134875297546\n",
      "Iter #5464064:  Learning rate = 0.000552:   Batch Loss = 0.138652, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23352602124214172, Accuracy = 0.9537471532821655\n",
      "Iter #5466112:  Learning rate = 0.000552:   Batch Loss = 0.117657, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22534319758415222, Accuracy = 0.9481829404830933\n",
      "Iter #5468160:  Learning rate = 0.000552:   Batch Loss = 0.161967, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24602177739143372, Accuracy = 0.9400104284286499\n",
      "Iter #5470208:  Learning rate = 0.000552:   Batch Loss = 0.147188, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2299131453037262, Accuracy = 0.9466179609298706\n",
      "Iter #5472256:  Learning rate = 0.000552:   Batch Loss = 0.126603, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2373679280281067, Accuracy = 0.9443575143814087\n",
      "Iter #5474304:  Learning rate = 0.000552:   Batch Loss = 0.112819, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2204439789056778, Accuracy = 0.9527038931846619\n",
      "Iter #5476352:  Learning rate = 0.000552:   Batch Loss = 0.114229, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2269906848669052, Accuracy = 0.9516605734825134\n",
      "Iter #5478400:  Learning rate = 0.000552:   Batch Loss = 0.131592, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.209994375705719, Accuracy = 0.9540949463844299\n",
      "Iter #5480448:  Learning rate = 0.000552:   Batch Loss = 0.119260, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23144596815109253, Accuracy = 0.9485306739807129\n",
      "Iter #5482496:  Learning rate = 0.000552:   Batch Loss = 0.107724, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21155975759029388, Accuracy = 0.9556598663330078\n",
      "Iter #5484544:  Learning rate = 0.000552:   Batch Loss = 0.120177, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2123979777097702, Accuracy = 0.9549643397331238\n",
      "Iter #5486592:  Learning rate = 0.000552:   Batch Loss = 0.104224, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21367591619491577, Accuracy = 0.9540949463844299\n",
      "Iter #5488640:  Learning rate = 0.000552:   Batch Loss = 0.105549, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20838004350662231, Accuracy = 0.9558337926864624\n",
      "Iter #5490688:  Learning rate = 0.000552:   Batch Loss = 0.130959, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21907396614551544, Accuracy = 0.9530516266822815\n",
      "Iter #5492736:  Learning rate = 0.000552:   Batch Loss = 0.113550, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22164052724838257, Accuracy = 0.954790472984314\n",
      "Iter #5494784:  Learning rate = 0.000552:   Batch Loss = 0.161758, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21305744349956512, Accuracy = 0.9565293192863464\n",
      "Iter #5496832:  Learning rate = 0.000552:   Batch Loss = 0.104895, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22236277163028717, Accuracy = 0.9528777599334717\n",
      "Iter #5498880:  Learning rate = 0.000552:   Batch Loss = 0.115469, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2187221646308899, Accuracy = 0.9535732865333557\n",
      "Iter #5500928:  Learning rate = 0.000530:   Batch Loss = 0.129225, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23178689181804657, Accuracy = 0.9478351473808289\n",
      "Iter #5502976:  Learning rate = 0.000530:   Batch Loss = 0.125880, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2180083692073822, Accuracy = 0.9513128399848938\n",
      "Iter #5505024:  Learning rate = 0.000530:   Batch Loss = 0.114249, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23665697872638702, Accuracy = 0.9492262005805969\n",
      "Iter #5507072:  Learning rate = 0.000530:   Batch Loss = 0.119349, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2234376072883606, Accuracy = 0.9452269077301025\n",
      "Iter #5509120:  Learning rate = 0.000530:   Batch Loss = 0.135121, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22462791204452515, Accuracy = 0.9507911801338196\n",
      "Iter #5511168:  Learning rate = 0.000530:   Batch Loss = 0.139020, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2218538522720337, Accuracy = 0.9514867067337036\n",
      "Iter #5513216:  Learning rate = 0.000530:   Batch Loss = 0.104992, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21343673765659332, Accuracy = 0.9537471532821655\n",
      "Iter #5515264:  Learning rate = 0.000530:   Batch Loss = 0.143649, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21432358026504517, Accuracy = 0.9549643397331238\n",
      "Iter #5517312:  Learning rate = 0.000530:   Batch Loss = 0.150624, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22898301482200623, Accuracy = 0.946270227432251\n",
      "Iter #5519360:  Learning rate = 0.000530:   Batch Loss = 0.125029, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22243615984916687, Accuracy = 0.9507911801338196\n",
      "Iter #5521408:  Learning rate = 0.000530:   Batch Loss = 0.136652, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22877547144889832, Accuracy = 0.9507911801338196\n",
      "Iter #5523456:  Learning rate = 0.000530:   Batch Loss = 0.107954, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2203533798456192, Accuracy = 0.9573987126350403\n",
      "Iter #5525504:  Learning rate = 0.000530:   Batch Loss = 0.125523, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2130047082901001, Accuracy = 0.9532255530357361\n",
      "Iter #5527552:  Learning rate = 0.000530:   Batch Loss = 0.164824, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22586748003959656, Accuracy = 0.946270227432251\n",
      "Iter #5529600:  Learning rate = 0.000530:   Batch Loss = 0.157558, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2162928283214569, Accuracy = 0.954790472984314\n",
      "Iter #5531648:  Learning rate = 0.000530:   Batch Loss = 0.132588, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2221793383359909, Accuracy = 0.9523561000823975\n",
      "Iter #5533696:  Learning rate = 0.000530:   Batch Loss = 0.119940, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22287309169769287, Accuracy = 0.949921727180481\n",
      "Iter #5535744:  Learning rate = 0.000530:   Batch Loss = 0.132944, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2123764008283615, Accuracy = 0.9546166062355042\n",
      "Iter #5537792:  Learning rate = 0.000530:   Batch Loss = 0.110606, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2204107791185379, Accuracy = 0.9544426798820496\n",
      "Iter #5539840:  Learning rate = 0.000530:   Batch Loss = 0.098165, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.223211407661438, Accuracy = 0.9500956535339355\n",
      "Iter #5541888:  Learning rate = 0.000530:   Batch Loss = 0.132570, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22957059741020203, Accuracy = 0.9514867067337036\n",
      "Iter #5543936:  Learning rate = 0.000530:   Batch Loss = 0.112952, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2165786325931549, Accuracy = 0.9539210796356201\n",
      "Iter #5545984:  Learning rate = 0.000530:   Batch Loss = 0.093952, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22636295855045319, Accuracy = 0.9518344402313232\n",
      "Iter #5548032:  Learning rate = 0.000530:   Batch Loss = 0.127416, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21007710695266724, Accuracy = 0.9532255530357361\n",
      "Iter #5550080:  Learning rate = 0.000530:   Batch Loss = 0.118798, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21875035762786865, Accuracy = 0.9553121328353882\n",
      "Iter #5552128:  Learning rate = 0.000530:   Batch Loss = 0.141167, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21679073572158813, Accuracy = 0.9546166062355042\n",
      "Iter #5554176:  Learning rate = 0.000530:   Batch Loss = 0.136484, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23156753182411194, Accuracy = 0.9511389136314392\n",
      "Iter #5556224:  Learning rate = 0.000530:   Batch Loss = 0.122859, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22141072154045105, Accuracy = 0.949921727180481\n",
      "Iter #5558272:  Learning rate = 0.000530:   Batch Loss = 0.102099, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2112111747264862, Accuracy = 0.9558337926864624\n",
      "Iter #5560320:  Learning rate = 0.000530:   Batch Loss = 0.140064, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22441259026527405, Accuracy = 0.9527038931846619\n",
      "Iter #5562368:  Learning rate = 0.000530:   Batch Loss = 0.118093, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.213076651096344, Accuracy = 0.9525299668312073\n",
      "Iter #5564416:  Learning rate = 0.000530:   Batch Loss = 0.126184, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21891391277313232, Accuracy = 0.9516605734825134\n",
      "Iter #5566464:  Learning rate = 0.000530:   Batch Loss = 0.114066, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22197866439819336, Accuracy = 0.9528777599334717\n",
      "Iter #5568512:  Learning rate = 0.000530:   Batch Loss = 0.130519, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21340787410736084, Accuracy = 0.9530516266822815\n",
      "Iter #5570560:  Learning rate = 0.000530:   Batch Loss = 0.139265, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22682029008865356, Accuracy = 0.9532255530357361\n",
      "Iter #5572608:  Learning rate = 0.000530:   Batch Loss = 0.143633, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21321049332618713, Accuracy = 0.9537471532821655\n",
      "Iter #5574656:  Learning rate = 0.000530:   Batch Loss = 0.112009, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22221234440803528, Accuracy = 0.9490523338317871\n",
      "Iter #5576704:  Learning rate = 0.000530:   Batch Loss = 0.152942, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20955465734004974, Accuracy = 0.9565293192863464\n",
      "Iter #5578752:  Learning rate = 0.000530:   Batch Loss = 0.128396, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22098873555660248, Accuracy = 0.9533994197845459\n",
      "Iter #5580800:  Learning rate = 0.000530:   Batch Loss = 0.114265, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20311065018177032, Accuracy = 0.9567031860351562\n",
      "Iter #5582848:  Learning rate = 0.000530:   Batch Loss = 0.102497, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21398687362670898, Accuracy = 0.9514867067337036\n",
      "Iter #5584896:  Learning rate = 0.000530:   Batch Loss = 0.127018, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21315565705299377, Accuracy = 0.9528777599334717\n",
      "Iter #5586944:  Learning rate = 0.000530:   Batch Loss = 0.151597, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2306661307811737, Accuracy = 0.9473134875297546\n",
      "Iter #5588992:  Learning rate = 0.000530:   Batch Loss = 0.110021, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21269305050373077, Accuracy = 0.955485999584198\n",
      "Iter #5591040:  Learning rate = 0.000530:   Batch Loss = 0.113594, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22333219647407532, Accuracy = 0.9530516266822815\n",
      "Iter #5593088:  Learning rate = 0.000530:   Batch Loss = 0.112453, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21330362558364868, Accuracy = 0.956181526184082\n",
      "Iter #5595136:  Learning rate = 0.000530:   Batch Loss = 0.108962, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21743740141391754, Accuracy = 0.9542688131332397\n",
      "Iter #5597184:  Learning rate = 0.000530:   Batch Loss = 0.094993, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2144116759300232, Accuracy = 0.9542688131332397\n",
      "Iter #5599232:  Learning rate = 0.000530:   Batch Loss = 0.111381, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23211705684661865, Accuracy = 0.9492262005805969\n",
      "Iter #5601280:  Learning rate = 0.000508:   Batch Loss = 0.147846, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22735783457756042, Accuracy = 0.9506173133850098\n",
      "Iter #5603328:  Learning rate = 0.000508:   Batch Loss = 0.114225, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23267745971679688, Accuracy = 0.9450530409812927\n",
      "Iter #5605376:  Learning rate = 0.000508:   Batch Loss = 0.118440, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2316587269306183, Accuracy = 0.9504433870315552\n",
      "Iter #5607424:  Learning rate = 0.000508:   Batch Loss = 0.134556, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21314696967601776, Accuracy = 0.9509650468826294\n",
      "Iter #5609472:  Learning rate = 0.000508:   Batch Loss = 0.120405, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23498943448066711, Accuracy = 0.9455747008323669\n",
      "Iter #5611520:  Learning rate = 0.000508:   Batch Loss = 0.160922, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22097010910511017, Accuracy = 0.9535732865333557\n",
      "Iter #5613568:  Learning rate = 0.000508:   Batch Loss = 0.132207, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22217223048210144, Accuracy = 0.9504433870315552\n",
      "Iter #5615616:  Learning rate = 0.000508:   Batch Loss = 0.135827, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22415710985660553, Accuracy = 0.9520083665847778\n",
      "Iter #5617664:  Learning rate = 0.000508:   Batch Loss = 0.098297, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21399086713790894, Accuracy = 0.9532255530357361\n",
      "Iter #5619712:  Learning rate = 0.000508:   Batch Loss = 0.123958, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22267583012580872, Accuracy = 0.9513128399848938\n",
      "Iter #5621760:  Learning rate = 0.000508:   Batch Loss = 0.115027, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21299465000629425, Accuracy = 0.9523561000823975\n",
      "Iter #5623808:  Learning rate = 0.000508:   Batch Loss = 0.132893, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22236277163028717, Accuracy = 0.9527038931846619\n",
      "Iter #5625856:  Learning rate = 0.000508:   Batch Loss = 0.137829, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2156221866607666, Accuracy = 0.9533994197845459\n",
      "Iter #5627904:  Learning rate = 0.000508:   Batch Loss = 0.105048, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2184460461139679, Accuracy = 0.9540949463844299\n",
      "Iter #5629952:  Learning rate = 0.000508:   Batch Loss = 0.106072, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23279687762260437, Accuracy = 0.9494001269340515\n",
      "Iter #5632000:  Learning rate = 0.000508:   Batch Loss = 0.098106, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21463456749916077, Accuracy = 0.9553121328353882\n",
      "Iter #5634048:  Learning rate = 0.000508:   Batch Loss = 0.128344, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2203947752714157, Accuracy = 0.9523561000823975\n",
      "Iter #5636096:  Learning rate = 0.000508:   Batch Loss = 0.115300, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2211376279592514, Accuracy = 0.9518344402313232\n",
      "Iter #5638144:  Learning rate = 0.000508:   Batch Loss = 0.109741, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23268219828605652, Accuracy = 0.9464440941810608\n",
      "Iter #5640192:  Learning rate = 0.000508:   Batch Loss = 0.121702, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23285141587257385, Accuracy = 0.9471396207809448\n",
      "Iter #5642240:  Learning rate = 0.000508:   Batch Loss = 0.129536, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23615065217018127, Accuracy = 0.9433141946792603\n",
      "Iter #5644288:  Learning rate = 0.000508:   Batch Loss = 0.164711, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2248525172472, Accuracy = 0.9535732865333557\n",
      "Iter #5646336:  Learning rate = 0.000508:   Batch Loss = 0.119831, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22834688425064087, Accuracy = 0.9509650468826294\n",
      "Iter #5648384:  Learning rate = 0.000508:   Batch Loss = 0.146781, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23603540658950806, Accuracy = 0.9481829404830933\n",
      "Iter #5650432:  Learning rate = 0.000508:   Batch Loss = 0.103012, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22215355932712555, Accuracy = 0.947661280632019\n",
      "Iter #5652480:  Learning rate = 0.000508:   Batch Loss = 0.128339, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2145422399044037, Accuracy = 0.9579203724861145\n",
      "Iter #5654528:  Learning rate = 0.000508:   Batch Loss = 0.124757, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20876216888427734, Accuracy = 0.9575725793838501\n",
      "Iter #5656576:  Learning rate = 0.000508:   Batch Loss = 0.124627, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24355456233024597, Accuracy = 0.9424448013305664\n",
      "Iter #5658624:  Learning rate = 0.000508:   Batch Loss = 0.112706, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23115654289722443, Accuracy = 0.9448791742324829\n",
      "Iter #5660672:  Learning rate = 0.000508:   Batch Loss = 0.123065, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2317160665988922, Accuracy = 0.9502695202827454\n",
      "Iter #5662720:  Learning rate = 0.000508:   Batch Loss = 0.127183, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.201055645942688, Accuracy = 0.9575725793838501\n",
      "Iter #5664768:  Learning rate = 0.000508:   Batch Loss = 0.120855, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21266669034957886, Accuracy = 0.954790472984314\n",
      "Iter #5666816:  Learning rate = 0.000508:   Batch Loss = 0.128772, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20654666423797607, Accuracy = 0.9587897658348083\n",
      "Iter #5668864:  Learning rate = 0.000508:   Batch Loss = 0.127327, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20557469129562378, Accuracy = 0.9568770527839661\n",
      "Iter #5670912:  Learning rate = 0.000508:   Batch Loss = 0.148334, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21528038382530212, Accuracy = 0.9575725793838501\n",
      "Iter #5672960:  Learning rate = 0.000508:   Batch Loss = 0.100997, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2237442433834076, Accuracy = 0.9528777599334717\n",
      "Iter #5675008:  Learning rate = 0.000508:   Batch Loss = 0.137369, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22160707414150238, Accuracy = 0.9525299668312073\n",
      "Iter #5677056:  Learning rate = 0.000508:   Batch Loss = 0.142726, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22536779940128326, Accuracy = 0.9488784670829773\n",
      "Iter #5679104:  Learning rate = 0.000508:   Batch Loss = 0.113089, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21595942974090576, Accuracy = 0.9549643397331238\n",
      "Iter #5681152:  Learning rate = 0.000508:   Batch Loss = 0.117923, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21782168745994568, Accuracy = 0.954790472984314\n",
      "Iter #5683200:  Learning rate = 0.000508:   Batch Loss = 0.126169, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23893432319164276, Accuracy = 0.9454007744789124\n",
      "Iter #5685248:  Learning rate = 0.000508:   Batch Loss = 0.126129, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21860450506210327, Accuracy = 0.9518344402313232\n",
      "Iter #5687296:  Learning rate = 0.000508:   Batch Loss = 0.139688, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2114720493555069, Accuracy = 0.9579203724861145\n",
      "Iter #5689344:  Learning rate = 0.000508:   Batch Loss = 0.115447, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21626466512680054, Accuracy = 0.956181526184082\n",
      "Iter #5691392:  Learning rate = 0.000508:   Batch Loss = 0.140218, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2196134477853775, Accuracy = 0.9544426798820496\n",
      "Iter #5693440:  Learning rate = 0.000508:   Batch Loss = 0.138840, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20405657589435577, Accuracy = 0.9570509195327759\n",
      "Iter #5695488:  Learning rate = 0.000508:   Batch Loss = 0.135984, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24224910140037537, Accuracy = 0.9443575143814087\n",
      "Iter #5697536:  Learning rate = 0.000508:   Batch Loss = 0.108532, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22081145644187927, Accuracy = 0.9514867067337036\n",
      "Iter #5699584:  Learning rate = 0.000508:   Batch Loss = 0.131465, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24464094638824463, Accuracy = 0.9433141946792603\n",
      "Iter #5701632:  Learning rate = 0.000488:   Batch Loss = 0.108927, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21659645438194275, Accuracy = 0.9513128399848938\n",
      "Iter #5703680:  Learning rate = 0.000488:   Batch Loss = 0.121956, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21474021673202515, Accuracy = 0.9523561000823975\n",
      "Iter #5705728:  Learning rate = 0.000488:   Batch Loss = 0.146694, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22906628251075745, Accuracy = 0.9504433870315552\n",
      "Iter #5707776:  Learning rate = 0.000488:   Batch Loss = 0.121956, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.215390145778656, Accuracy = 0.9549643397331238\n",
      "Iter #5709824:  Learning rate = 0.000488:   Batch Loss = 0.135518, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2227516770362854, Accuracy = 0.947661280632019\n",
      "Iter #5711872:  Learning rate = 0.000488:   Batch Loss = 0.109998, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2329341322183609, Accuracy = 0.9483568072319031\n",
      "Iter #5713920:  Learning rate = 0.000488:   Batch Loss = 0.118806, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23146413266658783, Accuracy = 0.9507911801338196\n",
      "Iter #5715968:  Learning rate = 0.000488:   Batch Loss = 0.152173, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22436335682868958, Accuracy = 0.9533994197845459\n",
      "Iter #5718016:  Learning rate = 0.000488:   Batch Loss = 0.141868, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22059451043605804, Accuracy = 0.9494001269340515\n",
      "Iter #5720064:  Learning rate = 0.000488:   Batch Loss = 0.106103, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2289128452539444, Accuracy = 0.9500956535339355\n",
      "Iter #5722112:  Learning rate = 0.000488:   Batch Loss = 0.123169, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22117407619953156, Accuracy = 0.9535732865333557\n",
      "Iter #5724160:  Learning rate = 0.000488:   Batch Loss = 0.116184, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2199202924966812, Accuracy = 0.9549643397331238\n",
      "Iter #5726208:  Learning rate = 0.000488:   Batch Loss = 0.141106, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21366778016090393, Accuracy = 0.9539210796356201\n",
      "Iter #5728256:  Learning rate = 0.000488:   Batch Loss = 0.146902, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21752290427684784, Accuracy = 0.954790472984314\n",
      "Iter #5730304:  Learning rate = 0.000488:   Batch Loss = 0.126604, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2059578150510788, Accuracy = 0.9537471532821655\n",
      "Iter #5732352:  Learning rate = 0.000488:   Batch Loss = 0.116789, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22239212691783905, Accuracy = 0.9495739936828613\n",
      "Iter #5734400:  Learning rate = 0.000488:   Batch Loss = 0.149805, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20948177576065063, Accuracy = 0.9560076594352722\n",
      "Iter #5736448:  Learning rate = 0.000488:   Batch Loss = 0.134298, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22657546401023865, Accuracy = 0.9523561000823975\n",
      "Iter #5738496:  Learning rate = 0.000488:   Batch Loss = 0.135663, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21374469995498657, Accuracy = 0.9514867067337036\n",
      "Iter #5740544:  Learning rate = 0.000488:   Batch Loss = 0.123401, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22440758347511292, Accuracy = 0.9504433870315552\n",
      "Iter #5742592:  Learning rate = 0.000488:   Batch Loss = 0.129014, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21234408020973206, Accuracy = 0.9546166062355042\n",
      "Iter #5744640:  Learning rate = 0.000488:   Batch Loss = 0.133076, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23248793184757233, Accuracy = 0.9511389136314392\n",
      "Iter #5746688:  Learning rate = 0.000488:   Batch Loss = 0.137736, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2313288152217865, Accuracy = 0.947661280632019\n",
      "Iter #5748736:  Learning rate = 0.000488:   Batch Loss = 0.125955, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2305850088596344, Accuracy = 0.9490523338317871\n",
      "Iter #5750784:  Learning rate = 0.000488:   Batch Loss = 0.130845, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2737981677055359, Accuracy = 0.9368805289268494\n",
      "Iter #5752832:  Learning rate = 0.000488:   Batch Loss = 0.134836, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22315698862075806, Accuracy = 0.9530516266822815\n",
      "Iter #5754880:  Learning rate = 0.000488:   Batch Loss = 0.102940, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2091638147830963, Accuracy = 0.9556598663330078\n",
      "Iter #5756928:  Learning rate = 0.000488:   Batch Loss = 0.119990, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2128697633743286, Accuracy = 0.9565293192863464\n",
      "Iter #5758976:  Learning rate = 0.000488:   Batch Loss = 0.116188, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2173902690410614, Accuracy = 0.9539210796356201\n",
      "Iter #5761024:  Learning rate = 0.000488:   Batch Loss = 0.121857, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22909492254257202, Accuracy = 0.9514867067337036\n",
      "Iter #5763072:  Learning rate = 0.000488:   Batch Loss = 0.113689, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2212105393409729, Accuracy = 0.9471396207809448\n",
      "Iter #5765120:  Learning rate = 0.000488:   Batch Loss = 0.127249, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2613399624824524, Accuracy = 0.933576762676239\n",
      "Iter #5767168:  Learning rate = 0.000488:   Batch Loss = 0.138404, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23119764029979706, Accuracy = 0.9487046003341675\n",
      "Iter #5769216:  Learning rate = 0.000488:   Batch Loss = 0.123920, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22243864834308624, Accuracy = 0.9539210796356201\n",
      "Iter #5771264:  Learning rate = 0.000488:   Batch Loss = 0.113838, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21610966324806213, Accuracy = 0.9537471532821655\n",
      "Iter #5773312:  Learning rate = 0.000488:   Batch Loss = 0.115369, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22601793706417084, Accuracy = 0.9495739936828613\n",
      "Iter #5775360:  Learning rate = 0.000488:   Batch Loss = 0.148842, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22386929392814636, Accuracy = 0.949921727180481\n",
      "Iter #5777408:  Learning rate = 0.000488:   Batch Loss = 0.131245, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21338751912117004, Accuracy = 0.9535732865333557\n",
      "Iter #5779456:  Learning rate = 0.000488:   Batch Loss = 0.134012, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2060861885547638, Accuracy = 0.9577465057373047\n",
      "Iter #5781504:  Learning rate = 0.000488:   Batch Loss = 0.152677, Accuracy = 0.95703125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23818638920783997, Accuracy = 0.9455747008323669\n",
      "Iter #5783552:  Learning rate = 0.000488:   Batch Loss = 0.120109, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2225000560283661, Accuracy = 0.9527038931846619\n",
      "Iter #5785600:  Learning rate = 0.000488:   Batch Loss = 0.135701, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23105141520500183, Accuracy = 0.9481829404830933\n",
      "Iter #5787648:  Learning rate = 0.000488:   Batch Loss = 0.130661, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23543378710746765, Accuracy = 0.9460963606834412\n",
      "Iter #5789696:  Learning rate = 0.000488:   Batch Loss = 0.110984, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23124179244041443, Accuracy = 0.9480090141296387\n",
      "Iter #5791744:  Learning rate = 0.000488:   Batch Loss = 0.108476, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20892374217510223, Accuracy = 0.959659218788147\n",
      "Iter #5793792:  Learning rate = 0.000488:   Batch Loss = 0.113443, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2286742925643921, Accuracy = 0.9497478604316711\n",
      "Iter #5795840:  Learning rate = 0.000488:   Batch Loss = 0.139960, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2139270156621933, Accuracy = 0.955485999584198\n",
      "Iter #5797888:  Learning rate = 0.000488:   Batch Loss = 0.123966, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20628848671913147, Accuracy = 0.9584420323371887\n",
      "Iter #5799936:  Learning rate = 0.000488:   Batch Loss = 0.132476, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2183537781238556, Accuracy = 0.9527038931846619\n",
      "Iter #5801984:  Learning rate = 0.000468:   Batch Loss = 0.110550, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22054734826087952, Accuracy = 0.9530516266822815\n",
      "Iter #5804032:  Learning rate = 0.000468:   Batch Loss = 0.146660, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22618913650512695, Accuracy = 0.9483568072319031\n",
      "Iter #5806080:  Learning rate = 0.000468:   Batch Loss = 0.126955, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21176253259181976, Accuracy = 0.9586158990859985\n",
      "Iter #5808128:  Learning rate = 0.000468:   Batch Loss = 0.131830, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22822651267051697, Accuracy = 0.9525299668312073\n",
      "Iter #5810176:  Learning rate = 0.000468:   Batch Loss = 0.121324, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21388183534145355, Accuracy = 0.9563553929328918\n",
      "Iter #5812224:  Learning rate = 0.000468:   Batch Loss = 0.115457, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21776261925697327, Accuracy = 0.9485306739807129\n",
      "Iter #5814272:  Learning rate = 0.000468:   Batch Loss = 0.138132, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2236880660057068, Accuracy = 0.9513128399848938\n",
      "Iter #5816320:  Learning rate = 0.000468:   Batch Loss = 0.097621, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22083288431167603, Accuracy = 0.954790472984314\n",
      "Iter #5818368:  Learning rate = 0.000468:   Batch Loss = 0.128271, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22453394532203674, Accuracy = 0.9530516266822815\n",
      "Iter #5820416:  Learning rate = 0.000468:   Batch Loss = 0.106373, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2310674488544464, Accuracy = 0.9487046003341675\n",
      "Iter #5822464:  Learning rate = 0.000468:   Batch Loss = 0.109745, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22101342678070068, Accuracy = 0.9516605734825134\n",
      "Iter #5824512:  Learning rate = 0.000468:   Batch Loss = 0.115905, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.227907195687294, Accuracy = 0.9504433870315552\n",
      "Iter #5826560:  Learning rate = 0.000468:   Batch Loss = 0.126265, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2282334566116333, Accuracy = 0.9521822333335876\n",
      "Iter #5828608:  Learning rate = 0.000468:   Batch Loss = 0.101838, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2153509557247162, Accuracy = 0.9542688131332397\n",
      "Iter #5830656:  Learning rate = 0.000468:   Batch Loss = 0.108265, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21645498275756836, Accuracy = 0.954790472984314\n",
      "Iter #5832704:  Learning rate = 0.000468:   Batch Loss = 0.118284, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24981725215911865, Accuracy = 0.9400104284286499\n",
      "Iter #5834752:  Learning rate = 0.000468:   Batch Loss = 0.111140, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23194095492362976, Accuracy = 0.9473134875297546\n",
      "Iter #5836800:  Learning rate = 0.000468:   Batch Loss = 0.110813, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23568382859230042, Accuracy = 0.946270227432251\n",
      "Iter #5838848:  Learning rate = 0.000468:   Batch Loss = 0.161500, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25057366490364075, Accuracy = 0.946270227432251\n",
      "Iter #5840896:  Learning rate = 0.000468:   Batch Loss = 0.119357, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2215108722448349, Accuracy = 0.9551382660865784\n",
      "Iter #5842944:  Learning rate = 0.000468:   Batch Loss = 0.146461, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2202332317829132, Accuracy = 0.9474874138832092\n",
      "Iter #5844992:  Learning rate = 0.000468:   Batch Loss = 0.111049, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21696865558624268, Accuracy = 0.954790472984314\n",
      "Iter #5847040:  Learning rate = 0.000468:   Batch Loss = 0.119424, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2046281397342682, Accuracy = 0.9582681059837341\n",
      "Iter #5849088:  Learning rate = 0.000468:   Batch Loss = 0.095833, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2305999994277954, Accuracy = 0.9488784670829773\n",
      "Iter #5851136:  Learning rate = 0.000468:   Batch Loss = 0.104748, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21557065844535828, Accuracy = 0.9528777599334717\n",
      "Iter #5853184:  Learning rate = 0.000468:   Batch Loss = 0.109918, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2248595654964447, Accuracy = 0.9528777599334717\n",
      "Iter #5855232:  Learning rate = 0.000468:   Batch Loss = 0.166058, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2295013666152954, Accuracy = 0.9502695202827454\n",
      "Iter #5857280:  Learning rate = 0.000468:   Batch Loss = 0.105311, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2074543684720993, Accuracy = 0.9589636325836182\n",
      "Iter #5859328:  Learning rate = 0.000468:   Batch Loss = 0.127685, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2492055594921112, Accuracy = 0.9473134875297546\n",
      "Iter #5861376:  Learning rate = 0.000468:   Batch Loss = 0.106850, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21248529851436615, Accuracy = 0.9553121328353882\n",
      "Iter #5863424:  Learning rate = 0.000468:   Batch Loss = 0.112278, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2516409754753113, Accuracy = 0.938445508480072\n",
      "Iter #5865472:  Learning rate = 0.000468:   Batch Loss = 0.112150, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23734356462955475, Accuracy = 0.9480090141296387\n",
      "Iter #5867520:  Learning rate = 0.000468:   Batch Loss = 0.154166, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2417241930961609, Accuracy = 0.9434880614280701\n",
      "Iter #5869568:  Learning rate = 0.000468:   Batch Loss = 0.156039, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22955045104026794, Accuracy = 0.9490523338317871\n",
      "Iter #5871616:  Learning rate = 0.000468:   Batch Loss = 0.131804, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21606005728244781, Accuracy = 0.9577465057373047\n",
      "Iter #5873664:  Learning rate = 0.000468:   Batch Loss = 0.141191, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2196764051914215, Accuracy = 0.9530516266822815\n",
      "Iter #5875712:  Learning rate = 0.000468:   Batch Loss = 0.112622, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22730301320552826, Accuracy = 0.9506173133850098\n",
      "Iter #5877760:  Learning rate = 0.000468:   Batch Loss = 0.155219, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2260018140077591, Accuracy = 0.9504433870315552\n",
      "Iter #5879808:  Learning rate = 0.000468:   Batch Loss = 0.115166, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21218380331993103, Accuracy = 0.9542688131332397\n",
      "Iter #5881856:  Learning rate = 0.000468:   Batch Loss = 0.117506, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23514187335968018, Accuracy = 0.9504433870315552\n",
      "Iter #5883904:  Learning rate = 0.000468:   Batch Loss = 0.112322, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21979594230651855, Accuracy = 0.9551382660865784\n",
      "Iter #5885952:  Learning rate = 0.000468:   Batch Loss = 0.108453, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21490666270256042, Accuracy = 0.9542688131332397\n",
      "Iter #5888000:  Learning rate = 0.000468:   Batch Loss = 0.120383, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21719440817832947, Accuracy = 0.9527038931846619\n",
      "Iter #5890048:  Learning rate = 0.000468:   Batch Loss = 0.131110, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22804021835327148, Accuracy = 0.9523561000823975\n",
      "Iter #5892096:  Learning rate = 0.000468:   Batch Loss = 0.116769, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22792011499404907, Accuracy = 0.9535732865333557\n",
      "Iter #5894144:  Learning rate = 0.000468:   Batch Loss = 0.118885, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21706536412239075, Accuracy = 0.9504433870315552\n",
      "Iter #5896192:  Learning rate = 0.000468:   Batch Loss = 0.123962, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22963929176330566, Accuracy = 0.9509650468826294\n",
      "Iter #5898240:  Learning rate = 0.000468:   Batch Loss = 0.127133, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2149939239025116, Accuracy = 0.9533994197845459\n",
      "Iter #5900288:  Learning rate = 0.000450:   Batch Loss = 0.112228, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2248672991991043, Accuracy = 0.9504433870315552\n",
      "Iter #5902336:  Learning rate = 0.000450:   Batch Loss = 0.102694, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21213042736053467, Accuracy = 0.9549643397331238\n",
      "Iter #5904384:  Learning rate = 0.000450:   Batch Loss = 0.102782, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21807801723480225, Accuracy = 0.9542688131332397\n",
      "Iter #5906432:  Learning rate = 0.000450:   Batch Loss = 0.120370, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22260451316833496, Accuracy = 0.9511389136314392\n",
      "Iter #5908480:  Learning rate = 0.000450:   Batch Loss = 0.092938, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21861021220684052, Accuracy = 0.9539210796356201\n",
      "Iter #5910528:  Learning rate = 0.000450:   Batch Loss = 0.105182, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22144007682800293, Accuracy = 0.9514867067337036\n",
      "Iter #5912576:  Learning rate = 0.000450:   Batch Loss = 0.155518, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.248835951089859, Accuracy = 0.9436619877815247\n",
      "Iter #5914624:  Learning rate = 0.000450:   Batch Loss = 0.132299, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22867511212825775, Accuracy = 0.9494001269340515\n",
      "Iter #5916672:  Learning rate = 0.000450:   Batch Loss = 0.108592, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22136002779006958, Accuracy = 0.9544426798820496\n",
      "Iter #5918720:  Learning rate = 0.000450:   Batch Loss = 0.109500, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2116132378578186, Accuracy = 0.9580942392349243\n",
      "Iter #5920768:  Learning rate = 0.000450:   Batch Loss = 0.100181, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21663428843021393, Accuracy = 0.9568770527839661\n",
      "Iter #5922816:  Learning rate = 0.000450:   Batch Loss = 0.118977, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20912957191467285, Accuracy = 0.9565293192863464\n",
      "Iter #5924864:  Learning rate = 0.000450:   Batch Loss = 0.110465, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22019359469413757, Accuracy = 0.9560076594352722\n",
      "Iter #5926912:  Learning rate = 0.000450:   Batch Loss = 0.135738, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21278446912765503, Accuracy = 0.9556598663330078\n",
      "Iter #5928960:  Learning rate = 0.000450:   Batch Loss = 0.103711, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21881386637687683, Accuracy = 0.9535732865333557\n",
      "Iter #5931008:  Learning rate = 0.000450:   Batch Loss = 0.104332, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21924243867397308, Accuracy = 0.9513128399848938\n",
      "Iter #5933056:  Learning rate = 0.000450:   Batch Loss = 0.123478, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21514880657196045, Accuracy = 0.9553121328353882\n",
      "Iter #5935104:  Learning rate = 0.000450:   Batch Loss = 0.100610, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21595808863639832, Accuracy = 0.9546166062355042\n",
      "Iter #5937152:  Learning rate = 0.000450:   Batch Loss = 0.112315, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21100175380706787, Accuracy = 0.955485999584198\n",
      "Iter #5939200:  Learning rate = 0.000450:   Batch Loss = 0.128432, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2110837996006012, Accuracy = 0.9580942392349243\n",
      "Iter #5941248:  Learning rate = 0.000450:   Batch Loss = 0.124132, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21003049612045288, Accuracy = 0.9573987126350403\n",
      "Iter #5943296:  Learning rate = 0.000450:   Batch Loss = 0.136498, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20835328102111816, Accuracy = 0.9580942392349243\n",
      "Iter #5945344:  Learning rate = 0.000450:   Batch Loss = 0.110454, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2118871808052063, Accuracy = 0.9573987126350403\n",
      "Iter #5947392:  Learning rate = 0.000450:   Batch Loss = 0.128026, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21979489922523499, Accuracy = 0.9540949463844299\n",
      "Iter #5949440:  Learning rate = 0.000450:   Batch Loss = 0.096411, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23080797493457794, Accuracy = 0.9513128399848938\n",
      "Iter #5951488:  Learning rate = 0.000450:   Batch Loss = 0.117644, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22106024622917175, Accuracy = 0.9516605734825134\n",
      "Iter #5953536:  Learning rate = 0.000450:   Batch Loss = 0.130599, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23187360167503357, Accuracy = 0.9511389136314392\n",
      "Iter #5955584:  Learning rate = 0.000450:   Batch Loss = 0.137155, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2098698616027832, Accuracy = 0.9560076594352722\n",
      "Iter #5957632:  Learning rate = 0.000450:   Batch Loss = 0.109999, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22725409269332886, Accuracy = 0.9466179609298706\n",
      "Iter #5959680:  Learning rate = 0.000450:   Batch Loss = 0.134108, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23233620822429657, Accuracy = 0.9506173133850098\n",
      "Iter #5961728:  Learning rate = 0.000450:   Batch Loss = 0.112638, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21743233501911163, Accuracy = 0.9523561000823975\n",
      "Iter #5963776:  Learning rate = 0.000450:   Batch Loss = 0.151472, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22355443239212036, Accuracy = 0.9537471532821655\n",
      "Iter #5965824:  Learning rate = 0.000450:   Batch Loss = 0.127289, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2065933346748352, Accuracy = 0.9575725793838501\n",
      "Iter #5967872:  Learning rate = 0.000450:   Batch Loss = 0.140632, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22219057381153107, Accuracy = 0.9525299668312073\n",
      "Iter #5969920:  Learning rate = 0.000450:   Batch Loss = 0.167542, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23277410864830017, Accuracy = 0.9495739936828613\n",
      "Iter #5971968:  Learning rate = 0.000450:   Batch Loss = 0.148484, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2290193736553192, Accuracy = 0.9487046003341675\n",
      "Iter #5974016:  Learning rate = 0.000450:   Batch Loss = 0.112865, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20759323239326477, Accuracy = 0.9544426798820496\n",
      "Iter #5976064:  Learning rate = 0.000450:   Batch Loss = 0.131516, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21988314390182495, Accuracy = 0.9560076594352722\n",
      "Iter #5978112:  Learning rate = 0.000450:   Batch Loss = 0.103814, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20676472783088684, Accuracy = 0.9601808190345764\n",
      "Iter #5980160:  Learning rate = 0.000450:   Batch Loss = 0.092105, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23190858960151672, Accuracy = 0.9492262005805969\n",
      "Iter #5982208:  Learning rate = 0.000450:   Batch Loss = 0.110479, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21666982769966125, Accuracy = 0.9532255530357361\n",
      "Iter #5984256:  Learning rate = 0.000450:   Batch Loss = 0.111577, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2147742211818695, Accuracy = 0.9558337926864624\n",
      "Iter #5986304:  Learning rate = 0.000450:   Batch Loss = 0.109875, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22219011187553406, Accuracy = 0.9488784670829773\n",
      "Iter #5988352:  Learning rate = 0.000450:   Batch Loss = 0.113930, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20751330256462097, Accuracy = 0.9587897658348083\n",
      "Iter #5990400:  Learning rate = 0.000450:   Batch Loss = 0.118864, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2167123407125473, Accuracy = 0.956181526184082\n",
      "Iter #5992448:  Learning rate = 0.000450:   Batch Loss = 0.114029, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2215835452079773, Accuracy = 0.9514867067337036\n",
      "Iter #5994496:  Learning rate = 0.000450:   Batch Loss = 0.110988, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20132103562355042, Accuracy = 0.9570509195327759\n",
      "Iter #5996544:  Learning rate = 0.000450:   Batch Loss = 0.123298, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22476831078529358, Accuracy = 0.9530516266822815\n",
      "Iter #5998592:  Learning rate = 0.000450:   Batch Loss = 0.107639, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2037460207939148, Accuracy = 0.9577465057373047\n",
      "Iter #6000640:  Learning rate = 0.000432:   Batch Loss = 0.129538, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21633023023605347, Accuracy = 0.9523561000823975\n",
      "Iter #6002688:  Learning rate = 0.000432:   Batch Loss = 0.130958, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21453335881233215, Accuracy = 0.9520083665847778\n",
      "Iter #6004736:  Learning rate = 0.000432:   Batch Loss = 0.107771, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2091766595840454, Accuracy = 0.9568770527839661\n",
      "Iter #6006784:  Learning rate = 0.000432:   Batch Loss = 0.104134, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.19632408022880554, Accuracy = 0.9580942392349243\n",
      "Iter #6008832:  Learning rate = 0.000432:   Batch Loss = 0.116534, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21037468314170837, Accuracy = 0.9560076594352722\n",
      "Iter #6010880:  Learning rate = 0.000432:   Batch Loss = 0.129662, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2184363454580307, Accuracy = 0.9528777599334717\n",
      "Iter #6012928:  Learning rate = 0.000432:   Batch Loss = 0.113646, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23098261654376984, Accuracy = 0.9509650468826294\n",
      "Iter #6014976:  Learning rate = 0.000432:   Batch Loss = 0.116514, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20948144793510437, Accuracy = 0.9575725793838501\n",
      "Iter #6017024:  Learning rate = 0.000432:   Batch Loss = 0.116902, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22319570183753967, Accuracy = 0.9539210796356201\n",
      "Iter #6019072:  Learning rate = 0.000432:   Batch Loss = 0.113058, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20859166979789734, Accuracy = 0.955485999584198\n",
      "Iter #6021120:  Learning rate = 0.000432:   Batch Loss = 0.111507, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21965590119361877, Accuracy = 0.9521822333335876\n",
      "Iter #6023168:  Learning rate = 0.000432:   Batch Loss = 0.117131, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2170150876045227, Accuracy = 0.9556598663330078\n",
      "Iter #6025216:  Learning rate = 0.000432:   Batch Loss = 0.115251, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21353884041309357, Accuracy = 0.9544426798820496\n",
      "Iter #6027264:  Learning rate = 0.000432:   Batch Loss = 0.098169, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20736807584762573, Accuracy = 0.9575725793838501\n",
      "Iter #6029312:  Learning rate = 0.000432:   Batch Loss = 0.113478, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2403191775083542, Accuracy = 0.9464440941810608\n",
      "Iter #6031360:  Learning rate = 0.000432:   Batch Loss = 0.107593, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.19917722046375275, Accuracy = 0.9593114256858826\n",
      "Iter #6033408:  Learning rate = 0.000432:   Batch Loss = 0.145740, Accuracy = 0.9609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2040780782699585, Accuracy = 0.9589636325836182\n",
      "Iter #6035456:  Learning rate = 0.000432:   Batch Loss = 0.127906, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2319885939359665, Accuracy = 0.9471396207809448\n",
      "Iter #6037504:  Learning rate = 0.000432:   Batch Loss = 0.102529, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2225896418094635, Accuracy = 0.9530516266822815\n",
      "Iter #6039552:  Learning rate = 0.000432:   Batch Loss = 0.130764, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21312612295150757, Accuracy = 0.9558337926864624\n",
      "Iter #6041600:  Learning rate = 0.000432:   Batch Loss = 0.107113, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22278431057929993, Accuracy = 0.9511389136314392\n",
      "Iter #6043648:  Learning rate = 0.000432:   Batch Loss = 0.116233, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21623168885707855, Accuracy = 0.9535732865333557\n",
      "Iter #6045696:  Learning rate = 0.000432:   Batch Loss = 0.129197, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22819313406944275, Accuracy = 0.9525299668312073\n",
      "Iter #6047744:  Learning rate = 0.000432:   Batch Loss = 0.121360, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2203734964132309, Accuracy = 0.9511389136314392\n",
      "Iter #6049792:  Learning rate = 0.000432:   Batch Loss = 0.153020, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2125076800584793, Accuracy = 0.9565293192863464\n",
      "Iter #6051840:  Learning rate = 0.000432:   Batch Loss = 0.098836, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2377757877111435, Accuracy = 0.9490523338317871\n",
      "Iter #6053888:  Learning rate = 0.000432:   Batch Loss = 0.120401, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21279987692832947, Accuracy = 0.9551382660865784\n",
      "Iter #6055936:  Learning rate = 0.000432:   Batch Loss = 0.099627, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21742109954357147, Accuracy = 0.9549643397331238\n",
      "Iter #6057984:  Learning rate = 0.000432:   Batch Loss = 0.112330, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20782318711280823, Accuracy = 0.9573987126350403\n",
      "Iter #6060032:  Learning rate = 0.000432:   Batch Loss = 0.126352, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2195415496826172, Accuracy = 0.9523561000823975\n",
      "Iter #6062080:  Learning rate = 0.000432:   Batch Loss = 0.102694, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21272289752960205, Accuracy = 0.9556598663330078\n",
      "Iter #6064128:  Learning rate = 0.000432:   Batch Loss = 0.113970, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21557559072971344, Accuracy = 0.9520083665847778\n",
      "Iter #6066176:  Learning rate = 0.000432:   Batch Loss = 0.127711, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21900013089179993, Accuracy = 0.9518344402313232\n",
      "Iter #6068224:  Learning rate = 0.000432:   Batch Loss = 0.112476, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2197960615158081, Accuracy = 0.9542688131332397\n",
      "Iter #6070272:  Learning rate = 0.000432:   Batch Loss = 0.119284, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21262648701667786, Accuracy = 0.956181526184082\n",
      "Iter #6072320:  Learning rate = 0.000432:   Batch Loss = 0.093035, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2147001177072525, Accuracy = 0.9570509195327759\n",
      "Iter #6074368:  Learning rate = 0.000432:   Batch Loss = 0.120345, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20542387664318085, Accuracy = 0.9587897658348083\n",
      "Iter #6076416:  Learning rate = 0.000432:   Batch Loss = 0.115695, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22559738159179688, Accuracy = 0.9532255530357361\n",
      "Iter #6078464:  Learning rate = 0.000432:   Batch Loss = 0.108983, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.216031014919281, Accuracy = 0.9540949463844299\n",
      "Iter #6080512:  Learning rate = 0.000432:   Batch Loss = 0.126334, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20771637558937073, Accuracy = 0.9601808190345764\n",
      "Iter #6082560:  Learning rate = 0.000432:   Batch Loss = 0.146499, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21552729606628418, Accuracy = 0.9558337926864624\n",
      "Iter #6084608:  Learning rate = 0.000432:   Batch Loss = 0.148118, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2235838621854782, Accuracy = 0.9546166062355042\n",
      "Iter #6086656:  Learning rate = 0.000432:   Batch Loss = 0.124206, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2088259756565094, Accuracy = 0.9560076594352722\n",
      "Iter #6088704:  Learning rate = 0.000432:   Batch Loss = 0.124760, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2163657248020172, Accuracy = 0.9560076594352722\n",
      "Iter #6090752:  Learning rate = 0.000432:   Batch Loss = 0.107701, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2070544958114624, Accuracy = 0.9584420323371887\n",
      "Iter #6092800:  Learning rate = 0.000432:   Batch Loss = 0.133086, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22747473418712616, Accuracy = 0.9502695202827454\n",
      "Iter #6094848:  Learning rate = 0.000432:   Batch Loss = 0.103963, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21618416905403137, Accuracy = 0.9544426798820496\n",
      "Iter #6096896:  Learning rate = 0.000432:   Batch Loss = 0.116163, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21003898978233337, Accuracy = 0.956181526184082\n",
      "Iter #6098944:  Learning rate = 0.000432:   Batch Loss = 0.103858, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21782918274402618, Accuracy = 0.9520083665847778\n",
      "Iter #6100992:  Learning rate = 0.000414:   Batch Loss = 0.140058, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.230350524187088, Accuracy = 0.9441836476325989\n",
      "Iter #6103040:  Learning rate = 0.000414:   Batch Loss = 0.118933, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24202659726142883, Accuracy = 0.9487046003341675\n",
      "Iter #6105088:  Learning rate = 0.000414:   Batch Loss = 0.099909, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20994389057159424, Accuracy = 0.9579203724861145\n",
      "Iter #6107136:  Learning rate = 0.000414:   Batch Loss = 0.111397, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2198646068572998, Accuracy = 0.9514867067337036\n",
      "Iter #6109184:  Learning rate = 0.000414:   Batch Loss = 0.101681, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22208908200263977, Accuracy = 0.9514867067337036\n",
      "Iter #6111232:  Learning rate = 0.000414:   Batch Loss = 0.107910, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2099829763174057, Accuracy = 0.9565293192863464\n",
      "Iter #6113280:  Learning rate = 0.000414:   Batch Loss = 0.104933, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20480617880821228, Accuracy = 0.9600069522857666\n",
      "Iter #6115328:  Learning rate = 0.000414:   Batch Loss = 0.108528, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21122094988822937, Accuracy = 0.9577465057373047\n",
      "Iter #6117376:  Learning rate = 0.000414:   Batch Loss = 0.118333, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20122069120407104, Accuracy = 0.9575725793838501\n",
      "Iter #6119424:  Learning rate = 0.000414:   Batch Loss = 0.129355, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21759414672851562, Accuracy = 0.9527038931846619\n",
      "Iter #6121472:  Learning rate = 0.000414:   Batch Loss = 0.125125, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20659755170345306, Accuracy = 0.9572248458862305\n",
      "Iter #6123520:  Learning rate = 0.000414:   Batch Loss = 0.108696, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21824172139167786, Accuracy = 0.9556598663330078\n",
      "Iter #6125568:  Learning rate = 0.000414:   Batch Loss = 0.098625, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2193642258644104, Accuracy = 0.9521822333335876\n",
      "Iter #6127616:  Learning rate = 0.000414:   Batch Loss = 0.139804, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2328357845544815, Accuracy = 0.9511389136314392\n",
      "Iter #6129664:  Learning rate = 0.000414:   Batch Loss = 0.134523, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22892603278160095, Accuracy = 0.9507911801338196\n",
      "Iter #6131712:  Learning rate = 0.000414:   Batch Loss = 0.112631, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2183084934949875, Accuracy = 0.9511389136314392\n",
      "Iter #6133760:  Learning rate = 0.000414:   Batch Loss = 0.109580, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24581116437911987, Accuracy = 0.9464440941810608\n",
      "Iter #6135808:  Learning rate = 0.000414:   Batch Loss = 0.116250, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21188345551490784, Accuracy = 0.9527038931846619\n",
      "Iter #6137856:  Learning rate = 0.000414:   Batch Loss = 0.119615, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2069685161113739, Accuracy = 0.9553121328353882\n",
      "Iter #6139904:  Learning rate = 0.000414:   Batch Loss = 0.110312, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2186397910118103, Accuracy = 0.9563553929328918\n",
      "Iter #6141952:  Learning rate = 0.000414:   Batch Loss = 0.134301, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20813214778900146, Accuracy = 0.9584420323371887\n",
      "Iter #6144000:  Learning rate = 0.000414:   Batch Loss = 0.093105, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2088349312543869, Accuracy = 0.9570509195327759\n",
      "Iter #6146048:  Learning rate = 0.000414:   Batch Loss = 0.114990, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21503788232803345, Accuracy = 0.9560076594352722\n",
      "Iter #6148096:  Learning rate = 0.000414:   Batch Loss = 0.113600, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22114191949367523, Accuracy = 0.9532255530357361\n",
      "Iter #6150144:  Learning rate = 0.000414:   Batch Loss = 0.108745, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20895791053771973, Accuracy = 0.9558337926864624\n",
      "Iter #6152192:  Learning rate = 0.000414:   Batch Loss = 0.106791, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22649149596691132, Accuracy = 0.9520083665847778\n",
      "Iter #6154240:  Learning rate = 0.000414:   Batch Loss = 0.098867, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21754804253578186, Accuracy = 0.9537471532821655\n",
      "Iter #6156288:  Learning rate = 0.000414:   Batch Loss = 0.107283, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21058085560798645, Accuracy = 0.9572248458862305\n",
      "Iter #6158336:  Learning rate = 0.000414:   Batch Loss = 0.111439, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2308703511953354, Accuracy = 0.949921727180481\n",
      "Iter #6160384:  Learning rate = 0.000414:   Batch Loss = 0.118207, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2174319624900818, Accuracy = 0.9528777599334717\n",
      "Iter #6162432:  Learning rate = 0.000414:   Batch Loss = 0.118258, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21023717522621155, Accuracy = 0.9567031860351562\n",
      "Iter #6164480:  Learning rate = 0.000414:   Batch Loss = 0.124728, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21594113111495972, Accuracy = 0.954790472984314\n",
      "Iter #6166528:  Learning rate = 0.000414:   Batch Loss = 0.114522, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20945271849632263, Accuracy = 0.9575725793838501\n",
      "Iter #6168576:  Learning rate = 0.000414:   Batch Loss = 0.108929, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22095301747322083, Accuracy = 0.9507911801338196\n",
      "Iter #6170624:  Learning rate = 0.000414:   Batch Loss = 0.113061, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21908393502235413, Accuracy = 0.9530516266822815\n",
      "Iter #6172672:  Learning rate = 0.000414:   Batch Loss = 0.114900, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21734637022018433, Accuracy = 0.9546166062355042\n",
      "Iter #6174720:  Learning rate = 0.000414:   Batch Loss = 0.111172, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22578153014183044, Accuracy = 0.9490523338317871\n",
      "Iter #6176768:  Learning rate = 0.000414:   Batch Loss = 0.109692, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2220786213874817, Accuracy = 0.9520083665847778\n",
      "Iter #6178816:  Learning rate = 0.000414:   Batch Loss = 0.133143, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2259405255317688, Accuracy = 0.9490523338317871\n",
      "Iter #6180864:  Learning rate = 0.000414:   Batch Loss = 0.095201, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21622303128242493, Accuracy = 0.9528777599334717\n",
      "Iter #6182912:  Learning rate = 0.000414:   Batch Loss = 0.119190, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21006610989570618, Accuracy = 0.9584420323371887\n",
      "Iter #6184960:  Learning rate = 0.000414:   Batch Loss = 0.114117, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20595954358577728, Accuracy = 0.9546166062355042\n",
      "Iter #6187008:  Learning rate = 0.000414:   Batch Loss = 0.103908, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23221880197525024, Accuracy = 0.9497478604316711\n",
      "Iter #6189056:  Learning rate = 0.000414:   Batch Loss = 0.114450, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22545203566551208, Accuracy = 0.946270227432251\n",
      "Iter #6191104:  Learning rate = 0.000414:   Batch Loss = 0.118027, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21313557028770447, Accuracy = 0.9546166062355042\n",
      "Iter #6193152:  Learning rate = 0.000414:   Batch Loss = 0.098110, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22091048955917358, Accuracy = 0.9533994197845459\n",
      "Iter #6195200:  Learning rate = 0.000414:   Batch Loss = 0.138255, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24617378413677216, Accuracy = 0.9459224343299866\n",
      "Iter #6197248:  Learning rate = 0.000414:   Batch Loss = 0.118612, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21162772178649902, Accuracy = 0.9553121328353882\n",
      "Iter #6199296:  Learning rate = 0.000414:   Batch Loss = 0.095162, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21324369311332703, Accuracy = 0.9537471532821655\n",
      "Iter #6201344:  Learning rate = 0.000398:   Batch Loss = 0.106143, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2243756651878357, Accuracy = 0.9511389136314392\n",
      "Iter #6203392:  Learning rate = 0.000398:   Batch Loss = 0.111025, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2076566517353058, Accuracy = 0.956181526184082\n",
      "Iter #6205440:  Learning rate = 0.000398:   Batch Loss = 0.131346, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21577244997024536, Accuracy = 0.9546166062355042\n",
      "Iter #6207488:  Learning rate = 0.000398:   Batch Loss = 0.113401, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20997576415538788, Accuracy = 0.9567031860351562\n",
      "Iter #6209536:  Learning rate = 0.000398:   Batch Loss = 0.112747, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2089921534061432, Accuracy = 0.9558337926864624\n",
      "Iter #6211584:  Learning rate = 0.000398:   Batch Loss = 0.118639, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23886951804161072, Accuracy = 0.9473134875297546\n",
      "Iter #6213632:  Learning rate = 0.000398:   Batch Loss = 0.114842, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21712306141853333, Accuracy = 0.9516605734825134\n",
      "Iter #6215680:  Learning rate = 0.000398:   Batch Loss = 0.105486, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22224032878875732, Accuracy = 0.9511389136314392\n",
      "Iter #6217728:  Learning rate = 0.000398:   Batch Loss = 0.103631, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20660671591758728, Accuracy = 0.9560076594352722\n",
      "Iter #6219776:  Learning rate = 0.000398:   Batch Loss = 0.102171, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21552595496177673, Accuracy = 0.9544426798820496\n",
      "Iter #6221824:  Learning rate = 0.000398:   Batch Loss = 0.101426, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21079501509666443, Accuracy = 0.9542688131332397\n",
      "Iter #6223872:  Learning rate = 0.000398:   Batch Loss = 0.103280, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21362324059009552, Accuracy = 0.9567031860351562\n",
      "Iter #6225920:  Learning rate = 0.000398:   Batch Loss = 0.110803, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21736298501491547, Accuracy = 0.9553121328353882\n",
      "Iter #6227968:  Learning rate = 0.000398:   Batch Loss = 0.118567, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2071179747581482, Accuracy = 0.9551382660865784\n",
      "Iter #6230016:  Learning rate = 0.000398:   Batch Loss = 0.114366, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21975205838680267, Accuracy = 0.9525299668312073\n",
      "Iter #6232064:  Learning rate = 0.000398:   Batch Loss = 0.123710, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.217562735080719, Accuracy = 0.9546166062355042\n",
      "Iter #6234112:  Learning rate = 0.000398:   Batch Loss = 0.132165, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22248445451259613, Accuracy = 0.9521822333335876\n",
      "Iter #6236160:  Learning rate = 0.000398:   Batch Loss = 0.124562, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2081572264432907, Accuracy = 0.9563553929328918\n",
      "Iter #6238208:  Learning rate = 0.000398:   Batch Loss = 0.119061, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2212740033864975, Accuracy = 0.9533994197845459\n",
      "Iter #6240256:  Learning rate = 0.000398:   Batch Loss = 0.104191, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2117725908756256, Accuracy = 0.9535732865333557\n",
      "Iter #6242304:  Learning rate = 0.000398:   Batch Loss = 0.123005, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21020323038101196, Accuracy = 0.9589636325836182\n",
      "Iter #6244352:  Learning rate = 0.000398:   Batch Loss = 0.111495, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20614036917686462, Accuracy = 0.9565293192863464\n",
      "Iter #6246400:  Learning rate = 0.000398:   Batch Loss = 0.115199, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22316429018974304, Accuracy = 0.9504433870315552\n",
      "Iter #6248448:  Learning rate = 0.000398:   Batch Loss = 0.116108, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21458092331886292, Accuracy = 0.9542688131332397\n",
      "Iter #6250496:  Learning rate = 0.000398:   Batch Loss = 0.121376, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22150705754756927, Accuracy = 0.9511389136314392\n",
      "Iter #6252544:  Learning rate = 0.000398:   Batch Loss = 0.112782, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2120092809200287, Accuracy = 0.9553121328353882\n",
      "Iter #6254592:  Learning rate = 0.000398:   Batch Loss = 0.116255, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2046494483947754, Accuracy = 0.9568770527839661\n",
      "Iter #6256640:  Learning rate = 0.000398:   Batch Loss = 0.127428, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21439751982688904, Accuracy = 0.9539210796356201\n",
      "Iter #6258688:  Learning rate = 0.000398:   Batch Loss = 0.102063, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21335890889167786, Accuracy = 0.9575725793838501\n",
      "Iter #6260736:  Learning rate = 0.000398:   Batch Loss = 0.124133, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2077200710773468, Accuracy = 0.9579203724861145\n",
      "Iter #6262784:  Learning rate = 0.000398:   Batch Loss = 0.112537, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20570683479309082, Accuracy = 0.9584420323371887\n",
      "Iter #6264832:  Learning rate = 0.000398:   Batch Loss = 0.120641, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2130928635597229, Accuracy = 0.9575725793838501\n",
      "Iter #6266880:  Learning rate = 0.000398:   Batch Loss = 0.102474, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24266928434371948, Accuracy = 0.9474874138832092\n",
      "Iter #6268928:  Learning rate = 0.000398:   Batch Loss = 0.112408, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21612873673439026, Accuracy = 0.9507911801338196\n",
      "Iter #6270976:  Learning rate = 0.000398:   Batch Loss = 0.096140, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2112542688846588, Accuracy = 0.955485999584198\n",
      "Iter #6273024:  Learning rate = 0.000398:   Batch Loss = 0.130415, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24427443742752075, Accuracy = 0.9445313811302185\n",
      "Iter #6275072:  Learning rate = 0.000398:   Batch Loss = 0.138698, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24935781955718994, Accuracy = 0.947661280632019\n",
      "Iter #6277120:  Learning rate = 0.000398:   Batch Loss = 0.117567, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2187218964099884, Accuracy = 0.9518344402313232\n",
      "Iter #6279168:  Learning rate = 0.000398:   Batch Loss = 0.115242, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20114575326442719, Accuracy = 0.9587897658348083\n",
      "Iter #6281216:  Learning rate = 0.000398:   Batch Loss = 0.111970, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2090797871351242, Accuracy = 0.9580942392349243\n",
      "Iter #6283264:  Learning rate = 0.000398:   Batch Loss = 0.091942, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20369574427604675, Accuracy = 0.9579203724861145\n",
      "Iter #6285312:  Learning rate = 0.000398:   Batch Loss = 0.129973, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2023760974407196, Accuracy = 0.9589636325836182\n",
      "Iter #6287360:  Learning rate = 0.000398:   Batch Loss = 0.108058, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2131766378879547, Accuracy = 0.9551382660865784\n",
      "Iter #6289408:  Learning rate = 0.000398:   Batch Loss = 0.115356, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20937085151672363, Accuracy = 0.955485999584198\n",
      "Iter #6291456:  Learning rate = 0.000398:   Batch Loss = 0.101480, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21080344915390015, Accuracy = 0.9575725793838501\n",
      "Iter #6293504:  Learning rate = 0.000398:   Batch Loss = 0.120544, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2227509319782257, Accuracy = 0.9474874138832092\n",
      "Iter #6295552:  Learning rate = 0.000398:   Batch Loss = 0.156306, Accuracy = 0.96484375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20555055141448975, Accuracy = 0.959659218788147\n",
      "Iter #6297600:  Learning rate = 0.000398:   Batch Loss = 0.106228, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21225374937057495, Accuracy = 0.9528777599334717\n",
      "Iter #6299648:  Learning rate = 0.000398:   Batch Loss = 0.105321, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22493255138397217, Accuracy = 0.9528777599334717\n",
      "Iter #6301696:  Learning rate = 0.000382:   Batch Loss = 0.129095, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20397284626960754, Accuracy = 0.9539210796356201\n",
      "Iter #6303744:  Learning rate = 0.000382:   Batch Loss = 0.125210, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2220354974269867, Accuracy = 0.9539210796356201\n",
      "Iter #6305792:  Learning rate = 0.000382:   Batch Loss = 0.115269, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.205430269241333, Accuracy = 0.9579203724861145\n",
      "Iter #6307840:  Learning rate = 0.000382:   Batch Loss = 0.109342, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21221502125263214, Accuracy = 0.9558337926864624\n",
      "Iter #6309888:  Learning rate = 0.000382:   Batch Loss = 0.097892, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21118102967739105, Accuracy = 0.9560076594352722\n",
      "Iter #6311936:  Learning rate = 0.000382:   Batch Loss = 0.146634, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2253255844116211, Accuracy = 0.9513128399848938\n",
      "Iter #6313984:  Learning rate = 0.000382:   Batch Loss = 0.105799, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2146243155002594, Accuracy = 0.9556598663330078\n",
      "Iter #6316032:  Learning rate = 0.000382:   Batch Loss = 0.131829, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2264910638332367, Accuracy = 0.9487046003341675\n",
      "Iter #6318080:  Learning rate = 0.000382:   Batch Loss = 0.160151, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23221564292907715, Accuracy = 0.9466179609298706\n",
      "Iter #6320128:  Learning rate = 0.000382:   Batch Loss = 0.102548, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22167053818702698, Accuracy = 0.9513128399848938\n",
      "Iter #6322176:  Learning rate = 0.000382:   Batch Loss = 0.111853, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20939010381698608, Accuracy = 0.9567031860351562\n",
      "Iter #6324224:  Learning rate = 0.000382:   Batch Loss = 0.123534, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21491402387619019, Accuracy = 0.9540949463844299\n",
      "Iter #6326272:  Learning rate = 0.000382:   Batch Loss = 0.114446, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2080496847629547, Accuracy = 0.9573987126350403\n",
      "Iter #6328320:  Learning rate = 0.000382:   Batch Loss = 0.116868, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2086939960718155, Accuracy = 0.9539210796356201\n",
      "Iter #6330368:  Learning rate = 0.000382:   Batch Loss = 0.144102, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23112821578979492, Accuracy = 0.9497478604316711\n",
      "Iter #6332416:  Learning rate = 0.000382:   Batch Loss = 0.127859, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20202749967575073, Accuracy = 0.9600069522857666\n",
      "Iter #6334464:  Learning rate = 0.000382:   Batch Loss = 0.097217, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21528691053390503, Accuracy = 0.954790472984314\n",
      "Iter #6336512:  Learning rate = 0.000382:   Batch Loss = 0.116090, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2054932564496994, Accuracy = 0.9573987126350403\n",
      "Iter #6338560:  Learning rate = 0.000382:   Batch Loss = 0.104990, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21274685859680176, Accuracy = 0.956181526184082\n",
      "Iter #6340608:  Learning rate = 0.000382:   Batch Loss = 0.112408, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22121542692184448, Accuracy = 0.954790472984314\n",
      "Iter #6342656:  Learning rate = 0.000382:   Batch Loss = 0.104005, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21572577953338623, Accuracy = 0.9537471532821655\n",
      "Iter #6344704:  Learning rate = 0.000382:   Batch Loss = 0.108264, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21081173419952393, Accuracy = 0.9568770527839661\n",
      "Iter #6346752:  Learning rate = 0.000382:   Batch Loss = 0.094941, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21334415674209595, Accuracy = 0.9573987126350403\n",
      "Iter #6348800:  Learning rate = 0.000382:   Batch Loss = 0.111093, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2195422649383545, Accuracy = 0.9544426798820496\n",
      "Iter #6350848:  Learning rate = 0.000382:   Batch Loss = 0.103892, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2184109389781952, Accuracy = 0.9507911801338196\n",
      "Iter #6352896:  Learning rate = 0.000382:   Batch Loss = 0.115571, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20720955729484558, Accuracy = 0.9591375589370728\n",
      "Iter #6354944:  Learning rate = 0.000382:   Batch Loss = 0.127674, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2081383764743805, Accuracy = 0.9563553929328918\n",
      "Iter #6356992:  Learning rate = 0.000382:   Batch Loss = 0.109600, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20722165703773499, Accuracy = 0.9556598663330078\n",
      "Iter #6359040:  Learning rate = 0.000382:   Batch Loss = 0.108865, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22174926102161407, Accuracy = 0.9535732865333557\n",
      "Iter #6361088:  Learning rate = 0.000382:   Batch Loss = 0.095023, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21693271398544312, Accuracy = 0.9528777599334717\n",
      "Iter #6363136:  Learning rate = 0.000382:   Batch Loss = 0.098596, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21474093198776245, Accuracy = 0.9533994197845459\n",
      "Iter #6365184:  Learning rate = 0.000382:   Batch Loss = 0.106046, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21946609020233154, Accuracy = 0.9537471532821655\n",
      "Iter #6367232:  Learning rate = 0.000382:   Batch Loss = 0.094006, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2222318947315216, Accuracy = 0.9513128399848938\n",
      "Iter #6369280:  Learning rate = 0.000382:   Batch Loss = 0.104476, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2153272181749344, Accuracy = 0.9544426798820496\n",
      "Iter #6371328:  Learning rate = 0.000382:   Batch Loss = 0.121580, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21692422032356262, Accuracy = 0.9537471532821655\n",
      "Iter #6373376:  Learning rate = 0.000382:   Batch Loss = 0.111609, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21418032050132751, Accuracy = 0.954790472984314\n",
      "Iter #6375424:  Learning rate = 0.000382:   Batch Loss = 0.114830, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21132305264472961, Accuracy = 0.9572248458862305\n",
      "Iter #6377472:  Learning rate = 0.000382:   Batch Loss = 0.099425, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23247192800045013, Accuracy = 0.9518344402313232\n",
      "Iter #6379520:  Learning rate = 0.000382:   Batch Loss = 0.109390, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20864778757095337, Accuracy = 0.9563553929328918\n",
      "Iter #6381568:  Learning rate = 0.000382:   Batch Loss = 0.102430, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2234269380569458, Accuracy = 0.9525299668312073\n",
      "Iter #6383616:  Learning rate = 0.000382:   Batch Loss = 0.112189, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21481898427009583, Accuracy = 0.9535732865333557\n",
      "Iter #6385664:  Learning rate = 0.000382:   Batch Loss = 0.119626, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23672166466712952, Accuracy = 0.9506173133850098\n",
      "Iter #6387712:  Learning rate = 0.000382:   Batch Loss = 0.131082, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2245161533355713, Accuracy = 0.9464440941810608\n",
      "Iter #6389760:  Learning rate = 0.000382:   Batch Loss = 0.144868, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22870972752571106, Accuracy = 0.9474874138832092\n",
      "Iter #6391808:  Learning rate = 0.000382:   Batch Loss = 0.111862, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20574501156806946, Accuracy = 0.9568770527839661\n",
      "Iter #6393856:  Learning rate = 0.000382:   Batch Loss = 0.114204, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21256083250045776, Accuracy = 0.9556598663330078\n",
      "Iter #6395904:  Learning rate = 0.000382:   Batch Loss = 0.146603, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20682795345783234, Accuracy = 0.955485999584198\n",
      "Iter #6397952:  Learning rate = 0.000382:   Batch Loss = 0.115657, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2122061848640442, Accuracy = 0.9556598663330078\n",
      "Iter #6400000:  Learning rate = 0.000367:   Batch Loss = 0.099892, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20590871572494507, Accuracy = 0.9556598663330078\n",
      "Iter #6402048:  Learning rate = 0.000367:   Batch Loss = 0.116032, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.215040385723114, Accuracy = 0.9563553929328918\n",
      "Iter #6404096:  Learning rate = 0.000367:   Batch Loss = 0.108146, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21239489316940308, Accuracy = 0.9558337926864624\n",
      "Iter #6406144:  Learning rate = 0.000367:   Batch Loss = 0.112000, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20897972583770752, Accuracy = 0.9577465057373047\n",
      "Iter #6408192:  Learning rate = 0.000367:   Batch Loss = 0.124337, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22330838441848755, Accuracy = 0.9537471532821655\n",
      "Iter #6410240:  Learning rate = 0.000367:   Batch Loss = 0.108970, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20982414484024048, Accuracy = 0.9579203724861145\n",
      "Iter #6412288:  Learning rate = 0.000367:   Batch Loss = 0.093651, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21901506185531616, Accuracy = 0.9542688131332397\n",
      "Iter #6414336:  Learning rate = 0.000367:   Batch Loss = 0.121200, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21322673559188843, Accuracy = 0.954790472984314\n",
      "Iter #6416384:  Learning rate = 0.000367:   Batch Loss = 0.101841, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2134419083595276, Accuracy = 0.9568770527839661\n",
      "Iter #6418432:  Learning rate = 0.000367:   Batch Loss = 0.120960, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2176704704761505, Accuracy = 0.9579203724861145\n",
      "Iter #6420480:  Learning rate = 0.000367:   Batch Loss = 0.104181, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21504224836826324, Accuracy = 0.955485999584198\n",
      "Iter #6422528:  Learning rate = 0.000367:   Batch Loss = 0.113115, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2230011224746704, Accuracy = 0.9528777599334717\n",
      "Iter #6424576:  Learning rate = 0.000367:   Batch Loss = 0.098233, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21435484290122986, Accuracy = 0.9558337926864624\n",
      "Iter #6426624:  Learning rate = 0.000367:   Batch Loss = 0.122052, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2143954634666443, Accuracy = 0.9530516266822815\n",
      "Iter #6428672:  Learning rate = 0.000367:   Batch Loss = 0.106634, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23013965785503387, Accuracy = 0.9521822333335876\n",
      "Iter #6430720:  Learning rate = 0.000367:   Batch Loss = 0.138915, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2056945264339447, Accuracy = 0.9601808190345764\n",
      "Iter #6432768:  Learning rate = 0.000367:   Batch Loss = 0.098047, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21015781164169312, Accuracy = 0.9575725793838501\n",
      "Iter #6434816:  Learning rate = 0.000367:   Batch Loss = 0.107339, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20071133971214294, Accuracy = 0.959659218788147\n",
      "Iter #6436864:  Learning rate = 0.000367:   Batch Loss = 0.114307, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20824569463729858, Accuracy = 0.9546166062355042\n",
      "Iter #6438912:  Learning rate = 0.000367:   Batch Loss = 0.097005, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22345343232154846, Accuracy = 0.9527038931846619\n",
      "Iter #6440960:  Learning rate = 0.000367:   Batch Loss = 0.117115, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2016931176185608, Accuracy = 0.9584420323371887\n",
      "Iter #6443008:  Learning rate = 0.000367:   Batch Loss = 0.106878, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22564715147018433, Accuracy = 0.9494001269340515\n",
      "Iter #6445056:  Learning rate = 0.000367:   Batch Loss = 0.111120, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2089601308107376, Accuracy = 0.9551382660865784\n",
      "Iter #6447104:  Learning rate = 0.000367:   Batch Loss = 0.103255, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2279907912015915, Accuracy = 0.9532255530357361\n",
      "Iter #6449152:  Learning rate = 0.000367:   Batch Loss = 0.118341, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2123616337776184, Accuracy = 0.955485999584198\n",
      "Iter #6451200:  Learning rate = 0.000367:   Batch Loss = 0.104075, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21968254446983337, Accuracy = 0.9537471532821655\n",
      "Iter #6453248:  Learning rate = 0.000367:   Batch Loss = 0.108017, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20757991075515747, Accuracy = 0.960354745388031\n",
      "Iter #6455296:  Learning rate = 0.000367:   Batch Loss = 0.101875, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20551717281341553, Accuracy = 0.9573987126350403\n",
      "Iter #6457344:  Learning rate = 0.000367:   Batch Loss = 0.084874, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21027979254722595, Accuracy = 0.9563553929328918\n",
      "Iter #6459392:  Learning rate = 0.000367:   Batch Loss = 0.099996, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21452000737190247, Accuracy = 0.9572248458862305\n",
      "Iter #6461440:  Learning rate = 0.000367:   Batch Loss = 0.133007, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22402703762054443, Accuracy = 0.9497478604316711\n",
      "Iter #6463488:  Learning rate = 0.000367:   Batch Loss = 0.126924, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2189275324344635, Accuracy = 0.9540949463844299\n",
      "Iter #6465536:  Learning rate = 0.000367:   Batch Loss = 0.111205, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.19775670766830444, Accuracy = 0.9594852924346924\n",
      "Iter #6467584:  Learning rate = 0.000367:   Batch Loss = 0.102764, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21380038559436798, Accuracy = 0.9551382660865784\n",
      "Iter #6469632:  Learning rate = 0.000367:   Batch Loss = 0.106471, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2064458131790161, Accuracy = 0.9570509195327759\n",
      "Iter #6471680:  Learning rate = 0.000367:   Batch Loss = 0.090874, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20526652038097382, Accuracy = 0.9600069522857666\n",
      "Iter #6473728:  Learning rate = 0.000367:   Batch Loss = 0.106590, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21947172284126282, Accuracy = 0.9530516266822815\n",
      "Iter #6475776:  Learning rate = 0.000367:   Batch Loss = 0.116047, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21027427911758423, Accuracy = 0.959659218788147\n",
      "Iter #6477824:  Learning rate = 0.000367:   Batch Loss = 0.102343, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21744883060455322, Accuracy = 0.9539210796356201\n",
      "Iter #6479872:  Learning rate = 0.000367:   Batch Loss = 0.101371, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2131635546684265, Accuracy = 0.9537471532821655\n",
      "Iter #6481920:  Learning rate = 0.000367:   Batch Loss = 0.110462, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20128893852233887, Accuracy = 0.9579203724861145\n",
      "Iter #6483968:  Learning rate = 0.000367:   Batch Loss = 0.123893, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21091772615909576, Accuracy = 0.9563553929328918\n",
      "Iter #6486016:  Learning rate = 0.000367:   Batch Loss = 0.099857, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20763176679611206, Accuracy = 0.9573987126350403\n",
      "Iter #6488064:  Learning rate = 0.000367:   Batch Loss = 0.123349, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20065344870090485, Accuracy = 0.9577465057373047\n",
      "Iter #6490112:  Learning rate = 0.000367:   Batch Loss = 0.099309, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22137326002120972, Accuracy = 0.9532255530357361\n",
      "Iter #6492160:  Learning rate = 0.000367:   Batch Loss = 0.111397, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20780813694000244, Accuracy = 0.9579203724861145\n",
      "Iter #6494208:  Learning rate = 0.000367:   Batch Loss = 0.104233, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.19771817326545715, Accuracy = 0.9617457985877991\n",
      "Iter #6496256:  Learning rate = 0.000367:   Batch Loss = 0.123205, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20827412605285645, Accuracy = 0.9584420323371887\n",
      "Iter #6498304:  Learning rate = 0.000367:   Batch Loss = 0.111124, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21588893234729767, Accuracy = 0.9525299668312073\n",
      "Iter #6500352:  Learning rate = 0.000352:   Batch Loss = 0.101944, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21383443474769592, Accuracy = 0.9539210796356201\n",
      "Iter #6502400:  Learning rate = 0.000352:   Batch Loss = 0.129019, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21265950798988342, Accuracy = 0.9586158990859985\n",
      "Iter #6504448:  Learning rate = 0.000352:   Batch Loss = 0.096955, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2069641500711441, Accuracy = 0.9568770527839661\n",
      "Iter #6506496:  Learning rate = 0.000352:   Batch Loss = 0.090432, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21337541937828064, Accuracy = 0.9563553929328918\n",
      "Iter #6508544:  Learning rate = 0.000352:   Batch Loss = 0.117230, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.220692977309227, Accuracy = 0.9533994197845459\n",
      "Iter #6510592:  Learning rate = 0.000352:   Batch Loss = 0.106293, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22535616159439087, Accuracy = 0.9460963606834412\n",
      "Iter #6512640:  Learning rate = 0.000352:   Batch Loss = 0.103363, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22394827008247375, Accuracy = 0.9514867067337036\n",
      "Iter #6514688:  Learning rate = 0.000352:   Batch Loss = 0.105538, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21152782440185547, Accuracy = 0.9573987126350403\n",
      "Iter #6516736:  Learning rate = 0.000352:   Batch Loss = 0.116786, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2068464159965515, Accuracy = 0.9591375589370728\n",
      "Iter #6518784:  Learning rate = 0.000352:   Batch Loss = 0.106341, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21009157598018646, Accuracy = 0.9582681059837341\n",
      "Iter #6520832:  Learning rate = 0.000352:   Batch Loss = 0.106807, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20604826509952545, Accuracy = 0.9580942392349243\n",
      "Iter #6522880:  Learning rate = 0.000352:   Batch Loss = 0.105615, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20374594628810883, Accuracy = 0.9568770527839661\n",
      "Iter #6524928:  Learning rate = 0.000352:   Batch Loss = 0.109556, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20811288058757782, Accuracy = 0.9570509195327759\n",
      "Iter #6526976:  Learning rate = 0.000352:   Batch Loss = 0.114738, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2123233675956726, Accuracy = 0.9528777599334717\n",
      "Iter #6529024:  Learning rate = 0.000352:   Batch Loss = 0.111748, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20354408025741577, Accuracy = 0.9572248458862305\n",
      "Iter #6531072:  Learning rate = 0.000352:   Batch Loss = 0.095324, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22027587890625, Accuracy = 0.9544426798820496\n",
      "Iter #6533120:  Learning rate = 0.000352:   Batch Loss = 0.146847, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24760746955871582, Accuracy = 0.9400104284286499\n",
      "Iter #6535168:  Learning rate = 0.000352:   Batch Loss = 0.123506, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20979657769203186, Accuracy = 0.9600069522857666\n",
      "Iter #6537216:  Learning rate = 0.000352:   Batch Loss = 0.138358, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2806927561759949, Accuracy = 0.9393149018287659\n",
      "Iter #6539264:  Learning rate = 0.000352:   Batch Loss = 0.114754, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21692952513694763, Accuracy = 0.9544426798820496\n",
      "Iter #6541312:  Learning rate = 0.000352:   Batch Loss = 0.133927, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.24337875843048096, Accuracy = 0.9436619877815247\n",
      "Iter #6543360:  Learning rate = 0.000352:   Batch Loss = 0.155044, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.25164151191711426, Accuracy = 0.9441836476325989\n",
      "Iter #6545408:  Learning rate = 0.000352:   Batch Loss = 0.091711, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23395565152168274, Accuracy = 0.9464440941810608\n",
      "Iter #6547456:  Learning rate = 0.000352:   Batch Loss = 0.123184, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2075524777173996, Accuracy = 0.9549643397331238\n",
      "Iter #6549504:  Learning rate = 0.000352:   Batch Loss = 0.102894, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20504041016101837, Accuracy = 0.9601808190345764\n",
      "Iter #6551552:  Learning rate = 0.000352:   Batch Loss = 0.120076, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2121225744485855, Accuracy = 0.9546166062355042\n",
      "Iter #6553600:  Learning rate = 0.000352:   Batch Loss = 0.126854, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.209841787815094, Accuracy = 0.9589636325836182\n",
      "Iter #6555648:  Learning rate = 0.000352:   Batch Loss = 0.124464, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2049320936203003, Accuracy = 0.9598330855369568\n",
      "Iter #6557696:  Learning rate = 0.000352:   Batch Loss = 0.097248, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2125735729932785, Accuracy = 0.955485999584198\n",
      "Iter #6559744:  Learning rate = 0.000352:   Batch Loss = 0.135613, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2026505023241043, Accuracy = 0.9580942392349243\n",
      "Iter #6561792:  Learning rate = 0.000352:   Batch Loss = 0.107167, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2084338366985321, Accuracy = 0.9573987126350403\n",
      "Iter #6563840:  Learning rate = 0.000352:   Batch Loss = 0.095559, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2144988775253296, Accuracy = 0.9518344402313232\n",
      "Iter #6565888:  Learning rate = 0.000352:   Batch Loss = 0.132611, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21273759007453918, Accuracy = 0.9577465057373047\n",
      "Iter #6567936:  Learning rate = 0.000352:   Batch Loss = 0.109420, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20366621017456055, Accuracy = 0.9620935320854187\n",
      "Iter #6569984:  Learning rate = 0.000352:   Batch Loss = 0.125311, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21019583940505981, Accuracy = 0.9546166062355042\n",
      "Iter #6572032:  Learning rate = 0.000352:   Batch Loss = 0.104487, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21654148399829865, Accuracy = 0.9549643397331238\n",
      "Iter #6574080:  Learning rate = 0.000352:   Batch Loss = 0.116644, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21619291603565216, Accuracy = 0.9546166062355042\n",
      "Iter #6576128:  Learning rate = 0.000352:   Batch Loss = 0.099990, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23255452513694763, Accuracy = 0.9481829404830933\n",
      "Iter #6578176:  Learning rate = 0.000352:   Batch Loss = 0.098181, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.209760844707489, Accuracy = 0.9584420323371887\n",
      "Iter #6580224:  Learning rate = 0.000352:   Batch Loss = 0.092606, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2212359756231308, Accuracy = 0.9544426798820496\n",
      "Iter #6582272:  Learning rate = 0.000352:   Batch Loss = 0.103175, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22249004244804382, Accuracy = 0.9523561000823975\n",
      "Iter #6584320:  Learning rate = 0.000352:   Batch Loss = 0.094755, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.202411949634552, Accuracy = 0.9568770527839661\n",
      "Iter #6586368:  Learning rate = 0.000352:   Batch Loss = 0.116044, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21487122774124146, Accuracy = 0.9573987126350403\n",
      "Iter #6588416:  Learning rate = 0.000352:   Batch Loss = 0.110636, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21704837679862976, Accuracy = 0.955485999584198\n",
      "Iter #6590464:  Learning rate = 0.000352:   Batch Loss = 0.106869, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21751558780670166, Accuracy = 0.9530516266822815\n",
      "Iter #6592512:  Learning rate = 0.000352:   Batch Loss = 0.095526, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22051078081130981, Accuracy = 0.9532255530357361\n",
      "Iter #6594560:  Learning rate = 0.000352:   Batch Loss = 0.109064, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20705121755599976, Accuracy = 0.9568770527839661\n",
      "Iter #6596608:  Learning rate = 0.000352:   Batch Loss = 0.093207, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21220016479492188, Accuracy = 0.9582681059837341\n",
      "Iter #6598656:  Learning rate = 0.000352:   Batch Loss = 0.099494, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22244825959205627, Accuracy = 0.9539210796356201\n",
      "Iter #6600704:  Learning rate = 0.000338:   Batch Loss = 0.108682, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22581496834754944, Accuracy = 0.9511389136314392\n",
      "Iter #6602752:  Learning rate = 0.000338:   Batch Loss = 0.117655, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22297948598861694, Accuracy = 0.9533994197845459\n",
      "Iter #6604800:  Learning rate = 0.000338:   Batch Loss = 0.111320, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2085811197757721, Accuracy = 0.9587897658348083\n",
      "Iter #6606848:  Learning rate = 0.000338:   Batch Loss = 0.103803, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21974721550941467, Accuracy = 0.9513128399848938\n",
      "Iter #6608896:  Learning rate = 0.000338:   Batch Loss = 0.118574, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20537421107292175, Accuracy = 0.9560076594352722\n",
      "Iter #6610944:  Learning rate = 0.000338:   Batch Loss = 0.103025, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20451852679252625, Accuracy = 0.960354745388031\n",
      "Iter #6612992:  Learning rate = 0.000338:   Batch Loss = 0.125772, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22732096910476685, Accuracy = 0.9537471532821655\n",
      "Iter #6615040:  Learning rate = 0.000338:   Batch Loss = 0.113194, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21159148216247559, Accuracy = 0.9540949463844299\n",
      "Iter #6617088:  Learning rate = 0.000338:   Batch Loss = 0.123306, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21995340287685394, Accuracy = 0.9540949463844299\n",
      "Iter #6619136:  Learning rate = 0.000338:   Batch Loss = 0.107418, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.212349072098732, Accuracy = 0.9591375589370728\n",
      "Iter #6621184:  Learning rate = 0.000338:   Batch Loss = 0.141386, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21615007519721985, Accuracy = 0.954790472984314\n",
      "Iter #6623232:  Learning rate = 0.000338:   Batch Loss = 0.093296, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23329231142997742, Accuracy = 0.9485306739807129\n",
      "Iter #6625280:  Learning rate = 0.000338:   Batch Loss = 0.115114, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22121530771255493, Accuracy = 0.9539210796356201\n",
      "Iter #6627328:  Learning rate = 0.000338:   Batch Loss = 0.111834, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22718572616577148, Accuracy = 0.9533994197845459\n",
      "Iter #6629376:  Learning rate = 0.000338:   Batch Loss = 0.111303, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20771098136901855, Accuracy = 0.9577465057373047\n",
      "Iter #6631424:  Learning rate = 0.000338:   Batch Loss = 0.121433, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2120782732963562, Accuracy = 0.9546166062355042\n",
      "Iter #6633472:  Learning rate = 0.000338:   Batch Loss = 0.104375, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23027697205543518, Accuracy = 0.9494001269340515\n",
      "Iter #6635520:  Learning rate = 0.000338:   Batch Loss = 0.133945, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22442568838596344, Accuracy = 0.9530516266822815\n",
      "Iter #6637568:  Learning rate = 0.000338:   Batch Loss = 0.112207, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21392136812210083, Accuracy = 0.9586158990859985\n",
      "Iter #6639616:  Learning rate = 0.000338:   Batch Loss = 0.093473, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21409104764461517, Accuracy = 0.9523561000823975\n",
      "Iter #6641664:  Learning rate = 0.000338:   Batch Loss = 0.124129, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2076071798801422, Accuracy = 0.9563553929328918\n",
      "Iter #6643712:  Learning rate = 0.000338:   Batch Loss = 0.105131, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20340415835380554, Accuracy = 0.959659218788147\n",
      "Iter #6645760:  Learning rate = 0.000338:   Batch Loss = 0.119707, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2023382931947708, Accuracy = 0.9593114256858826\n",
      "Iter #6647808:  Learning rate = 0.000338:   Batch Loss = 0.096551, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.203456312417984, Accuracy = 0.9589636325836182\n",
      "Iter #6649856:  Learning rate = 0.000338:   Batch Loss = 0.117316, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21607056260108948, Accuracy = 0.9530516266822815\n",
      "Iter #6651904:  Learning rate = 0.000338:   Batch Loss = 0.114387, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20737497508525848, Accuracy = 0.9568770527839661\n",
      "Iter #6653952:  Learning rate = 0.000338:   Batch Loss = 0.118268, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21679195761680603, Accuracy = 0.9535732865333557\n",
      "Iter #6656000:  Learning rate = 0.000338:   Batch Loss = 0.106263, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21084733307361603, Accuracy = 0.9539210796356201\n",
      "Iter #6658048:  Learning rate = 0.000338:   Batch Loss = 0.123147, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22858798503875732, Accuracy = 0.9539210796356201\n",
      "Iter #6660096:  Learning rate = 0.000338:   Batch Loss = 0.164102, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2100403755903244, Accuracy = 0.9600069522857666\n",
      "Iter #6662144:  Learning rate = 0.000338:   Batch Loss = 0.133156, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21012255549430847, Accuracy = 0.9580942392349243\n",
      "Iter #6664192:  Learning rate = 0.000338:   Batch Loss = 0.099609, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21978960931301117, Accuracy = 0.9551382660865784\n",
      "Iter #6666240:  Learning rate = 0.000338:   Batch Loss = 0.101947, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21883712708950043, Accuracy = 0.9532255530357361\n",
      "Iter #6668288:  Learning rate = 0.000338:   Batch Loss = 0.116037, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2043459713459015, Accuracy = 0.9598330855369568\n",
      "Iter #6670336:  Learning rate = 0.000338:   Batch Loss = 0.096625, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2113644927740097, Accuracy = 0.9568770527839661\n",
      "Iter #6672384:  Learning rate = 0.000338:   Batch Loss = 0.110106, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20068353414535522, Accuracy = 0.9587897658348083\n",
      "Iter #6674432:  Learning rate = 0.000338:   Batch Loss = 0.119865, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22323530912399292, Accuracy = 0.9549643397331238\n",
      "Iter #6676480:  Learning rate = 0.000338:   Batch Loss = 0.096948, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2036091536283493, Accuracy = 0.9591375589370728\n",
      "Iter #6678528:  Learning rate = 0.000338:   Batch Loss = 0.159775, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21971294283866882, Accuracy = 0.9528777599334717\n",
      "Iter #6680576:  Learning rate = 0.000338:   Batch Loss = 0.140462, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20982050895690918, Accuracy = 0.9553121328353882\n",
      "Iter #6682624:  Learning rate = 0.000338:   Batch Loss = 0.109797, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20934367179870605, Accuracy = 0.9594852924346924\n",
      "Iter #6684672:  Learning rate = 0.000338:   Batch Loss = 0.089546, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21509234607219696, Accuracy = 0.9565293192863464\n",
      "Iter #6686720:  Learning rate = 0.000338:   Batch Loss = 0.129087, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2083999216556549, Accuracy = 0.9587897658348083\n",
      "Iter #6688768:  Learning rate = 0.000338:   Batch Loss = 0.098969, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21431027352809906, Accuracy = 0.9551382660865784\n",
      "Iter #6690816:  Learning rate = 0.000338:   Batch Loss = 0.136388, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21632449328899384, Accuracy = 0.954790472984314\n",
      "Iter #6692864:  Learning rate = 0.000338:   Batch Loss = 0.097747, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2127971053123474, Accuracy = 0.9570509195327759\n",
      "Iter #6694912:  Learning rate = 0.000338:   Batch Loss = 0.109353, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20434968173503876, Accuracy = 0.9587897658348083\n",
      "Iter #6696960:  Learning rate = 0.000338:   Batch Loss = 0.096397, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21105696260929108, Accuracy = 0.9540949463844299\n",
      "Iter #6699008:  Learning rate = 0.000338:   Batch Loss = 0.114432, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21386340260505676, Accuracy = 0.9560076594352722\n",
      "Iter #6701056:  Learning rate = 0.000324:   Batch Loss = 0.151022, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21832400560379028, Accuracy = 0.9542688131332397\n",
      "Iter #6703104:  Learning rate = 0.000324:   Batch Loss = 0.121962, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20857898890972137, Accuracy = 0.9575725793838501\n",
      "Iter #6705152:  Learning rate = 0.000324:   Batch Loss = 0.099686, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20728543400764465, Accuracy = 0.9570509195327759\n",
      "Iter #6707200:  Learning rate = 0.000324:   Batch Loss = 0.141034, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22077059745788574, Accuracy = 0.9549643397331238\n",
      "Iter #6709248:  Learning rate = 0.000324:   Batch Loss = 0.128332, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2155306041240692, Accuracy = 0.9535732865333557\n",
      "Iter #6711296:  Learning rate = 0.000324:   Batch Loss = 0.093405, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.1974978744983673, Accuracy = 0.960354745388031\n",
      "Iter #6713344:  Learning rate = 0.000324:   Batch Loss = 0.138701, Accuracy = 0.96875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2096330225467682, Accuracy = 0.955485999584198\n",
      "Iter #6715392:  Learning rate = 0.000324:   Batch Loss = 0.096174, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21887627243995667, Accuracy = 0.9551382660865784\n",
      "Iter #6717440:  Learning rate = 0.000324:   Batch Loss = 0.120268, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21166138350963593, Accuracy = 0.9560076594352722\n",
      "Iter #6719488:  Learning rate = 0.000324:   Batch Loss = 0.098916, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20706148445606232, Accuracy = 0.9533994197845459\n",
      "Iter #6721536:  Learning rate = 0.000324:   Batch Loss = 0.095965, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20144496858119965, Accuracy = 0.9594852924346924\n",
      "Iter #6723584:  Learning rate = 0.000324:   Batch Loss = 0.113062, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21097534894943237, Accuracy = 0.9575725793838501\n",
      "Iter #6725632:  Learning rate = 0.000324:   Batch Loss = 0.124285, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20887520909309387, Accuracy = 0.9567031860351562\n",
      "Iter #6727680:  Learning rate = 0.000324:   Batch Loss = 0.088811, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22722846269607544, Accuracy = 0.9520083665847778\n",
      "Iter #6729728:  Learning rate = 0.000324:   Batch Loss = 0.110906, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23219826817512512, Accuracy = 0.9480090141296387\n",
      "Iter #6731776:  Learning rate = 0.000324:   Batch Loss = 0.112950, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2324872612953186, Accuracy = 0.9502695202827454\n",
      "Iter #6733824:  Learning rate = 0.000324:   Batch Loss = 0.104151, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22760534286499023, Accuracy = 0.9500956535339355\n",
      "Iter #6735872:  Learning rate = 0.000324:   Batch Loss = 0.129436, Accuracy = 0.97265625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22475141286849976, Accuracy = 0.9535732865333557\n",
      "Iter #6737920:  Learning rate = 0.000324:   Batch Loss = 0.091993, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22164174914360046, Accuracy = 0.9528777599334717\n",
      "Iter #6739968:  Learning rate = 0.000324:   Batch Loss = 0.097818, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2093726545572281, Accuracy = 0.9575725793838501\n",
      "Iter #6742016:  Learning rate = 0.000324:   Batch Loss = 0.102652, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21045251190662384, Accuracy = 0.9553121328353882\n",
      "Iter #6744064:  Learning rate = 0.000324:   Batch Loss = 0.092994, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.1998060941696167, Accuracy = 0.9629629850387573\n",
      "Iter #6746112:  Learning rate = 0.000324:   Batch Loss = 0.109814, Accuracy = 0.99609375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2137322723865509, Accuracy = 0.9570509195327759\n",
      "Iter #6748160:  Learning rate = 0.000324:   Batch Loss = 0.113681, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20413994789123535, Accuracy = 0.9570509195327759\n",
      "Iter #6750208:  Learning rate = 0.000324:   Batch Loss = 0.108053, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22152754664421082, Accuracy = 0.9542688131332397\n",
      "Iter #6752256:  Learning rate = 0.000324:   Batch Loss = 0.110721, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21246588230133057, Accuracy = 0.9551382660865784\n",
      "Iter #6754304:  Learning rate = 0.000324:   Batch Loss = 0.122290, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21851253509521484, Accuracy = 0.9540949463844299\n",
      "Iter #6756352:  Learning rate = 0.000324:   Batch Loss = 0.112796, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20832517743110657, Accuracy = 0.9553121328353882\n",
      "Iter #6758400:  Learning rate = 0.000324:   Batch Loss = 0.104513, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22227084636688232, Accuracy = 0.9530516266822815\n",
      "Iter #6760448:  Learning rate = 0.000324:   Batch Loss = 0.119258, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20825377106666565, Accuracy = 0.9565293192863464\n",
      "Iter #6762496:  Learning rate = 0.000324:   Batch Loss = 0.090840, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2131505012512207, Accuracy = 0.9591375589370728\n",
      "Iter #6764544:  Learning rate = 0.000324:   Batch Loss = 0.091400, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.23080921173095703, Accuracy = 0.9533994197845459\n",
      "Iter #6766592:  Learning rate = 0.000324:   Batch Loss = 0.129412, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22575414180755615, Accuracy = 0.9481829404830933\n",
      "Iter #6768640:  Learning rate = 0.000324:   Batch Loss = 0.127263, Accuracy = 0.984375\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.22576099634170532, Accuracy = 0.9532255530357361\n",
      "Iter #6770688:  Learning rate = 0.000324:   Batch Loss = 0.099649, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2163003832101822, Accuracy = 0.9527038931846619\n",
      "Iter #6772736:  Learning rate = 0.000324:   Batch Loss = 0.136873, Accuracy = 0.9765625\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20425403118133545, Accuracy = 0.960354745388031\n",
      "Iter #6774784:  Learning rate = 0.000324:   Batch Loss = 0.099142, Accuracy = 0.9921875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2068013995885849, Accuracy = 0.9575725793838501\n",
      "Iter #6776832:  Learning rate = 0.000324:   Batch Loss = 0.091226, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20879891514778137, Accuracy = 0.9558337926864624\n",
      "Iter #6778880:  Learning rate = 0.000324:   Batch Loss = 0.107427, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21765944361686707, Accuracy = 0.9560076594352722\n",
      "Iter #6780928:  Learning rate = 0.000324:   Batch Loss = 0.127281, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.20861688256263733, Accuracy = 0.9573987126350403\n",
      "Iter #6782976:  Learning rate = 0.000324:   Batch Loss = 0.088950, Accuracy = 1.0\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21344715356826782, Accuracy = 0.9539210796356201\n",
      "Iter #6785024:  Learning rate = 0.000324:   Batch Loss = 0.108826, Accuracy = 0.98828125\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.2210264950990677, Accuracy = 0.9551382660865784\n",
      "Iter #6787072:  Learning rate = 0.000324:   Batch Loss = 0.122437, Accuracy = 0.98046875\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.21325606107711792, Accuracy = 0.9539210796356201\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss = 0.21131828427314758, Accuracy = 0.9544426798820496\n",
      "TOTAL TIME:  2153.2955384254456\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "time_start = time.time()\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "\n",
    "while step * batch_size <= training_iters:\n",
    "    #print (sess.run(learning_rate)) #decaying learning rate\n",
    "    #print (sess.run(global_step)) # global number of iterations\n",
    "    if len(unsampled_indices) < batch_size:\n",
    "        unsampled_indices = list(range(0,len(X_train))) \n",
    "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    batch_ys = one_hot(raw_labels)\n",
    "    # check that encoded output is same length as num_classes, if not, pad it \n",
    "    if len(batch_ys[0]) < n_classes:\n",
    "        temp_ys = np.zeros((batch_size, n_classes))\n",
    "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
    "        batch_ys = temp_ys\n",
    "       \n",
    "    \n",
    "\n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Iter #\" + str(step*batch_size) + \\\n",
    "              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET:             \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n",
    "time_stop = time.time()\n",
    "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3316\n",
      "26513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 95.44426798820496%\n",
      "\n",
      "Precision: 95.500332335475%\n",
      "Recall: 95.44427056164146%\n",
      "f1_score: 95.45884537525906%\n",
      "\n",
      "Confusion Matrix:\n",
      "Created using test set of 5751 datapoints, normalised to % of each class in the test dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (Inline plots: )\n",
    "%matplotlib inline\n",
    "\n",
    "font = {\n",
    "    'family' : 'Bitstream Vera Sans',\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 18\n",
    "}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "#plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(\n",
    "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
    "    [training_iters]\n",
    ")\n",
    "#plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n",
    "print(len(test_accuracies))\n",
    "print(len(train_accuracies))\n",
    "\n",
    "plt.title(\"Training session's Accuracy over Iterations\")\n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.xlabel('Training Iteration')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "\n",
    "#print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.Blues\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[307.589 162.976 319.364 ...   0.    329.752 161.651]\n",
      "  [307.567 162.979 319.362 ...   0.    328.527 161.655]\n",
      "  [306.298 162.951 319.351 ...   0.    328.495 161.681]\n",
      "  ...\n",
      "  [293.291 122.534 307.676 ... 128.953 315.438 119.884]\n",
      "  [289.392 140.743 307.615 ...   0.    315.393 139.408]\n",
      "  [295.848 161.658 307.628 ... 160.331 314.112 160.264]]]\n",
      "[array([[ 4.7256007, -0.5436836,  0.3069304, -0.7505709, -2.1633954,\n",
      "        -1.5723332]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_val_path = DATASET_PATH + \"X_val.txt\"\n",
    "X_val = load_X(X_val_path)\n",
    "print(X_val)\n",
    "\n",
    "preds = sess.run(\n",
    "   [pred],\n",
    "   feed_dict={\n",
    "       x: X_val\n",
    "  }\n",
    ")\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21213701, 0.07946444, 0.23665449, 0.21144149, 0.21352808, 0.13701965, 0.22830813, 0.21144149, 0.2295253, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.23126413, 0.2295253, 0.23074248, 0.13580246, 0.22830813, 0.2123109, 0.23039472, 0.22830813, 0.21091984, 0.22830813, 0.23039472, 0.23039472, 0.21144149, 0.22830813, 0.22830813, 0.23039472, 0.2123109, 0.22830813, 0.22830813, 0.21144149, 0.23039472, 0.2123109, 0.22848201, 0.22830813, 0.23039472, 0.2123109, 0.23039472, 0.2123109, 0.2123109, 0.22830813, 0.22830813, 0.22969918, 0.21144149, 0.22848201, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.21144149, 0.2295253, 0.22830813, 0.21144149, 0.2295253, 0.22848201, 0.22830813, 0.22969918, 0.2295253, 0.22830813, 0.21144149, 0.21144149, 0.22848201, 0.22830813, 0.22830813, 0.21144149, 0.2295253, 0.22848201, 0.22830813, 0.2295253, 0.21144149, 0.22830813, 0.22830813, 0.22848201, 0.2295253, 0.22830813, 0.22917753, 0.2295253, 0.22830813, 0.22848201, 0.22848201, 0.21144149, 0.22830813, 0.22900365, 0.2305686, 0.22848201, 0.22830813, 0.22830813, 0.23039472, 0.2295253, 0.22830813, 0.22830813, 0.2305686, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22848201, 0.21144149, 0.22830813, 0.22848201, 0.2295253, 0.2295253, 0.22830813, 0.22830813, 0.2295253, 0.21144149, 0.22830813, 0.21144149, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.2295253, 0.21144149, 0.22830813, 0.22830813, 0.2295253, 0.2295253, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.22813424, 0.21144149, 0.2295253, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.22848201, 0.2295253, 0.22830813, 0.22830813, 0.22848201, 0.22830813, 0.2295253, 0.2295253, 0.22830813, 0.22830813, 0.22900365, 0.22830813, 0.22848201, 0.26725787, 0.19022779, 0.22830813, 0.22830813, 0.19022779, 0.295253, 0.21144149, 0.2295253, 0.22830813, 0.21144149, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.22830813, 0.21144149, 0.21144149, 0.22830813, 0.2295253, 0.22830813, 0.21144149, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.2295253, 0.2295253, 0.21144149, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22796035, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.2295253, 0.2295253, 0.23178577, 0.2112676, 0.22830813, 0.23126413, 0.23109025, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.2295253, 0.2295253, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22969918, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.2295253, 0.2295253, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.21144149, 0.2295253, 0.22830813, 0.21144149, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.2295253, 0.2295253, 0.22830813, 0.22830813, 0.22987306, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.21144149, 0.21144149, 0.21144149, 0.22830813, 0.22830813, 0.2295253, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22830813, 0.22848201, 0.22830813, 0.22848201, 0.22830813, 0.23022082, 0.26308468, 0.3022083, 0.3114241, 0.31281516, 0.3568075, 0.20883325, 0.29073206, 0.3658494, 0.27490872, 0.4115806, 0.3894975, 0.42323074, 0.4661798, 0.5106938, 0.43835855, 0.47765607, 0.48861068, 0.52025735, 0.33524606, 0.4946966, 0.54825246, 0.55294734, 0.56598854, 0.6063293, 0.5701617, 0.63136846, 0.6056338, 0.6362372, 0.612763, 0.5955486, 0.60528606, 0.68214226, 0.6864893, 0.6965745, 0.6616241, 0.7261346, 0.7179621, 0.74369675, 0.7096157, 0.6852721, 0.69414014, 0.72474355, 0.753782, 0.74526167, 0.7419579, 0.7503043, 0.71657103, 0.7158755, 0.7583029, 0.72248304, 0.77482176, 0.74560946, 0.7737785, 0.7737785, 0.7353504, 0.7781255, 0.7553469, 0.7904712, 0.7591723, 0.7238741, 0.6925752, 0.74960876, 0.7875152, 0.7647366, 0.77516955, 0.78542864, 0.79290557, 0.78003824, 0.81551033, 0.79290557, 0.7920362, 0.7847331, 0.790819, 0.7986437, 0.81012, 0.7659537, 0.8036863, 0.78977567, 0.7824726, 0.7998609, 0.8095983, 0.7906451, 0.7889063, 0.76091117, 0.8061207, 0.64649624, 0.7353504, 0.69414014, 0.7217875, 0.74474007, 0.7904712, 0.7856025, 0.75117373, 0.79447055, 0.81429315, 0.8045557, 0.7979482, 0.81220657, 0.78786296, 0.33507216, 0.32481307, 0.46357155, 0.46165884, 0.47574335, 0.53416795, 0.5953747, 0.58894104, 0.5745088, 0.60737264, 0.64162755, 0.66249347, 0.69327074, 0.67866457, 0.6936185, 0.7049209, 0.6031994, 0.6678839, 0.7310033, 0.7490871, 0.7784733, 0.72474355, 0.75952005, 0.75795513, 0.7470005, 0.7591723, 0.7348287, 0.68196833, 0.7829943, 0.7636933, 0.768562, 0.77708226, 0.7776039, 0.8002087, 0.7939489, 0.784907, 0.79533994, 0.75899845, 0.79638326, 0.7643888, 0.76091117, 0.799687, 0.7798644, 0.8120327, 0.81238043, 0.77708226, 0.80472964, 0.81551033, 0.8163798, 0.8116849, 0.8061207, 0.77690834, 0.8092506, 0.81551033, 0.8095983, 0.81742305, 0.81846637, 0.82211787, 0.81620586, 0.8116849, 0.81551033, 0.81342375, 0.8076856, 0.81864023, 0.81185883, 0.82333505, 0.8158581, 0.83463746, 0.8325509, 0.8440271, 0.8381151, 0.8455921, 0.84089726, 0.83707184, 0.8229873, 0.8212485, 0.84072334, 0.8095983, 0.84489655, 0.802643, 0.7664754, 0.82837766, 0.81255436, 0.84263605, 0.8243784, 0.8261172, 0.84141886, 0.82490003, 0.838289, 0.83985394, 0.8033385, 0.8290732, 0.83237696, 0.8266388, 0.83272475, 0.84454876, 0.8450704, 0.8322031, 0.8330725, 0.84385324, 0.8325509, 0.8212485, 0.8313337, 0.8017736, 0.84767866, 0.8330725, 0.84385324, 0.8473309, 0.83150756, 0.8454182, 0.8356807, 0.83272475, 0.848722, 0.84802645, 0.82959485, 0.83481133, 0.8534168, 0.8473309, 0.85376453, 0.84385324, 0.8549817, 0.84263605, 0.8076856, 0.73743695, 0.8109894, 0.8076856, 0.81968355, 0.8311598, 0.84680927, 0.8586333, 0.8424622, 0.84976524, 0.8455921, 0.8290732, 0.82959485, 0.85985047, 0.8172492, 0.83950615, 0.8483742, 0.8534168, 0.85046077, 0.8435055, 0.85880715, 0.8513302, 0.84802645, 0.8433316, 0.8261172, 0.83741957, 0.8483742, 0.8381151, 0.8574161, 0.84889585, 0.86071986, 0.853069, 0.84385324, 0.8582855, 0.85706836, 0.8565467, 0.85515565, 0.8521996, 0.84263605, 0.8608938, 0.8521996, 0.8687185, 0.85793775, 0.84993917, 0.8612415, 0.85915494, 0.85672057, 0.8643714, 0.8549817, 0.8614154, 0.84993917, 0.8541123, 0.8589811, 0.8396801, 0.8433316, 0.8603721, 0.8589811, 0.85515565, 0.87097895, 0.86593634, 0.86280644, 0.8626326, 0.854634, 0.8553295, 0.844201, 0.85515565, 0.86071986, 0.84976524, 0.84489655, 0.8612415, 0.8626326, 0.8391584, 0.8633281, 0.86541474, 0.8612415, 0.8657625, 0.8506347, 0.85915494, 0.866458, 0.841245, 0.86298037, 0.86541474, 0.86611027, 0.8582855, 0.86298037, 0.8617632, 0.8555034, 0.85706836, 0.85793775, 0.85080856, 0.86819685, 0.8565467, 0.8695879, 0.8683707, 0.8513302, 0.86419755, 0.86663187, 0.854634, 0.86193705, 0.86071986, 0.87097895, 0.85637283, 0.8702834, 0.85880715, 0.86611027, 0.8640236, 0.85689443, 0.8475048, 0.85515565, 0.84054947, 0.8577639, 0.86071986, 0.84698313, 0.86193705, 0.87445664, 0.8786298, 0.87184834, 0.8574161, 0.86384976, 0.86663187, 0.8648931, 0.8617632, 0.8624587, 0.87010956, 0.84993917, 0.85880715, 0.85080856, 0.866458, 0.85324293, 0.86976177, 0.86506695, 0.87010956, 0.86228484, 0.8699357, 0.8553295, 0.8685446, 0.85602504, 0.87132674, 0.86315423, 0.8695879, 0.8704573, 0.8763693, 0.86906624, 0.8748044, 0.8633281, 0.8614154, 0.8521996, 0.8542862, 0.8582855, 0.87445664, 0.8735872, 0.8734133, 0.8735872, 0.8595027, 0.85359067, 0.8368979, 0.8092506, 0.8419405, 0.86106765, 0.8617632, 0.8612415, 0.8676752, 0.8746305, 0.879847, 0.8794992, 0.88349855, 0.8742827, 0.868023, 0.8640236, 0.85672057, 0.8699357, 0.8767171, 0.8767171, 0.8748044, 0.88089025, 0.8765432, 0.868023, 0.8621109, 0.821944, 0.84072334, 0.85289514, 0.8542862, 0.8527213, 0.8595027, 0.8685446, 0.8711528, 0.8708051, 0.87097895, 0.8716745, 0.8824552, 0.8735872, 0.85915494, 0.87619543, 0.8648931, 0.88089025, 0.87323946, 0.86906624, 0.8788037, 0.88506347, 0.8767171, 0.8706312, 0.87323946, 0.873935, 0.8668058, 0.863502, 0.8621109, 0.88123804, 0.86941403, 0.8640236, 0.8789776, 0.86106765, 0.87497824, 0.8652408, 0.85915494, 0.86697966, 0.8789776, 0.87010956, 0.8742827, 0.79638326, 0.8687185, 0.87376106, 0.85515565, 0.8727178, 0.87915146, 0.8775865, 0.8789776, 0.86976177, 0.8838463, 0.8805425, 0.8292471, 0.8506347, 0.8770649, 0.87915146, 0.88036865, 0.8789776, 0.8754999, 0.8817597, 0.8734133, 0.8774126, 0.8829769, 0.88193357, 0.8711528, 0.8807164, 0.8400278, 0.8789776, 0.8838463, 0.8748044, 0.87689096, 0.8695879, 0.8716745, 0.86697966, 0.87323946, 0.88228136, 0.88628066, 0.88002086, 0.8817597, 0.87689096, 0.88002086, 0.8888889, 0.8751522, 0.8876717, 0.8779343, 0.88158584, 0.8708051, 0.8786298, 0.8490697, 0.863502, 0.87376106, 0.86071986, 0.8801947, 0.8748044, 0.8786298, 0.8873239, 0.86976177, 0.8770649, 0.8758477, 0.8702834, 0.87254393, 0.88228136, 0.8372457, 0.844201, 0.84976524, 0.8521996, 0.83776736, 0.86976177, 0.88193357, 0.87689096, 0.8814119, 0.8838463, 0.88315076, 0.88089025, 0.86298037, 0.8814119, 0.8796731, 0.88523734, 0.8695879, 0.88158584, 0.88836724, 0.8909755, 0.8765432, 0.87619543, 0.8789776, 0.86471915, 0.8751522, 0.87010956, 0.8807164, 0.88349855, 0.88523734, 0.88506347, 0.8756738, 0.8673274, 0.8841941, 0.8948009, 0.8880195, 0.8829769, 0.88089025, 0.8645453, 0.86906624, 0.86697966, 0.87497824, 0.8826291, 0.8939315, 0.8810642, 0.8937576, 0.8826291, 0.88906276, 0.87915146, 0.89132327, 0.89740914, 0.8937576, 0.8941054, 0.8838463, 0.8833246, 0.8904538, 0.8864545, 0.8956703, 0.8756738, 0.89027995, 0.885759, 0.8892366, 0.89132327, 0.891671, 0.8777604, 0.88402015, 0.8422883, 0.84576595, 0.87323946, 0.87619543, 0.8829769, 0.88315076, 0.88089025, 0.8951487, 0.89845246, 0.9010607, 0.89114934, 0.8841941, 0.8814119, 0.86628413, 0.88836724, 0.8972353, 0.89340985, 0.89619195, 0.89740914, 0.8899322, 0.9010607, 0.89149714, 0.8937576, 0.8716745, 0.8848896, 0.88036865, 0.8988002, 0.8951487, 0.89219266, 0.88402015, 0.8941054, 0.89653975, 0.89497477, 0.9000174, 0.89966965, 0.9034951, 0.89827853, 0.9000174, 0.88836724, 0.87323946, 0.88941056, 0.89445317, 0.88506347, 0.8986263, 0.83741957, 0.8544601, 0.8523735, 0.8909755, 0.8948009, 0.888715, 0.8845418, 0.8796731, 0.8895844, 0.8833246, 0.88402015, 0.8956703, 0.8855851, 0.89027995, 0.899148, 0.89810467, 0.8937576, 0.878282, 0.8626326, 0.8954964, 0.8789776, 0.8868023, 0.89427924, 0.90123457, 0.89845246, 0.899148, 0.9007129, 0.90332115, 0.90158236, 0.8958442, 0.8956703, 0.89966965, 0.9000174, 0.89340985, 0.90679884, 0.88941056, 0.8908016, 0.9048861, 0.8951487, 0.8986263, 0.9066249, 0.9010607, 0.9066249, 0.89219266, 0.89462703, 0.89254045, 0.90210396, 0.8951487, 0.89497477, 0.88715005, 0.8868023, 0.89184487, 0.8972353, 0.8948009, 0.89775693, 0.9008868, 0.9017562, 0.8767171, 0.9019301, 0.8833246, 0.88523734, 0.8756738, 0.893236, 0.9041906, 0.90732044, 0.9090593, 0.8994957, 0.89062774, 0.86663187, 0.89306206, 0.893236, 0.89427924, 0.89306206, 0.87723875, 0.8801947, 0.8904538, 0.9000174, 0.8968875, 0.88541126, 0.90245175, 0.9029734, 0.8988002, 0.8937576, 0.8704573, 0.8998435, 0.8998435, 0.90158236, 0.90523386, 0.90523386, 0.90523386, 0.8775865, 0.8868023, 0.90453833, 0.89740914, 0.90332115, 0.9010607, 0.88819337, 0.8864545, 0.91062427, 0.9069727, 0.90140843, 0.8988002, 0.8873239, 0.8763693, 0.9085376, 0.9069727, 0.9078421, 0.8866284, 0.89966965, 0.90453833, 0.9010607, 0.8897583, 0.9022779, 0.9019301, 0.8937576, 0.88836724, 0.89062774, 0.90019125, 0.9008868, 0.89132327, 0.88036865, 0.9017562, 0.90436447, 0.9019301, 0.8868023, 0.8956703, 0.89775693, 0.9010607, 0.9017562, 0.8786298, 0.8958442, 0.8994957, 0.90506, 0.9057555, 0.9017562, 0.8960181, 0.88836724, 0.899148, 0.89445317, 0.9019301, 0.9057555, 0.9078421, 0.90836376, 0.91149366, 0.8948009, 0.90923315, 0.90436447, 0.8988002, 0.8951487, 0.90645105, 0.9061033, 0.8939315, 0.89966965, 0.90332115, 0.9007129, 0.9003652, 0.9022779, 0.89845246, 0.89775693, 0.90523386, 0.9031473, 0.9102765, 0.9034951, 0.90019125, 0.90679884, 0.9022779, 0.9031473, 0.90332115, 0.9019301, 0.90366894, 0.9102765, 0.8928882, 0.9003652, 0.85289514, 0.9008868, 0.89740914, 0.897583, 0.8989741, 0.8866284, 0.90332115, 0.90019125, 0.9038428, 0.9066249, 0.9034951, 0.9071466, 0.9094071, 0.9038428, 0.9048861, 0.89462703, 0.90140843, 0.9010607, 0.89340985, 0.8868023, 0.8968875, 0.8954964, 0.89114934, 0.88593286, 0.90332115, 0.9097548, 0.8998435, 0.89219266, 0.8941054, 0.9010607, 0.90836376, 0.90558165, 0.91114587, 0.8967136, 0.90366894, 0.9097548, 0.86506695, 0.8866284, 0.9022779, 0.9003652, 0.90158236, 0.90019125, 0.90210396, 0.90523386, 0.8986263, 0.9134064, 0.8967136, 0.9094071, 0.90245175, 0.9094071, 0.9059294, 0.90245175, 0.91062427, 0.9059294, 0.9040167, 0.9101026, 0.9048861, 0.8958442, 0.90923315, 0.90506, 0.8954964, 0.9101026, 0.89532256, 0.885759, 0.8989741, 0.90506, 0.91045034, 0.91444963, 0.90766823, 0.90958095, 0.90732044, 0.9008868, 0.9102765, 0.908016, 0.9027995, 0.9097548, 0.9066249, 0.9097548, 0.90923315, 0.8958442, 0.90453833, 0.8988002, 0.89340985, 0.8994957, 0.90645105, 0.90506, 0.9120153, 0.90836376, 0.9130586, 0.9048861, 0.9061033, 0.9000174, 0.90523386, 0.90123457, 0.91079813, 0.9017562, 0.91045034, 0.90558165, 0.9137541, 0.9041906, 0.9059294, 0.91149366, 0.90749437, 0.9147974, 0.91114587, 0.9081899, 0.9147974, 0.9038428, 0.9054077, 0.90732044, 0.90332115, 0.9147974, 0.9094071, 0.8970614, 0.89966965, 0.89932185, 0.9062772, 0.9026256, 0.91045034, 0.9099287, 0.9101026, 0.9097548, 0.9102765, 0.90366894, 0.8939315, 0.90645105, 0.9102765, 0.91358024, 0.9088854, 0.8994957, 0.91271085, 0.9031473, 0.8970614, 0.90523386, 0.9057555, 0.8876717, 0.8937576, 0.8935837, 0.90140843, 0.90453833, 0.9113198, 0.90679884, 0.9121892, 0.908016, 0.90453833, 0.9128847, 0.9081899, 0.90836376, 0.9121892, 0.9097548, 0.91358024, 0.9137541, 0.91079813, 0.90158236, 0.9097548, 0.9047122, 0.91114587, 0.91549295, 0.90766823, 0.91114587, 0.9094071, 0.91149366, 0.9097548, 0.90871155, 0.90645105, 0.90436447, 0.9118414, 0.9085376, 0.89810467, 0.90958095, 0.9128847, 0.9094071, 0.912537, 0.9130586, 0.9078421, 0.91045034, 0.9097548, 0.9099287, 0.8989741, 0.908016, 0.90923315, 0.9026256, 0.9149713, 0.9137541, 0.9085376, 0.9031473, 0.9061033, 0.9102765, 0.9118414, 0.91149366, 0.9022779, 0.88749784, 0.9066249, 0.90123457, 0.9047122, 0.91045034, 0.9062772, 0.912537, 0.9088854, 0.9141019, 0.9085376, 0.9066249, 0.9088854, 0.9161885, 0.9071466, 0.91949224, 0.91740566, 0.9130586, 0.90332115, 0.916884, 0.9066249, 0.9132325, 0.90958095, 0.9071466, 0.90958095, 0.90523386, 0.9048861, 0.91636235, 0.9088854, 0.91236305, 0.91444963, 0.91392803, 0.9057555, 0.9099287, 0.9147974, 0.9160146, 0.9071466, 0.90523386, 0.90558165, 0.9088854, 0.9057555, 0.90558165, 0.91462356, 0.91062427, 0.9121892, 0.9102765, 0.9078421, 0.90558165, 0.918449, 0.90436447, 0.9153191, 0.9193184, 0.9149713, 0.91444963, 0.91079813, 0.9161885, 0.9137541, 0.9120153, 0.91079813, 0.916884, 0.9134064, 0.91149366, 0.9029734, 0.9134064, 0.9181012, 0.91236305, 0.91671014, 0.91114587, 0.91444963, 0.9160146, 0.90923315, 0.9201878, 0.9165363, 0.9137541, 0.88906276, 0.9102765, 0.91114587, 0.9071466, 0.8972353, 0.910972, 0.89932185, 0.910972, 0.90732044, 0.8923665, 0.8866284, 0.9165363, 0.9141019, 0.9094071, 0.9151452, 0.91636235, 0.9147974, 0.90523386, 0.91462356, 0.9029734, 0.9233177, 0.91984004, 0.9137541, 0.90923315, 0.91358024, 0.9099287, 0.90749437, 0.91045034, 0.9151452, 0.91705793, 0.91114587, 0.92070943, 0.9031473, 0.91462356, 0.91062427, 0.9059294, 0.91757953, 0.9094071, 0.91062427, 0.91827506, 0.9156668, 0.9116675, 0.91775346, 0.9066249, 0.910972, 0.9141019, 0.91462356, 0.9156668, 0.9156668, 0.90140843, 0.9179273, 0.9172318, 0.92296994, 0.9215789, 0.91584074, 0.9090593, 0.9193184, 0.91757953, 0.9088854, 0.91862285, 0.90923315, 0.89619195, 0.89775693, 0.9161885, 0.91149366, 0.91149366, 0.91462356, 0.9147974, 0.91114587, 0.91757953, 0.9120153, 0.9189706, 0.8941054, 0.91236305, 0.91358024, 0.9099287, 0.91757953, 0.9149713, 0.918449, 0.91757953, 0.91636235, 0.9210572, 0.916884, 0.9187967, 0.9132325, 0.9203617, 0.91062427, 0.91114587, 0.8948009, 0.9151452, 0.91236305, 0.918449, 0.9189706, 0.910972, 0.91984004, 0.9121892, 0.9187967, 0.9264476, 0.9181012, 0.92366546, 0.9241871, 0.9212311, 0.9130586, 0.91462356, 0.9255782, 0.9203617, 0.92453486, 0.92262214, 0.91705793, 0.91462356, 0.9134064, 0.9102765, 0.92140496, 0.91444963, 0.92070943, 0.91427577, 0.9120153, 0.9200139, 0.9240132, 0.92070943, 0.918449, 0.9250565, 0.9191445, 0.9215789, 0.9165363, 0.9130586, 0.9189706, 0.90923315, 0.9132325, 0.9189706, 0.8968875, 0.9101026, 0.91462356, 0.9088854, 0.9141019, 0.9247087, 0.9113198, 0.9252304, 0.922796, 0.9240132, 0.91949224, 0.9137541, 0.91775346, 0.91949224, 0.92140496, 0.92349154, 0.92609984, 0.9292297, 0.92575204, 0.9274909, 0.9172318, 0.9088854, 0.918449, 0.9259259, 0.90749437, 0.91271085, 0.9007129, 0.90558165, 0.9290558, 0.9210572, 0.92609984, 0.9201878, 0.9181012, 0.93201184, 0.9278386, 0.93166405, 0.9172318, 0.9203617, 0.922796, 0.9221005, 0.9147974, 0.9222744, 0.9240132, 0.92488265, 0.9181012, 0.92488265, 0.9240132, 0.9219266, 0.9233177, 0.92731696, 0.91757953, 0.9252304, 0.92540425, 0.9179273, 0.92349154, 0.918449, 0.92175275, 0.9313163, 0.92662144, 0.92296994, 0.92853415, 0.9300991, 0.92175275, 0.9212311, 0.9212311, 0.92679536, 0.9281864, 0.922796, 0.92609984, 0.9041906, 0.908016, 0.9153191, 0.916884, 0.9221005, 0.9208833, 0.9231438, 0.9208833, 0.9255782, 0.9208833, 0.92366546, 0.91949224, 0.91636235, 0.9147974, 0.92766476, 0.9147974, 0.91827506, 0.9281864, 0.9201878, 0.92053556, 0.90679884, 0.9221005, 0.92296994, 0.9311424, 0.9274909, 0.9151452, 0.92731696, 0.9264476, 0.9292297, 0.9280125, 0.91757953, 0.92383933, 0.9165363, 0.918449, 0.9071466, 0.9147974, 0.92853415, 0.9203617, 0.9280125, 0.9271431, 0.92853415, 0.9292297, 0.93392456, 0.9252304, 0.92679536, 0.9156668, 0.9161885, 0.92853415, 0.92696923, 0.9323596, 0.9290558, 0.9262737, 0.9292297, 0.9264476, 0.9294036, 0.924361, 0.93253344, 0.91827506, 0.9200139, 0.9212311, 0.92383933, 0.9078421, 0.916884, 0.9203617, 0.9255782, 0.92888194, 0.92975134, 0.92696923, 0.9088854, 0.92262214, 0.9274909, 0.9278386, 0.92575204, 0.93514174, 0.9255782, 0.92957747, 0.93079466, 0.9311424, 0.9262737, 0.9172318, 0.9193184, 0.9264476, 0.92731696, 0.9294036, 0.9342723, 0.9090593, 0.91392803, 0.8848896, 0.897583, 0.9215789, 0.9222744, 0.9191445, 0.92540425, 0.92453486, 0.9321857, 0.93357676, 0.92731696, 0.9255782, 0.93270737, 0.9278386, 0.9247087, 0.933229, 0.9313163, 0.9274909, 0.9342723, 0.9294036, 0.92609984, 0.93392456, 0.9370544, 0.9292297, 0.92975134, 0.9290558, 0.92609984, 0.92957747, 0.93479395, 0.92888194, 0.9318379, 0.9299252, 0.92488265, 0.91462356, 0.9290558, 0.92262214, 0.91775346, 0.9278386, 0.9300991, 0.9247087, 0.9283603, 0.9323596, 0.9346201, 0.93253344, 0.9274909, 0.92853415, 0.9323596, 0.93444616, 0.9393149, 0.9321857, 0.92957747, 0.930273, 0.9323596, 0.9290558, 0.93566334, 0.9384455, 0.9408798, 0.9255782, 0.9294036, 0.92766476, 0.9318379, 0.9221005, 0.93583727, 0.9370544, 0.93079466, 0.9403582, 0.93392456, 0.93044686, 0.9311424, 0.9300991, 0.9380977, 0.9386194, 0.933229, 0.9374022, 0.9323596, 0.9342723, 0.9290558, 0.92679536, 0.9281864, 0.9287081, 0.93757606, 0.9274909, 0.9340984, 0.9337506, 0.9346201, 0.93514174, 0.93879324, 0.9334029, 0.9040167, 0.9287081, 0.9340984, 0.93514174, 0.93201184, 0.9393149, 0.92662144, 0.9323596, 0.93583727, 0.93253344, 0.93879324, 0.93392456, 0.9292297, 0.93392456, 0.9417493, 0.9318379, 0.93983656, 0.936185, 0.93392456, 0.9210572, 0.92696923, 0.9306208, 0.9274909, 0.9259259, 0.92488265, 0.9287081, 0.92696923, 0.9314902, 0.9283603, 0.92975134, 0.9292297, 0.93201184, 0.93583727, 0.92053556, 0.9259259, 0.9292297, 0.94105375, 0.92957747, 0.93357676, 0.9330551, 0.9400104, 0.9311424, 0.9370544, 0.9306208, 0.9252304, 0.9396627, 0.9386194, 0.93583727, 0.936185, 0.9368805, 0.9271431, 0.9215789, 0.936185, 0.93514174, 0.9340984, 0.93253344, 0.93514174, 0.9294036, 0.92662144, 0.93444616, 0.9386194, 0.9318379, 0.9323596, 0.9311424, 0.9380977, 0.92609984, 0.94348806, 0.9417493, 0.9412276, 0.93948877, 0.9401843, 0.9323596, 0.93566334, 0.9354895, 0.9247087, 0.9354895, 0.9452269, 0.92679536, 0.93601114, 0.9370544, 0.9393149, 0.93357676, 0.93479395, 0.93392456, 0.933229, 0.92975134, 0.9382716, 0.9354895, 0.9354895, 0.93914104, 0.9318379, 0.93792385, 0.93879324, 0.93166405, 0.93775, 0.93044686, 0.93948877, 0.94227093, 0.9417493, 0.93792385, 0.93601114, 0.9393149, 0.9321857, 0.92453486, 0.93879324, 0.9405321, 0.9414015, 0.9370544, 0.93635887, 0.9340984, 0.9408798, 0.936185, 0.936185, 0.9415754, 0.93948877, 0.93757606, 0.936185, 0.9389671, 0.9389671, 0.92296994, 0.9401843, 0.92853415, 0.93444616, 0.9403582, 0.9354895, 0.93357676, 0.94192314, 0.93270737, 0.9476613, 0.942097, 0.9323596, 0.94505304, 0.9405321, 0.93879324, 0.92679536, 0.93253344, 0.9334029, 0.93270737, 0.9380977, 0.9408798, 0.93914104, 0.94192314, 0.94383585, 0.93392456, 0.9368805, 0.9384455, 0.9382716, 0.93514174, 0.93566334, 0.93635887, 0.9396627, 0.93879324, 0.94192314, 0.9414015, 0.94070596, 0.93879324, 0.9462702, 0.9393149, 0.9384455, 0.9403582, 0.94470525, 0.94192314, 0.94192314, 0.9433142, 0.93253344, 0.93479395, 0.93670666, 0.936185, 0.93879324, 0.9393149, 0.9342723, 0.9431403, 0.9412276, 0.9386194, 0.9281864, 0.9455747, 0.93879324, 0.93879324, 0.9424448, 0.9309685, 0.93983656, 0.9389671, 0.94505304, 0.9353156, 0.9424448, 0.9454008, 0.94192314, 0.94609636, 0.93566334, 0.93635887, 0.93583727, 0.9300991, 0.94696575, 0.93288124, 0.93288124, 0.94192314, 0.9311424, 0.9382716, 0.94661796, 0.9440097, 0.9467919, 0.9321857, 0.936185, 0.933229, 0.93757606, 0.9452269, 0.94261867, 0.9354895, 0.94470525, 0.94783515, 0.94383585, 0.9401843, 0.94070596, 0.9396627, 0.9340984, 0.9400104, 0.9389671, 0.936185, 0.9474874, 0.9408798, 0.930273, 0.93879324, 0.9433142, 0.94696575, 0.9415754, 0.9454008, 0.94070596, 0.94592243, 0.93983656, 0.93166405, 0.9386194, 0.9455747, 0.93566334, 0.9452269, 0.93948877, 0.9414015, 0.94348806, 0.94227093, 0.9370544, 0.9424448, 0.94348806, 0.94974786, 0.93914104, 0.9440097, 0.9405321, 0.9431403, 0.94418365, 0.942097, 0.94348806, 0.9473135, 0.94592243, 0.94261867, 0.94383585, 0.943662, 0.93670666, 0.9440097, 0.94905233, 0.9454008, 0.9474874, 0.94505304, 0.9389671, 0.9414015, 0.9405321, 0.93670666, 0.94296646, 0.9354895, 0.9433142, 0.9504434, 0.9401843, 0.9492262, 0.9380977, 0.9417493, 0.94296646, 0.92070943, 0.93253344, 0.90506, 0.9264476, 0.9516606, 0.9492262, 0.9382716, 0.94609636, 0.94887847, 0.93270737, 0.9321857, 0.9372283, 0.9346201, 0.9382716, 0.94783515, 0.9353156, 0.93775, 0.9431403, 0.93670666, 0.9424448, 0.9386194, 0.9455747, 0.9448792, 0.9476613, 0.94227093, 0.943662, 0.94383585, 0.9483568, 0.9492262, 0.94227093, 0.942097, 0.9443575, 0.94974786, 0.94470525, 0.9467919, 0.9514867, 0.9462702, 0.9474874, 0.9414015, 0.94348806, 0.9452269, 0.943662, 0.93601114, 0.9492262, 0.9516606, 0.93757606, 0.94348806, 0.95096505, 0.94661796, 0.94261867, 0.93879324, 0.9448792, 0.94887847, 0.943662, 0.9467919, 0.95200837, 0.9502695, 0.949574, 0.9464441, 0.9405321, 0.9440097, 0.94418365, 0.9346201, 0.94470525, 0.93583727, 0.9462702, 0.9454008, 0.9370544, 0.93079466, 0.9412276, 0.9464441, 0.9389671, 0.9353156, 0.94070596, 0.9452269, 0.9382716, 0.9431403, 0.9443575, 0.9384455, 0.94905233, 0.9476613, 0.94348806, 0.9467919, 0.94905233, 0.9448792, 0.94887847, 0.94783515, 0.9353156, 0.9499217, 0.9417493, 0.91236305, 0.94227093, 0.9514867, 0.9467919, 0.9414015, 0.94661796, 0.94070596, 0.93948877, 0.95131284, 0.9464441, 0.94818294, 0.9452269, 0.94105375, 0.9487046, 0.9368805, 0.94783515, 0.94348806, 0.9514867, 0.95183444, 0.94887847, 0.94505304, 0.9464441, 0.94261867, 0.94261867, 0.9454008, 0.92540425, 0.93757606, 0.9294036, 0.9454008, 0.9492262, 0.94696575, 0.9485307, 0.93792385, 0.94574857, 0.9389671, 0.9502695, 0.94783515, 0.9393149, 0.94470525, 0.94470525, 0.9483568, 0.9492262, 0.9483568, 0.94505304, 0.95322555, 0.948009, 0.9455747, 0.9443575, 0.948009, 0.94470525, 0.9455747, 0.95096505, 0.94296646, 0.9527039, 0.9400104, 0.94592243, 0.9313163, 0.9499217, 0.94470525, 0.94905233, 0.9527039, 0.9382716, 0.943662, 0.94887847, 0.93775, 0.93514174, 0.9393149, 0.92766476, 0.94279253, 0.942097, 0.9485307, 0.9506173, 0.94974786, 0.95218223, 0.9445314, 0.94592243, 0.95009565, 0.94887847, 0.9382716, 0.942097, 0.95096505, 0.95096505, 0.9474874, 0.9462702, 0.9558338, 0.9506173, 0.95252997, 0.95009565, 0.9499217, 0.9306208, 0.94905233, 0.9504434, 0.94609636, 0.93983656, 0.9514867, 0.9511389, 0.95496434, 0.94192314, 0.9415754, 0.94418365, 0.94192314, 0.9471396, 0.95218223, 0.9502695, 0.94974786, 0.9499217, 0.9448792, 0.949574, 0.9507912, 0.9527039, 0.9474874, 0.9403582, 0.9474874, 0.943662, 0.94348806, 0.9511389, 0.955486, 0.9467919, 0.94348806, 0.9507912, 0.94227093, 0.949574, 0.95374715, 0.9567032, 0.94592243, 0.94696575, 0.9374022, 0.94696575, 0.95096505, 0.95287776, 0.9504434, 0.949574, 0.9487046, 0.9516606, 0.9452269, 0.9476613, 0.9504434, 0.9492262, 0.9546166, 0.9514867, 0.94905233, 0.9400104, 0.9433142, 0.95252997, 0.94105375, 0.9474874, 0.9448792, 0.94592243, 0.94696575, 0.949574, 0.94505304, 0.9454008, 0.9474874, 0.94296646, 0.9514867, 0.94661796, 0.9474874, 0.95009565, 0.94348806, 0.9483568, 0.95131284, 0.9523561, 0.9396627, 0.93879324, 0.94279253, 0.9485307, 0.9471396, 0.94818294, 0.95009565, 0.9424448, 0.9467919, 0.9575726, 0.94505304, 0.94905233, 0.9431403, 0.94818294, 0.9448792, 0.95218223, 0.9403582, 0.94227093, 0.95096505, 0.9443575, 0.9452269, 0.95409495, 0.95322555, 0.95252997, 0.94696575, 0.9474874, 0.94905233, 0.9405321, 0.9464441, 0.9474874, 0.94974786, 0.95496434, 0.94887847, 0.94905233, 0.94418365, 0.94696575, 0.94974786, 0.95218223, 0.9405321, 0.942097, 0.9511389, 0.95252997, 0.9462702, 0.9440097, 0.943662, 0.9414015, 0.9400104, 0.9483568, 0.9455747, 0.9502695, 0.94505304, 0.948009, 0.9393149, 0.9454008, 0.93983656, 0.9499217, 0.9415754, 0.94279253, 0.9476613, 0.9485307, 0.9523561, 0.94661796, 0.9372283, 0.9454008, 0.9563554, 0.95374715, 0.9523561, 0.9474874, 0.94661796, 0.9473135, 0.9504434, 0.95009565, 0.94418365, 0.9440097, 0.94348806, 0.9507912, 0.9443575, 0.95131284, 0.949574, 0.94905233, 0.9506173, 0.9476613, 0.9455747, 0.9405321, 0.94296646, 0.9433142, 0.94227093, 0.94609636, 0.95096505, 0.95374715, 0.94818294, 0.9483568, 0.949574, 0.95322555, 0.943662, 0.9445314, 0.9499217, 0.9433142, 0.933229, 0.95409495, 0.93757606, 0.93270737, 0.9448792, 0.94505304, 0.95513827, 0.95409495, 0.95218223, 0.9539211, 0.95374715, 0.9527039, 0.9507912, 0.9400104, 0.9494001, 0.94696575, 0.9502695, 0.94661796, 0.95374715, 0.94905233, 0.94905233, 0.94905233, 0.9471396, 0.93601114, 0.9445314, 0.9448792, 0.9471396, 0.9516606, 0.9547905, 0.949574, 0.9452269, 0.9499217, 0.94296646, 0.948009, 0.9535733, 0.95096505, 0.9431403, 0.95200837, 0.95183444, 0.95096505, 0.9504434, 0.9483568, 0.9485307, 0.9504434, 0.93983656, 0.93948877, 0.9467919, 0.94383585, 0.94783515, 0.9471396, 0.9530516, 0.94905233, 0.9445314, 0.94818294, 0.9471396, 0.95131284, 0.9487046, 0.9516606, 0.9417493, 0.9539211, 0.95722485, 0.9539211, 0.948009, 0.94974786, 0.9448792, 0.95183444, 0.94887847, 0.95565987, 0.9494001, 0.9464441, 0.94696575, 0.95322555, 0.9476613, 0.9504434, 0.9530516, 0.94418365, 0.95287776, 0.94418365, 0.9455747, 0.94887847, 0.9507912, 0.9455747, 0.9570509, 0.95218223, 0.9547905, 0.9448792, 0.9443575, 0.9507912, 0.95287776, 0.95131284, 0.94609636, 0.9445314, 0.9473135, 0.95183444, 0.9474874, 0.9454008, 0.9452269, 0.95096505, 0.955486, 0.95322555, 0.95409495, 0.95131284, 0.95374715, 0.9396627, 0.9471396, 0.9433142, 0.94905233, 0.9511389, 0.9504434, 0.9433142, 0.9455747, 0.95531213, 0.95131284, 0.9487046, 0.94905233, 0.943662, 0.9506173, 0.95183444, 0.95183444, 0.95252997, 0.9499217, 0.9567032, 0.9546166, 0.94592243, 0.9412276, 0.9455747, 0.9494001, 0.9487046, 0.94070596, 0.9454008, 0.94609636, 0.9546166, 0.9464441, 0.9563554, 0.94887847, 0.9462702, 0.94609636, 0.9547905, 0.9403582, 0.9507912, 0.9546166, 0.9462702, 0.9494001, 0.9448792, 0.9403582, 0.9485307, 0.95252997, 0.95200837, 0.94261867, 0.94383585, 0.95183444, 0.95009565, 0.9462702, 0.9433142, 0.9487046, 0.94348806, 0.9506173, 0.95252997, 0.95009565, 0.9424448, 0.95496434, 0.9507912, 0.9533994, 0.94974786, 0.9349678, 0.95200837, 0.9455747, 0.9546166, 0.9445314, 0.9539211, 0.9476613, 0.948009, 0.9462702, 0.95600766, 0.95218223, 0.9374022, 0.9499217, 0.9471396, 0.95131284, 0.9485307, 0.94592243, 0.9368805, 0.94383585, 0.955486, 0.94974786, 0.94696575, 0.95287776, 0.94887847, 0.95218223, 0.95531213, 0.94783515, 0.9565293, 0.95513827, 0.9494001, 0.9514867, 0.95322555, 0.95218223, 0.9464441, 0.94661796, 0.94818294, 0.95009565, 0.94383585, 0.9492262, 0.9506173, 0.9535733, 0.95287776, 0.9547905, 0.9514867, 0.95200837, 0.9440097, 0.94974786, 0.95183444, 0.9502695, 0.95096505, 0.9511389, 0.9487046, 0.9504434, 0.94696575, 0.94661796, 0.9507912, 0.95687705, 0.94609636, 0.9473135, 0.95409495, 0.95565987, 0.9561815, 0.9476613, 0.9455747, 0.94661796, 0.9452269, 0.9467919, 0.9504434, 0.955486, 0.9487046, 0.94505304, 0.9527039, 0.9506173, 0.9483568, 0.949574, 0.95409495, 0.95183444, 0.9523561, 0.9241871, 0.9440097, 0.9464441, 0.9514867, 0.9473135, 0.95322555, 0.95531213, 0.95531213, 0.9485307, 0.94661796, 0.95496434, 0.9527039, 0.9565293, 0.94279253, 0.9516606, 0.94574857, 0.95183444, 0.95531213, 0.9523561, 0.9474874, 0.94887847, 0.95374715, 0.9535733, 0.9533994, 0.96000695, 0.9575726, 0.95252997, 0.95200837, 0.9527039, 0.9462702, 0.95218223, 0.95252997, 0.95131284, 0.9354895, 0.9504434, 0.95322555, 0.9483568, 0.9527039, 0.9502695, 0.95513827, 0.9448792, 0.9539211, 0.949574, 0.95218223, 0.9492262, 0.9473135, 0.95374715, 0.94818294, 0.9400104, 0.94661796, 0.9443575, 0.9527039, 0.9516606, 0.95409495, 0.9485307, 0.95565987, 0.95496434, 0.95409495, 0.9558338, 0.9530516, 0.9547905, 0.9565293, 0.95287776, 0.9535733, 0.94783515, 0.95131284, 0.9492262, 0.9452269, 0.9507912, 0.9514867, 0.95374715, 0.95496434, 0.9462702, 0.9507912, 0.9507912, 0.9573987, 0.95322555, 0.9462702, 0.9547905, 0.9523561, 0.9499217, 0.9546166, 0.9544427, 0.95009565, 0.9514867, 0.9539211, 0.95183444, 0.95322555, 0.95531213, 0.9546166, 0.9511389, 0.9499217, 0.9558338, 0.9527039, 0.95252997, 0.9516606, 0.95287776, 0.9530516, 0.95322555, 0.95374715, 0.94905233, 0.9565293, 0.9533994, 0.9567032, 0.9514867, 0.95287776, 0.9473135, 0.955486, 0.9530516, 0.9561815, 0.9542688, 0.9542688, 0.9492262, 0.9506173, 0.94505304, 0.9504434, 0.95096505, 0.9455747, 0.9535733, 0.9504434, 0.95200837, 0.95322555, 0.95131284, 0.9523561, 0.9527039, 0.9533994, 0.95409495, 0.9494001, 0.95531213, 0.9523561, 0.95183444, 0.9464441, 0.9471396, 0.9433142, 0.9535733, 0.95096505, 0.94818294, 0.9476613, 0.9579204, 0.9575726, 0.9424448, 0.9448792, 0.9502695, 0.9575726, 0.9547905, 0.95878977, 0.95687705, 0.9575726, 0.95287776, 0.95252997, 0.94887847, 0.95496434, 0.9547905, 0.9454008, 0.95183444, 0.9579204, 0.9561815, 0.9544427, 0.9570509, 0.9443575, 0.9514867, 0.9433142, 0.95131284, 0.9523561, 0.9504434, 0.95496434, 0.9476613, 0.9483568, 0.9507912, 0.9533994, 0.9494001, 0.95009565, 0.9535733, 0.95496434, 0.9539211, 0.9547905, 0.95374715, 0.949574, 0.95600766, 0.9523561, 0.9514867, 0.9504434, 0.9546166, 0.9511389, 0.9476613, 0.94905233, 0.9368805, 0.9530516, 0.95565987, 0.9565293, 0.9539211, 0.9514867, 0.9471396, 0.93357676, 0.9487046, 0.9539211, 0.95374715, 0.949574, 0.9499217, 0.9535733, 0.9577465, 0.9455747, 0.9527039, 0.94818294, 0.94609636, 0.948009, 0.9596592, 0.94974786, 0.955486, 0.95844203, 0.9527039, 0.9530516, 0.9483568, 0.9586159, 0.95252997, 0.9563554, 0.9485307, 0.95131284, 0.9547905, 0.9530516, 0.9487046, 0.9516606, 0.9504434, 0.95218223, 0.9542688, 0.9547905, 0.9400104, 0.9473135, 0.9462702, 0.9462702, 0.95513827, 0.9474874, 0.9547905, 0.9582681, 0.94887847, 0.95287776, 0.95287776, 0.9502695, 0.95896363, 0.9473135, 0.95531213, 0.9384455, 0.948009, 0.94348806, 0.94905233, 0.9577465, 0.9530516, 0.9506173, 0.9504434, 0.9542688, 0.9504434, 0.95513827, 0.9542688, 0.9527039, 0.9523561, 0.9535733, 0.9504434, 0.95096505, 0.9533994, 0.9504434, 0.95496434, 0.9542688, 0.9511389, 0.9539211, 0.9514867, 0.943662, 0.9494001, 0.9544427, 0.95809424, 0.95687705, 0.9565293, 0.95600766, 0.95565987, 0.9535733, 0.95131284, 0.95531213, 0.9546166, 0.955486, 0.95809424, 0.9573987, 0.95809424, 0.9573987, 0.95409495, 0.95131284, 0.9516606, 0.9511389, 0.95600766, 0.94661796, 0.9506173, 0.9523561, 0.95374715, 0.9575726, 0.95252997, 0.949574, 0.9487046, 0.9544427, 0.95600766, 0.9601808, 0.9492262, 0.95322555, 0.9558338, 0.94887847, 0.95878977, 0.9561815, 0.9514867, 0.9570509, 0.9530516, 0.9577465, 0.9523561, 0.95200837, 0.95687705, 0.95809424, 0.95600766, 0.95287776, 0.95096505, 0.9575726, 0.9539211, 0.955486, 0.95218223, 0.95565987, 0.9544427, 0.9575726, 0.9464441, 0.9593114, 0.95896363, 0.9471396, 0.9530516, 0.9558338, 0.9511389, 0.9535733, 0.95252997, 0.9511389, 0.9565293, 0.94905233, 0.95513827, 0.95496434, 0.9573987, 0.9523561, 0.95565987, 0.95200837, 0.95183444, 0.9542688, 0.9561815, 0.9570509, 0.95878977, 0.95322555, 0.95409495, 0.9601808, 0.9558338, 0.9546166, 0.95600766, 0.95600766, 0.95844203, 0.9502695, 0.9544427, 0.9561815, 0.95200837, 0.94418365, 0.9487046, 0.9579204, 0.9514867, 0.9514867, 0.9565293, 0.96000695, 0.9577465, 0.9575726, 0.9527039, 0.95722485, 0.95565987, 0.95218223, 0.9511389, 0.9507912, 0.9511389, 0.9464441, 0.9527039, 0.95531213, 0.9563554, 0.95844203, 0.9570509, 0.95600766, 0.95322555, 0.9558338, 0.95200837, 0.95374715, 0.95722485, 0.9499217, 0.95287776, 0.9567032, 0.9547905, 0.9575726, 0.9507912, 0.9530516, 0.9546166, 0.94905233, 0.95200837, 0.94905233, 0.95287776, 0.95844203, 0.9546166, 0.94974786, 0.9462702, 0.9546166, 0.9533994, 0.94592243, 0.95531213, 0.95374715, 0.9511389, 0.9561815, 0.9546166, 0.9567032, 0.9558338, 0.9473135, 0.9516606, 0.9511389, 0.95600766, 0.9544427, 0.9542688, 0.9567032, 0.95531213, 0.95513827, 0.95252997, 0.9546166, 0.95218223, 0.9563554, 0.9533994, 0.9535733, 0.95896363, 0.9565293, 0.9504434, 0.9542688, 0.9511389, 0.95531213, 0.95687705, 0.9539211, 0.9575726, 0.9579204, 0.95844203, 0.9575726, 0.9474874, 0.9507912, 0.955486, 0.9445314, 0.9476613, 0.95183444, 0.95878977, 0.95809424, 0.9579204, 0.95896363, 0.95513827, 0.955486, 0.9575726, 0.9474874, 0.9596592, 0.95287776, 0.95287776, 0.9539211, 0.9539211, 0.9579204, 0.9558338, 0.95600766, 0.95131284, 0.95565987, 0.9487046, 0.94661796, 0.95131284, 0.9567032, 0.95409495, 0.9573987, 0.9539211, 0.94974786, 0.96000695, 0.9547905, 0.9573987, 0.9561815, 0.9547905, 0.95374715, 0.95687705, 0.9573987, 0.9544427, 0.9507912, 0.95913756, 0.9563554, 0.95565987, 0.9535733, 0.95287776, 0.9533994, 0.95374715, 0.95131284, 0.9544427, 0.95374715, 0.9547905, 0.95722485, 0.95183444, 0.9563554, 0.95252997, 0.9535733, 0.9506173, 0.9464441, 0.9474874, 0.95687705, 0.95565987, 0.955486, 0.95565987, 0.95565987, 0.9563554, 0.9558338, 0.9577465, 0.95374715, 0.9579204, 0.9542688, 0.9547905, 0.95687705, 0.9579204, 0.955486, 0.95287776, 0.9558338, 0.9530516, 0.95218223, 0.9601808, 0.9575726, 0.9596592, 0.9546166, 0.9527039, 0.95844203, 0.9494001, 0.95513827, 0.95322555, 0.955486, 0.95374715, 0.96035475, 0.9573987, 0.9563554, 0.95722485, 0.94974786, 0.95409495, 0.9594853, 0.95513827, 0.9570509, 0.96000695, 0.9530516, 0.9596592, 0.9539211, 0.95374715, 0.9579204, 0.9563554, 0.9573987, 0.9577465, 0.95322555, 0.9579204, 0.9617458, 0.95844203, 0.95252997, 0.9539211, 0.9586159, 0.95687705, 0.9563554, 0.9533994, 0.94609636, 0.9514867, 0.9573987, 0.95913756, 0.9582681, 0.95809424, 0.95687705, 0.9570509, 0.95287776, 0.95722485, 0.9544427, 0.9400104, 0.96000695, 0.9393149, 0.9544427, 0.943662, 0.94418365, 0.9464441, 0.95496434, 0.9601808, 0.9546166, 0.95896363, 0.9598331, 0.955486, 0.95809424, 0.9573987, 0.95183444, 0.9577465, 0.96209353, 0.9546166, 0.95496434, 0.9546166, 0.94818294, 0.95844203, 0.9544427, 0.9523561, 0.95687705, 0.9573987, 0.955486, 0.9530516, 0.95322555, 0.95687705, 0.9582681, 0.9539211, 0.9511389, 0.9533994, 0.95878977, 0.95131284, 0.95600766, 0.96035475, 0.95374715, 0.95409495, 0.95409495, 0.95913756, 0.9547905, 0.9485307, 0.9539211, 0.9533994, 0.9577465, 0.9546166, 0.9494001, 0.9530516, 0.9586159, 0.9523561, 0.9563554, 0.9596592, 0.9593114, 0.95896363, 0.9530516, 0.95687705, 0.9535733, 0.9539211, 0.9539211, 0.96000695, 0.95809424, 0.95513827, 0.95322555, 0.9598331, 0.95687705, 0.95878977, 0.95496434, 0.95913756, 0.95287776, 0.95531213, 0.9594853, 0.9565293, 0.95878977, 0.95513827, 0.9547905, 0.9570509, 0.95878977, 0.95409495, 0.95600766, 0.9542688, 0.9575726, 0.9570509, 0.95496434, 0.9535733, 0.96035475, 0.955486, 0.95513827, 0.95600766, 0.9533994, 0.9594853, 0.9575726, 0.9567032, 0.95200837, 0.948009, 0.9502695, 0.95009565, 0.9535733, 0.95287776, 0.9575726, 0.95531213, 0.962963, 0.9570509, 0.9570509, 0.9542688, 0.95513827, 0.95409495, 0.95531213, 0.9530516, 0.9565293, 0.95913756, 0.9533994, 0.94818294, 0.95322555, 0.9527039, 0.96035475, 0.9575726, 0.9558338, 0.95600766, 0.9573987, 0.9539211, 0.95513827, 0.9539211, 0.9544427]\n"
     ]
    }
   ],
   "source": [
    "print(test_accuracies)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.6.13 ('tfcontrib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eaf5aacb79ec998dd6c8ef5b26869cd42c3a1056147eb19094600b4fe39cdf32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
